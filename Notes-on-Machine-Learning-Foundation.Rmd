---
title: "Notes on Machine Learning Foundations"
author: "Yang Ge"
date: "September 28, 2015"
output: 
  html_document:
    keep_md: true
    toc: true
---

# Week 2

## 1. When can machine learn?

### Hypothesis: 'perceptron'

Each "tall" $w$ represents a hypothesis $h$ and multiply the "tall" $x$.

Perceptron is equivalent to linear (binary) classifier.

### Perceptron Learning Algorithm

want g is close to f (hard when _f_ unknow)

Ideally,
$$
g(x_n) = f(x_n) = y_n
$$

Difficulty: H is infinite space.

Idea: start from some g0, and correct its mistakes on D.

Algorithm:
1. Find a mistakes xn(t)
2. Updates w(t+1) <- w(t) + yn(t)\*xn(t)
Geometric explaination: if the angle between w and x is too big, we make it
smaller, otherwise, make it bigger.

A fault confessed is half redressed.

## Guarantee of PLA

The normalized inner product of w_f and w_t will increase monotonously.
It will stop at certain iteration.

## Non separable data

Pros: simple to implement, fast, work for any dimension d.
Cons:
* Assume linearly separable.
* not fully sure how long halting takes (rho depends on wf)

PLA Algorithm

Run enough iteration.

# Types of Learning

## Learning with different output space

Binary classification
* Credit card approval
* Email spam/not spam
* Patient sick/not sick
* ad profitable/not profitable
* answer correct/incorrect

## Learning with different labels
## Learning with different protocols
## Learning with different input space

# Lecture 4, Feasible of Learning

## Learning is impossible?

No free launch!

## Probability to the Rescue

Bin model.

in sample $\nu$ is likely close to unknow $\mu$.

Hoeffding's Inequality:

$$
Pr(|\mu-\nu| \geq \epsilon) \leq exp(-2\epsilon^{2}N)
$$

**_Probably Approximately Correct_** (PAC).

## Connection to Learning

Learning.

Unknown **f**
target f(x)

h is wrong/orange equivalent h(x) not equals to f(x)
h is right/green
Check h on D = {(xn, yn)}

### Added components

$E_{in}$: in sample

$E_{out}$: out sample

### The Formal Guarantee

* Valid for all $N$ and $\epsilon$.
* Doesn't depend on $E_{out}(h)$, $f$ and $P$ can stay unknown.
* $E_{in}(h) = E_{out}(h)$ is PAC.

## Connection to real learning

### BAD Data for Many $h$

no 'freedom of choice' by A, 踩到雷。

### Bound of BAD data

If $|\mathcal{H}| = M$ is finite, $N$ large enough, for whatever $g$ picked by $\mathcal{A}$, $E_{out}(g) \approx E_{in}(g)$.

If $\mathcal{A}$ finds one $g$ with $E_{in}(g) \approx 0$, PAC guarantees for $E_{out}(g) \approx 0$.

# Lecture 5 Training v.s. Testing

## Recap and Preview

If $|\mathcal{H}| = M$ is finite, $N$ large enough, for whatever $g$ picked by $\mathcal{A}$, $E_{out}(g) \approx E_{in}(g)$.

If $\mathcal{A}$ finds one $g$ with $E_{in}(g) \approx 0$, PAC guarantees for $E_{out}(g) \approx 0$.

$$
E_{out}(h) \underbrace{\approx}_{test} E_{in}(h) \underbrace{\approx}_{train} 0
$$

**Two Central Questions:**

1. Can we make sure that $E_{out}(h)$ is close enough to $E_{in}(g)$?

2. Can we make $E_{in}(g)$ small enough?

What role does $\underbrace{|\mathcal{H}|}_{M}$ play for these two questions?

**Trade-off on $M$**

* small $M$

    1. Yes! $Pr(BAD) \leq 2 \cdot M \cdot exp(\dots)$
    2. No! too few choices!

* large $M$

    1. No! $Pr(BAD) \leq 2 \cdot M \cdot exp(\dots)$ is big.
    2. Yes! many choices

* What if $M = \infty$

**Todo:**

* establish a finite quantity that replaces $M$
$$
Pr(|E_{in}(g)-E_{out}(g)| > \epsilon) \leq 2 m_{\mathcal{H}} exp(-2 \epsilon^2 N)
$$
* justify the feasiblity of learning for infinite $M$
* study $m_{\mathcal{H}}$ to understand its trade-off for 'right' $\mathcal{H}$

These will be covered in next 3 lectures :-)

## Effective Number of Lines

**Where did M come from**

* *BAD events $B_m$*: $|E_{in}(B_m) - E_{out}(B_m)| > \epsilon$
* To give $\mathcal{A}$ freedom choice, need to bound $Pr(B_1 \cup B_2 \cup \dots B_M)$
* Worse case: all $B_m$ non-overlapping. It fails when M is infinite.
$$
Pr(B_1 \cup B_2 \cup \dots B_M) \leq Pr(B_1) + \dots + Pr(B_M)
$$

**Where did uniform bound fail?**

* *BAD events $B_m$*: $|E_{in}(B_m) - E_{out}(B_m)| > \epsilon$ overlapping for similar hypothesis $h_1 \approx h_2$
* Why?
    * $E_{out}(h_1) \approx E_{out}(h_2)$
    * for most $D$, $E_{in}(h_1) \approx E_{in}(h_2)$
* union bound *over-estimation*
* Solution: group similar hypothesis by kind.

**How Many lines Are There?**

$\mathcal{H} = {all\ lines\ in\ R^2}$

* how many lines? $\infty$
* how many kinds of lines from $\mathbf{x_1}$ point of view? 2 kinds:
    * $h_1(x_1) = +1$ and $h_2(x_1) = -1$
* how many kinds of lines for 4 points? At most 14.

**Effective Number of Lines**

It means max number of lines w.r.t N inputs $x_1, \dots, x_N$

* must be $\leq 2^N$
* finite 'grouping' of infinitely-many lines $\in \mathcal{H}$
* wish:

$$
Pr(|E_{in}(g)-E_{out}(g)| > \epsilon) \leq 2\ eff(N)\ exp(-2 \epsilon^2 N)
$$

If

1. eff(N) can replace $M$ and 
2. eff(N) << $2^N$

Then learning is possbile for infinite lines :-)

## Effective Number of Hypothesis

**Dichotomy: mini hypothesis**

* Definition: hypothesis 'limited' to the eyes of $x_1, \dots, x_n$, i.e.
$$
h(x_1, \dots, x_n) = (h(x_1), \dots, h(x_n)) \in \{\times, \circ\}^N
$$

* Dichotomy set: $\mathcal{H}(x_1, \dots, x_n)$: all dichotomies implemented by $\mathcal{H}$ on $x_1, \dots, x_n$.
* Dichotomy set is bounded by $2^N$, $\mathcal{H}$ can be infinite.
* $|\mathcal{H}(x_1, \dots, x_n)|$ is a candidate for replacing $M$.

**Growth Function**

In order to remove the dependance between the size of dichotomy set and the inputs,
define $m_H(N) = max|\mathcal{H}(x_1, \dots, x_n)|$. For example, for PLA,
$m_H(1) = 2, m_H(2) = 4, m_H(3) = 8, m_H(4) = 14$.

It is called growth function and bounded by $2^N$.

**Growth Function for Positive Rays**

* $\mathcal{X} = \mathbb{R}$
* $\mathcal{H}$ contains $h$, where each $h(x) = sign(x-1)$ for threshold $a$.
* positive half of 1D perceptrons.
* $m_{\mathcal{H}}(N)=N+1 \ll 2^N$

**Growth Function for Positive Intervals**

* $\mathcal{X} = \mathbb{R}$
* $\mathcal{H}$ contains $h$, where each $h(x) = sign(x-1)$ iff $x \in [l,r)$, -1 otherwise.
$$
m_{\mathcal{H}}(N) = \binom{N+1}{2} + 1 = \frac{N^2}{2} + \frac{N}{2} + 1 \ll 2^N
$$

**Growth Function for convex set**

* If $x_1, \dots, x_n$ on a big circle, then every dichotomy can be implemented.
$$
m_{\mathcal{H}}(N) = 2^N
$$
* call those $N$ inputs 'shattered' by $\mathcal{H}$.

## Break Point

**The 4 growth functions**

* positive ray, $m_{\mathcal{H}}(N)=N+1$
* postitive intervals, $\frac{N^2}{2} + \frac{N}{2} + 1$
* convex set, $2^N$
* 2D perceptrons: $m_{\mathcal{H}}(N) < 2^N$ in some cases

Can we replace $M$ by $\mathcal{H}$? Polynomial good, exponential bad...

$$
Pr(|E_{in}(g)-E_{out}(g)| > \epsilon) \overset{?}{\leq} 2\ m_{\mathcal{H}}(N) \ exp(-2 \epsilon^2 N)
$$

**Break point of \mathcal{H}**

* three inputs: 'exists' shatter; for inputs, 'for all' no shatter
* if no $k$ inputs can be shattered by $\mathcal{H}$, the $k$ is a break point.
* Any $k+i$ is break points.
* conjecture: break point $k$, $m_{\mathcal{H}}(N) = O(N^{k-1})$

## Conclusion

* Recap and Preview:
    * two questions, $E_{out} \approx E_{in}$ and $E_{in}(g) \approx 0$
* Effective No. of lines
* Effective No. of Hypothesis
* Break point
    * When $m_{\mathcal{H}}$ becomes non-exponential

# Lecture 6 Theory of Generalization

## Restriction of Break Point

* max possible $m_{\mathcal{H}}$ when $N=3$ and $k=2$
    * 4 dichotomies, shatter two points, no, but 5 yes.

* break point $k$ restricts maximum possible $m_{\mathcal{H}}$ a lot for $N > k$

* idea:
$$
m_{\mathcal{H}} \leq m_{\mathcal{H}}\ given\ k \leq ploy(N)
$$

## Bounding Function: Basic Cases

* bounding function $B(N, k)$: max possible $m_{\mathcal{H}}$ when break point $= k$
* combinatorial quantity: max No. of length-N vectors with $(\circ, \times)$
while 'no shatter' any length-$k$ subvectors.
* new goal $B(N, K) \leq poly(N)$

## Bounding Function: Inductive Cases

* Estimating $B(4, 3)$, after exhaustively searching the $2^{2^4}$, the winner is 11.
For 4 points, there are 16 possible $(\circ, \times)$ combinations, each of them
can either be in or not in the dichotomy set, so there are $2^{16}$ possible
dichotomy sets.

* Estimating $B(4, 3)$, $B(4, 3) = 11 = 2\alpha + \beta$. $\alpha+\beta$: dichotomies on $(x_1, x_2, x_3)$.
$\alpha+\beta \leq B(3, 3)$

* If we only look at the orange part, no any two points can be shattered. So $\alpha \leq B(3, 2)$.

* We have $B(4, 3) \leq B(3, 3) + B(3, 2)$. And $B(N, k) \leq B(N-1, k) + B(N-1, k-1)$.

**Bounding Function: The Theorem**

$$
B(N,k) \leq \sum_{i=0}^{k-1} \binom{N}{i}
$$

The bottom line is $m_{\mathcal{H}}$ is poly(N) if break point exists.

**Three Break Points**

2D perceptrons: $m_{\mathcal{H}}(4) = 14 < 2^4$: break point at 4.

## A pictorial Proof

**want**

$$
Pr(\exists h \in \mathcal{H}\ s.t. |E_{in}(g)-E_{out}(g)| > \epsilon) \leq 2\ m_{\mathcal{H}}(N) \ exp(-2 \epsilon^2 N)
$$

**we can get**
$$
Pr(\exists h \in \mathcal{H}\ s.t. |E_{in}(g)-E_{out}(g)| > \epsilon) \leq 2\cdot 2\ m_{\mathcal{H}}(2N) \ exp(-2 \frac{1}{16} \epsilon^2 N)
$$

**Step1: Replace $E_{out}$ by $E_{in}'$**

* How? sample verification set $D'$ of size N to calculate $E_{in}'$
* Bad $h$ of $E_{in}-E_{out}$ can probably infer bad $h$ of $E_{in}-E_{in}'$

**Step2: Decompose $\mathcal{H}$ by kind**

$$
BAD \leq 2Pr(\exists h \in \mathcal{H}\ s.t. |E_{in}(h)-E_{in}'(h)| > \epsilon/2)
\leq 2m_{\mathcal{H}}(2N)Pr(fixed\ h \in \mathcal{H}\ s.t. |E_{in}(h)-E_{in}'(h)| > \epsilon/2)
$$

**Step3: Use Hoeffding without Replacement**

VC bound:

$$
BAD
\leq 2m_{\mathcal{H}}(2N)Pr(fixed\ h \in \mathcal{H}\ s.t. |E_{in}(h)-E_{in}'(h)| > \epsilon/2)
\leq 4m_{\mathcal{H}}(2N)exp(-\frac{1}{8}\epsilon^2N)
$$

## Conclusion

* Restriction of Break Point: breaks consequent points
* Bounding Function: Basic Cases, $B(N,k)$ bounds $m_{\mathcal{H}}(N)$
* Bounding Function: Inductive Cases, $B(N,k)$ is poly(N)
* A pictorial Proof: $m_{\mathcal{H}}(N)$ can replace $M$ with a few changes.

# Lecture 7 The VC Dimension

## Definition of VC Dimension

**Recap: More on Growth Function**

$m_{\mathcal{H}}(N)$ of break point $k \leq B(N,k)=\sum_{i=0}^{k-1} \binom{N}{i} \sim O(N^{k-1})$

**Recap: More on VC bound**

For any $g=\mathcal{A}(\mathcal{D}) \in \mathcal{H}$ and 'statistival' large $\mathcal{D}$

$$
\begin{split}
\mathbb{P}_{\mathcal{D}}(|E_{in}(g)-E_{out}(g)| > \epsilon) \\
\leq \mathbb{P}_{\mathcal{D}}(\exists h \in \mathcal{H}\ s.t. |E_{in}(h)-E_{in}'(h)| > \epsilon) \\
\leq 4m_{\mathcal{H}}(2N)exp(-\frac{1}{8}\epsilon^2N) \\
\leq 4(2N)^{k-1}exp(-\frac{1}{8}\epsilon^2N)
\end{split}
$$

If:

1. $m_{\mathcal{H}}$ breaks at $k$, (good $\mathcal{H}$)
2. $N$ large enough, (good $\mathcal{D}$)
3. $\mathcal{A}$ picks good $g$ with small $E_{in}$, good $\mathcal{A}$


**VC Dimension**

Definition:

* $d_{VC}(\mathcal{H})$, VC dimesion of $\mathcal{H}$: largest $N$ $m_{\mathcal{H}}(N) = 2^N$
* the most input $\mathcal{H}$ that can shatter
* $N \leq d_{vc} \Longrightarrow \mathcal{H}$ can shatter some $N$ inputs
* $k \geq d_{vc} \Longrightarrow k$ is a break point

$$
m_{\mathcal{H}}(N) \leq N^{d_{VC}}, if N \geq 2, d_{VC} \geq 2
$$

**The Four VC Dimensions**

* positive ray, $d_{VC} = 1$
* positive intervals, $d_{VC} = 2$
* convex set, $d_{VC} = \infty$
* 2D perceptrons, $d_{VC} = 3$

**VC Dimension and Learning**

* regardless of learning algorithm
* regardless of input distribution $P$
* regardless of target function $f$

**FUn time**
No conclusion

## VC Dimension of Perceptrons

**2D PLA revisited**

* linearly separable $\mathcal{D}$ -> PLA can converge -> $T$ iterations -> $E_{in}(g) = 0$
* with $x_n \sim P$ and $y_n = f(x_n)$ -> $\mathbb{P}_{\mathcal{D}}(|E_{in}(g)-E_{out}(g)| > \epsilon) \leq \dots$
because $d_{VC} = 3$ -> $N$ is large -> $E_{in}(g) \approx E_{out}(g) \approx 0$

**VC Dimension of Perceptrons**

* 1D (pos/neg rays): $d_{VC}=2$
* 2D: $d_{VC}=3$
* d-D: : $d_{VC} \overset{?}{=} d+1$
    * $d_{VC} \geq d+1$
    * $d_{VC} \leq d+1$

**$d_{VC} \geq d+1$**

* A special matrix

**$d_{VC} \leq d+1$**

* linear dependency restricts dichotomy
$$
w^Tx_4 = w^Tx_2 + w^Tx_3 - w^Tx_1 > 0
$$

## Physical Intuition of VC Dimenson

**Degrees of Freedom**

**Two Old Friends**

* Positive Rays, free parameters: $a$
* Positive Intervals, free parameters: $l, r$
* practical rule of thumb: $d_{VC} \approx$ # of free parameters(but not always).

**$M$ and $d_{VC}$**

* small $d_{VC}$
* large $d_{VC}$
* using the right $d_{VC}$ (or $\mathcal{H})

## Interpreting VC Dimension

**VC Bound Rephrase: Penalty for Model Complexity**

For any $g=\mathcal{A}(\mathcal{D}) \in \mathcal{H}$ and 'statistival' large $\mathcal{D}$

$$
\mathbb{P}_{\mathcal{D}}(|E_{in}(g)-E_{out}(g)| > \epsilon) \leq
4(2N)^{d_{VC}}exp(-\frac{1}{8}\epsilon^2N)
= \delta
$$
$$
\epsilon = \sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{VC}}}{\delta})}
$$
with probablity $1-\delta$, :
Generalization error $|E_{in}(g)-E_{out}(g)| \leq \sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{VC}}}{\delta})}$
$$
E_{out}(g) \leq E_{in}(g)+\sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{VC}}}{\delta})}
$$

**THE VC message**

$\Omega(N, \mathcal{H}, \delta) = \sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{VC}}}{\delta})}$
is called model complexity penalty.

* $d_{VC} \uparrow$, $E_{in} \downarrow$ but $\Omega \uparrow$
* $d_{VC} \downarrow$, $E_{in} \uparrow$ but $\Omega \downarrow$
* best $d_{VC}$ is in the middle
* powerful $\mathcal{H}$ not always good!

**VC Bound Rephrase: Sample Complexity**

* Need $10000d_{VC}$ in theory
* Practical rule of thumb: $10d_{VC}$

**Looseness of VC Bound**

* Hoeffding for unknown $E_out$, any distribution any target.
* $m_{\mathcal{H}}$ instead of $|\mathcal{H}(x_1, \dots, x_n)|$, any data
* $N^{d_{VC}}$ instead of $m_{\mathcal{H}}$, any $\mathcal{H}$ of same $d_{VC}$
* union bound on worst cases, any choice made by $\mathcal{A}$
* But hardly better, and similar loose for all models
* Philosophical message of VC bound is important for improving ML

## Conclusions

* Definition VC Dimension: max non-break point
* VC Dimension of Perceptrons: $d+1$
* Physical Intuition of VC Dimenson: $\approx$ # free parameters
* Interpreting VC Dimension: model and sample complexity

# Lecture 8 Noise and Error

## Noise and Probabilistic Target

What if there is noise?

**Noise**

* noise in y: good customer mislabeled as bad
* noise in y: same customer different labels
* noise in x: inaccurate information

Does VC bound work under noise?

**Probabilistic Marbels**

* Probabilistic (noise) marbles
    * marble $x \sim P(x)$
    * probabilistic color, $y \ne h(x)$ with $y \sim P(y|x)$

* VC holds for
    * $x \overset{i.i.d}{\sim} P(x)$
    * $y \overset{i.i.d}{\sim} P(y|x)$
    * $(x, y) \overset{i.i.d}{\sim} P(x, y)$

**Target Distribution $P(y|x)$**

* can be viewed as 'idea mini target' + noise, e.g.
    * $P(\circ|x) = 0.7, P(\times|x) = 0.3$
    * idea mini target $f(x) = \circ$
    * 'flipping' noise level = 0.3
* deterministic target $f$: *special case of target distribution*
    * $P(y|x) = 1$ for $y = f(x)$
    * $P(y|x) = 0$ for $y \ne f(x)$
    
* goal of learning: predict idea mini target (w.r.t $P(y|x)$)
on often-seen inputs (w.r.t $P(x)$)

**New Learning flow**

VC still works, pocket algorithm explained :)

## Error Measure

final hypothesis $g \approx f$

* how well? previously, considered out-of-sample measure
$$
E_{out}(g) = \underset{X \sim P}{\epsilon}(g(x) \ne f(x))
$$

* more generally, error measure $E(g, f)$
* natrually considered
    * out-of-sample: averaged over unknow $x$
    * pointwise: evaluated on one $x$
    * classification: [prediction $\ne$ target], often also called '0/1 error'

**Point Error Measure**

$$
E_{out}(g) = \underset{X \sim P}{\mathcal{E}} \underbrace{[g(x) \ne f(x)]}_{error(g(x), f(x))}
$$

* in-sample

$$
E_{in}(g) = \frac{1}{N} \sum_{n=1}^{N}err(g(x_n), f(x_n))
$$

* out-of-sample

$$
E_{out}(g) = \underset{X \sim P}{\mathcal{E}} err(g(x), f(x))
$$

will mainly consider pointwise err for simplicity

**Two Important Pointwise Error Measures**

* 0/1 error
    * $err(\tilde{y}, y) = [\hat{y} \ne y]$
    * correct or not
    * often for classification
    
* squared error
    * $err(\tilde{y}, y) = (\hat{y}-y)^2$
    * how far $\hat{y}$ from $y$
    * often for regression

How does err 'guide' learning?

*Ideal Mini-Target*

$$
P(y=1|x) = 0.2, P(y=2|x) = 0.7, P(y=3|x) = 0.1
$$

* 0/1 error, $err(\tilde{y}, y) = [\tilde{y} \ne y]$
    * $\tilde{y} = 1$, avg. err 0.8
    * $\tilde{y} = 2$, avg. err 0.3
    * $\tilde{y} = 3$, avg. err 0.9
    * $\tilde{y} = 1.9$, avg. err 1
    * $f(x) = \underset{y\in\mathcal{y}}{argmax}(P(y|x))$
    
* squared error, $err(\tilde{y}, y) = (\tilde{y}-y)^2$
    * $\tilde{y} = 1$, avg. err 1.1
    * $\tilde{y} = 2$, avg. err 0.3
    * $\tilde{y} = 3$, avg. err 1.5
    * $\tilde{y} = 1.9$, avg. err 0.29
    * $f(x) = \sum_{y\in\mathcal{y}} y \cdot P(y|x)$

**Learning Flow with Error Measure**

Extended VC theory/'Philosophy' works for most $\mathcal{H}$ and err.

## Algorithmic Error Measure

**Choice of Error Measure**

two types of error: false accept and false reject

0/1 error penalizes both types equally.

* supermarket: fingerprint for discount
    * false reject: very unhappy customer, lose future business, (penalty 10)
    * false accept: give away a minor discount, intruder left fingerprint (penalty 1)
* CIA: fingerprint for entrance
    * false accept: very serious consequences! (penalty 1000)
    * false reject: unhappy employee, but so what? :) (penalty 1)

**Take-home Message for Now**

err is application/user-dependent

Algorithm Error Measures $\widehat{error}$

* true: just err
* plausible:
    * 0/1 minimum 'flipping noise' - NP-hard to optimize 
    * squared: minimum Gaussian noise
* friendly: easy to optimize for $\mathcal{A}$
    * closed-form solution
    * convex objective function

err: application goal

$\widehat{err}$: a key part of many $mathcal{A}$

## Weighted Classification

$$
\begin{array}{c|c}
  &
    \begin{matrix}
    h(x) \\
    1 \qquad -1
    \end{matrix}
  \\
  \hline
    \begin{matrix}
    f(x) & 1 \\
         & -1
    \end{matrix}
  &
    \begin{matrix}
    0 & 1 \\
    1000 & 0
    \end{matrix}  
\end{array}
$$

out-of-sample $E_out(h)$ = 

* 1, if $y_n = +1$
* 1000, if $y_n = -1$

in-sample is similar

**Minimizing $E_{in}$ for weighted Classification**

Naive Thoughts:

* PLA: doesn't matter if linear separable.
* pocket: modify pocket-replacement rule
    * if $w_{t+1}$ reaches smaller $E_{in}^{w}$ than $\hat{w}$

**Systematic Route: Connect $E_{in}^w$ and $E_{in}^{0/1}$**

* idea: duplicate the data point with -1 label 1000 times.

**weighted pocket algorithm**

using 'virtual copying', weighted pocket algorithm include:

* weighted PLA: randomly check -1 example mistakes with 1000 times more probability
* weighted pocket replacement: if $w_{t+1}$ reaches smaller $E_{in}^{w}$ than $\hat{w}$, replace $\hat{w}$ by $w_{t+1}$

systematic route (called reduction): can be applied to many other algorithm

**Fun time**

Unbalanced data

## Conclusions

* Noise and Probabilistic Target
    * can replace $f(x)$ with $P(y|x)$
* Error Measure
    * affect 'ideal' target
* Algorithmic Error Measure
    * user-dependent => plausible or friendly
* Weighted Classification
    * easily done by virtual 'example copying'
    
# Lecture 9
# Lecture 10