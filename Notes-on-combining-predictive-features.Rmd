---
title: "Notes-on-combining-predictive-features"
author: "Yang Ge"
date: "5/16/2017"
header-includes:
   - \usepackage{bm}
output:
  html_document:
    keep_md: false
    toc: true
    # includes:
    #    in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Lecture 7 Blending and Bagging

## Motivation of Aggregation

* An Aggregation Story

How to combine $g_1, g_2, \dots, g_T$ to a $g_t(x)$

1. Select best one: validation!
$$
G(x) = g_{t_*}(x)\ \text{with } t_* = \underset{t\in{1,2,\dots,T}}{\text{argmin}}\ E_{val}(g_t^-)
$$
2. Mix uniformly
$$
G(x) = \text{sign } (\sum_{1}^{T} 1 \cdot g_t(x))
$$
3. Mix non-uniformly
$$
G(x) = \text{sign } (\sum_{1}^{T} \alpha_t \cdot g_t(x))
$$
  * include case 1: $\alpha_t = \| E_{val}(g_t^-)\ \text{smallest } \|$
  * include case 2: $\alpha_t = 1$
4. Combine conditionally
    * Some one is good at tech stock, others are good at energy stock
    * include case 3.

$$
G(x) = \text{sign } (\sum_{1}^{T} q_t(x) \cdot g_t(x))
$$

**_Aggregation: mix or combine hypothesis to get better performance_, it's a rich family**

* Selection by validation

$$
G(x) = g_{t_*}(x)\ \text{with } t_* = \underset{t\in{1,2,\dots,T}}{\text{argmin}}\ E_{val}(g_t^-)
$$
  
* simple and popular
* What if use $E_{in}(g_t)$ instead of $E_{val}(g_t^-)$? complexity price with $d_{vc}$
* need one strong $g_t^-$ to guarantee small $E_{val}$ and small $E_{out}$

* **selection**: rely on one strong hypothesis
* **Aggreration**: can we do better with many (possibly weaker) hypothesis.

## Uniform Blending

* Uniform blending for classification
    * known $g_t$, each with 1 ballot
    * same $g_t$, as good as one single $g_t$
    * very different $g_t$ (diversity + democracy), majority corrects minority
    * similar results with uniform voting for multi-class
    
$$
G(x) = \text{sign } (\sum_{1}^{T} 1 \cdot g_t(x))
$$
$$
G(x) = \underset{1 \le k \le K}{\text{argmax}}\ \sum_{1}^{T} [g_t(x)=k]
$$

### Theoretic analysis of Uniform Blending

$$
\text{For a given } x\\
G=\frac{1}{T}\sum{g_t} = \text{avg}(g_t), \text{then} \\
\text{avg}((g_t - f)^2) = \text{avg}((g_t - G)^2) + (G-f)^2 \\
\text{avg}(E_{out}(g_t)) = \text{avg}(E(g_t-G)^2) + (G-f)^2 \ge (G-f)^2
$$

### Special g

$$
\text{avg}(E_{out}(g_t)) = \text{avg}(E(g_t-\overline{g})^2) + 
E_{out}(\overline{g})^2 
$$

* The 2nd part is called the performance of consensus
* The 1st part is called variance of consensus
* When variance goes down, the performance goes up.

## Linear and Any Blending

### Linear blending

$$
G(x) = \text{sign } (\sum_{1}^{T} \alpha_t \cdot g_t(x))\ \text{with } \alpha_t \ge 0 \\
\text{good }\alpha_t: \underset{\alpha_t \ge 0}{\text{min}} E_{in}(\alpha) \\
\text{linear blending for regression: }
\underset{\alpha_t \ge 0}{\text{min}} \frac{1}{N}\sum_{1}^{N}(y_n - \sum_{1}^{T} \alpha_t g_t(x_n))^2\\
\text{linear Reg + Transformation: }
\underset{w_i}{\text{min}} \frac{1}{N}\sum_{n = 1}^{N}(y_n - \sum_{i = 1}^{\tilde{d}} w_i \phi_i(x_n) )^2
$$

### Constraints on alpha

* Linear blending = linear model + hypothesis as transform + constraints

$$
\underset{\alpha_t \ge 0}{\text{min}} \frac{1}{N}\sum_{1}^{N}
\text{err } (y_n - \sum_{1}^{T} \alpha_t g_t(x_n))
$$

* linear blending for binary classification

$$
\text{if }\alpha_t < 0 \Rightarrow \alpha_t g_t(x) = |\alpha_t|(-g_t(x)) \\
$$

* In practice, Linear blending = linear model + hypothesis as transform (removed constraints)

### Blending versus Selection

* Usually $g_1 \in \mathcal{H_1}, \dots, g_T \in \mathcal{H_T}$.
* Selection is best of best, paying $d_{vc}(\bigcup_{1}^{T}\mathcal{H_t})$
* Blending is Aggregation of best, paying even higher.
* Practically, blending done with $E_{val}$ instead of $E_{in}$ + $g_t^{-}$ from minimum $E_{train}$

### Any blending

* Given $g^-_1, \dots, g^-_T$ from $D_{train}$, transform $x_n, y_n$ in $D_{val}$ to
$z_n = \phi^-(x_n) = (g^-_1(x), \dots, g^-_T(x)), y_n$

* Linear blending
    * compute $\alpha = \text{Lin}({z_n, y_n})$
    * return $G_{LINB}(x) = \text{LinH}(\text{innerprod}(\alpha,\phi(x)))$, note here, the minus on phi is
      removed.
* Any blending (Stacking)
    * compute $\tilde{g} = \text{Any}({z_n, y_n})$
    * return $G_{ANYB}(x) = \tilde{g}(\phi(x))$
* Very powerful but danger of overfitting.

### Blending in Practice

* KDD Cup 2011, "A linear ensemble of individual and blended models for music rating applications"
    * validation set blending: a special any blending model.
* test set blending: linear blending using $\tilde{E}_{test}$. (Is it kinda cheating?)

## Bagging (Bootstrap Aggregation)

### What we have done

| aggregation type | blending | learning |
| ---------------- | -------- | -------- |
| uniform | voting/averaging | |
| non-uniform | linear | |
| conditional | stacking | |

learning $g_t$ for uniform aggregation: diversity is important.

* diversity by different models $g_1 \in \mathcal{H_1}, \dots, g_T \in \mathcal{H_T}$
* diversity by different parameters: GD with$\eta = 0.001, 0.01, \dots, 10$
* diversity by algorithmic randomness: random PLA with different random seeds
* diversity by data randomness: within-cross-validation hypotheses $g_v^-$

### Bootstrapping Aggregation

* bootstrapping: bootstrap sampe $\tilde{D_t}$: re-sample N examples from $\mathcal{D}$
  uniformly with replacement, meaning the sample can have duplicates.
* Don't have to be N, can be N'. The important part is sample with replacement.
* bootstrap aggregation:
    * request size-N' data $\tilde{D_t}$ from bootstrapping
    * obtain $g_t$ by by $\mathcal{A}(\tilde{D_t})$, then G = uniform($g_t$)
* A simple meta algorithm on top of base algorithm $\mathcal{A}$

# Lecture 8 Adaptive Boosting

## Motivation of boosting

### Motivation

* Students: simple hypothesis $g_t$ (like vertical/horizontal lines)
* Class: sophisticated hypothesis $G$
* Teacher: a tactic learning algorithm that directs the students to focus on key examples.

## Diversity by Re-weighting

### Bootstrapping as Re-weighting Process

$$
D = \{(x_1, y_1), (x_2, y_2),(x_3, y_3), (x_4, y_4)\} \\
\text{bootstrap} => \tilde{D_t} = \{(x_1, y_1), (x_1, y_1),(x_2, y_2), (x_4, y_4)\}
$$

* Weighted $E_{in}$ on $\mathcal{D}$

$$
E_{in}^u(h) = 1/4 \sum_{n=1}^4 u_n \cdot [y_n \ne h(x_n)] \\
(x_1, y_1), u_1 = 2 \\
(x_2, y_2), u_2 = 1 \\
(x_3, y_3), u_3 = 0 \\
(x_4, y_4), u_4 = 1 \\
$$

* $E_{in}$ on $\tilde{\mathcal{D_t}}$ 

$$
E_{in}^{0/1}(h) = 1/4 \sum_{(x,y) \in \tilde{\mathcal{D_t}}} \cdot [y_n \ne h(x_n)] \\
\tilde{\mathcal{D_t}} = \{(x_1, y_1), (x_1, y_1),(x_2, y_2), (x_4, y_4)\}
$$

* Each diverse $g_t$ in bagging: by minimizing bootstrap-weighted error

### Weighted Base Algorithm

* minimize (regularized)

$$
E_{in}^u(h) = 1/N \sum_{n=1}^N u_n \cdot \text{err} (y_n, h(x_n)) \\
$$

* SVM can handle weighted sample

$$
E_{in}^u(h) \propto C\ \sum_{n=1}^N\ u_n \widehat{\text{err}}_{SVM} \\
\Leftrightarrow 0 \le \alpha \le Cu_n
$$

* logistic regression can handle weighted sample
    * $E_{in}^u(h) \propto \sum_{n=1}^N\ u_n \text{err}_{CE}$ by SGD
      $\Leftrightarrow$ sample $(x_n, y_n)$ with probability proportional to $u_n$

* example-weighted learning:
    * extension of class-weighted learning in lecture 8 of ML Foundations.
    
### Re-weighting for More Diverse Hypothesis

* How to re-weight for more diverse hypothesis?

$$
g_t \leftarrow \underset{h \in \mathcal{H} }{\text{argmin}}
\sum_{n=1}^N u_n^{(t)} [y_n \ne h(x_n)]
\\
g_{t+1} \leftarrow \underset{h \in \mathcal{H} }{\text{argmin}}
\sum_{n=1}^N u_{n}^{(t+1)} [y_n \ne h(x_n)]
$$

* Diversity means, if $g_t$ is not good $u_{n}^{(t+1)}$, then
  $g_t$-like hypothesis will not returned as $g_{t+1}$. Then $g_{t+1}$ will
  diverse from $g_{t}$

* idea: construct $u_{n}^{(t+1)}$ to make $g_{t}$ random-like

$$
\frac
{\sum_{n=1}^N u_{n}^{(t+1)} [y_n \ne g_t(x_n)] } 
{\sum_{n=1}^N u_{n}^{(t+1)} }
=
\frac{1}{2}
$$

### optimal re-weighting

* weighted incorrect rate $\epsilon_t$
* multiply incorrect $\propto (1-\epsilon_t); u_n^{t+1} \leftarrow (1-\epsilon_t)u_n^{t}$
; multiply correct $\propto \epsilon_t; u_n^{t+1} \leftarrow \epsilon_t u_n^{t}$

## Adaptive Boosting algorithm

### Scaling factor

* Here, the important thing is you don't care $\sum_{n=1}^N [y_n \ne g_t(x_n)]$ much
  You care about $\sum_{n=1}^N u_n^{(t)}[y_n \ne g_t(x_n)]$
* Let me give an example, let's say we have 5 points, their current weight is (1, 2, 3, 4, 5).
  The current $g_t$ gives $(F, F, T, T, T)$, then weighted error rate is $3/15$.
  Then new $u_n^{t+1}$ would be $(12, 12x2, 3x3, 3x4, 3x5) = (12, 24, 9, 12, 15)$.
* Now if we consider non-weighted error, which is $2/5$,
  Then new $u_n^{t+1}$ would be simply be $(3, 3, 2, 2, 2)$
* 可以问一下老师为什么要用weighted error rate去scale $u_n$，而不是简单的用non-weighted
  error rate去scale $u_n$。这样也可以让$g_t$的error rate变成$1/2$。我的理解是如果用后者，就
  切断了$u^{t+1}$以及$g_{t+1}$和之前的联系。是不是之后aggregate起来就没有VC dim的保证了？

$$
\epsilon_t
=
\frac
{\sum_{n=1}^N u_n^{(t)}[y_n \ne g_t(x_n)]}
{\sum_{n=1}^N u_n^{(t)}}
$$

* multipy incorrect $\propto (1-\epsilon_t)$; multiply correct $\propto \epsilon_t$

* The following is equivalent

$$
\diamondsuit_t = \sqrt{\frac{1-\epsilon_t}{\epsilon_t}}
\\
\text{incorrect} \leftarrow \text{incorrect} \cdot \diamondsuit_t
\\
\text{correct} \leftarrow \text{correct} / \diamondsuit_t
$$

* $\diamondsuit_t > 1$ iff $\epsilon_t \le 1/2$,
  which means scale up incorrect and scale down correct, like the teacher does.
* scaling up incorrect examples leads to diverse hypothesis

### A Preliminary Algorithm

* Problems remain:
    1. What could be our $u^{(1)}$
    2. How to aggregate $g_t$ to get $G(x)$

* Answer to 1. $u^{(1)} = 1/N$ would be a good choice.
* Answer to 2. uniform is not a good choice because $g_2$ would behave bad on original dataset.
  linear aggregation might be good.
  
### Linear aggregation on the fly

$$
G(x) = sign(\sum_1^T \alpha_t g_t)
$$

* The intention is to give good $g_t$ with bigger $\alpha_t$. According to Prof. Lin,
  good $\alpha_t$ means smaller $\epsilon_t$, means larger $\diamondsuit_t$.
* Original designer set

$$
\alpha_t = ln(\diamondsuit_t)
$$

* Bad $g_t$ with $\epsilon_t = 1/2$ will have $\alpha_t = 0$.
* Super $g_t$ with $\epsilon_t = 0$ will have $\alpha_t = \inf$
* 我又有个问题了这里的$\epsilon_t$ 并不是原来的$E_{in}$呀。
* Adaptive boosting:
    * weak base algorithm (student)
    * optimal re-weighting factor $\diamondsuit_t$ (teacher)
    * 'magic' linear aggregation $\alpha_t$ (Class)

### Adaptive Boosting (AdaBoost) Algorithm

* See slides 208, pg 17.

### Theoretical Guarantee of AdaBoost

* VC bound

$$
E_{out}(G) \le E_{in}(G) + O(\sqrt
{O(d_{vc}(\mathcal{H}) \cdot T logT) \cdot \frac{logN}{N} }
)
$$

* The $d_{vc}(\mathcal{H})$ is for $g$, and when they aggregate together,
  The $d_{vc}$ is $d_{vc}(\mathcal{H}) \cdot T logT$
* First term can be small, from the original paper:
  $E_{in}(G) = 0$ after $T = O(log N)$ iterations if $\epsilon_t \le \epsilon < 1/2$ always .
* Second term can be small: overall $d_{vc}$ grows "slowly" with T
* boosting view of AdaBoost: if A is weak but always slightly better than random
  $\epsilon_t \le \epsilon < 1/2$, then (AdaBoost + A) can be strong ($E_{in}(G) = 0$)
  and $E_{out} small$.

## Adaptive Boosting in Action

### Decision stump

* Want a 'weak' base learning algorithm $\mathcal{A}$, which leads to $\epsilon < 1/2$ .
* A popular choice: decision stump

$$
h_{s,i,\theta}(x) = s \cdot \text{sign}(x_i - \theta)
$$

* physical meaning: vertical/horizontal lines in 2D
* efficient to optimize: $O(d \cdot N log N )$
* decision stump model: allows efficient minimization of $E_{in}^u$, but too weak by it self.
* Adaboost + decision stump: non-linear yet efficient.

### AdaBoost-stump in application

* Real-time face detection
* feature selection achieved through Adaboost
* linear aggregation to rule out non-face earlier

# Lecture 9 Decision Tree

## Decision tree hypothesis

### What we have done

* Blending: aggregate after getting $g_t$;
* Learning: aggregate as well as getting $g_t$;

| aggregation type | blending | learning |
|:-----------:|:------:|:---:|
| uniform     | voting/averaging | Bagging |
| non-uniform | linear           | Adaboost |
| conditional | stacking         | Decision tree |

* Decision tree: a traditional learning model that realizes conditional aggregation

### Decision Tree for Watching MOOC Lectures

$$
G(x) = \sum_{t=1}^T
q_t(x) \cdot g_t(x)
$$

* base hypothesis $g_t(x)$ leaf at end of path $t$,
  a constant here
* condition $q_t(x)$: is $x$ on path $t$?
* Usually with simple internal nodes
* Arguably one of the most human-mimicking models

### Recursive View of Decision Tree

* Path view

$$
G(x) = \sum_{t=1}^T [ x \text{ on path } t ] \cdot \text{leaf}_t(x)
$$

* Recursive View

$$
G(x) = \sum_{c=1}^C [b(x) = c] \cdot G_c(x)
$$

* G(x): full tree hypothesis
* b(x): branching criteria
* $G_c(x)$: sub-tree hypothesis at c-th branch
* tree = (root, sub-trees)

### Disclaimers about Decision Tree

* human-explainable
* simple
* efficient in prediction and training
* heuristic: little theoretical explanations
* heuristics: confusing to beginners
* arguably no single representative algorithm

## Decision Tree Algorithm

### A basic Decision Tree Algorithm

$$
G(x) = \sum_{c=1}^C [b(x) = c] \cdot G_c(x)
$$

* Four choices:
    * number of branches
    * branching criteria
    * termination criteria
    * base hypothesis

### Classification and Regression Tree (C&RT)

$\mathcal{D} = \{(x_n, y_n)\}_{n=1}^N$

* number of branches: 2
* base hypothesis: $g_t(x) = E_{in}$-optimal constant
    * classification: majority vote
    * regression: average of $y_n$
* branching criteria: decision stump, branch by purifying

$$
b(x) = \underset{\text{decision stump } h(x)}{argmin}
\sum_{c=1}^2
|D_c \text{ with } h| \cdot \text{impurity}(D_c \text{ with } h)
$$

### Impurity Functions

* Regression

$$
\text{impurity}(\mathcal{D}) =
1/N \sum (y_n - \hat{y} )
$$

* Classification

$$
\text{impurity}(\mathcal{D}) =
1/N \sum (y_n \ne \hat{y} )
$$

* for classification, we can also use Gini index
  the intuition is consider all $k$ together, the distribution
  of all $k$ is considered in the Gini index.
  
$$
1 - 
\sum_{k=1}^{K}
(\frac{\sum_{n=1}^N[y_n = k]}{N})^2
$$

* Popular choices: Gini for classification, regression error for regression.

### termination in C&RT

* all $y_n$ the same, all $x_n$ the same.

## Decision Tree Heuristic in CR&T

### Basic CR&T

* Easy handle binary classfication, regression and multi-class classification

### Regularization by pruning

* fully grown tree: $E_{in}(G) = 0$ if all $x_n$ different
* but overfit because low-level trees built with small $D_c$
* need a regularizer, say $\Omega(G)$ = Number of Leaves(G)
* want regularized decision tree, called pruned decision tree:

$$
\underset{\text{all possible } G}{\text{argmin}}
E_{in}(G) +
\lambda \Omega(G)
$$

* Cannot enumerate computationally, in practice
    * $G^{(0)}$ = fully-grown tree
    * $G^{(i)} = \text{argmin}_G E_{in}(G)$ such that G is one leaf removed from $G^{(i-1)}$
* Systematic choice of $\lambda$? Validation

### Branching on Categorical Feature

* numerical feature, e.g. blood pressure
* branching for numerical: decision stump
* Categorical features, e.g. major sympton
* branching for categorical

$$
b(x) = [x_i \in S] + 1
\\
S \subset \{1, 2, \dots, K\}
$$

* decision trees handles categorical features easily
* 可以问一下老师，subset可以有$2^n$那么多，怎么选呢？

### Missing Features by surrogate Branch

* possible $b(x) = [weight \le 50\ kg]$

* if weight missing during prediction: surrogate branch
    * maintain surrogate branch $b_1(x), \dots \approx \text{best branch } b(x)$ during training
    * allow missing feature for b(x) during prediction by using surrogate instead
* 可以问一下老师或者自己研究一下surrogate到底怎么做的
* CR&T handles missing feature easily
    
## Decision tree in action

* C&RT: 'divide and conquer'
* human-explainable
* multiclass easily
* categorical features easily
* missing features easily
* efficient non-linear training (and testing)
* almost no other learning model shares all such specialities

## Summary

* Decision Tree Hypothesis: express path-conditional aggregation
* Decision Tree Algorithm: recursive branching until termination to base
* Decision Tree Heuristics in C&RT: pruning, categorical branching, surrogate
* Decision Tree in Action: explainable and efficient
* next: aggregation of aggregation.

# Lecture 10 Random Forest

## Random Forest Algorithm

### Recall: Bagging and Decision Tree

* Bagging: function Bag(D, A)
* For $t = 1, 2, \dots, T$

1. request size-$N'$ data $\tilde{D}_t$ by bootstrapping with $D$
2. obtain base $g_t$ by $A(\tilde{D}_t)$

* return G = uniform({$g_t$})
* reduce variance by voting and averaging

* Decision Tree function DTree(D)
* if termination returns base $g_t$
* else

1. learn $b(x)$ and split D to $D_c$ by $b(x)$
2. build $G_c \leftarrow Dtree(D_c)$
3. return $G(x) = \sum_{c=1}^C [b(x) = c] \cdot G_c(x)$

* Large variance especially if fully-grown
* putting them together (撒尿虾＋牛丸＝撒尿牛丸) aggregation of aggregation

### Random Forest

* random forest = bagging + fully-grown C&RT decision tree
* function RandomForest(D)
* For $t = 1, 2, \dots, T$

1. request size-$N'$ data $\tilde{D}_t$ by bootstrapping with $D$
2. obtain base $g_t$ by DTree($\tilde{D}_t$)

* return G = uniform({$g_t$})
* highly parallel/efficient to learn
* inherits procs of C&RT
* eliminates cons of fully-grown tree

### Diversifying by Feature Projection

* recall: data randomness for diversity in bagging, randomly sample $N'$ examples
  from $\mathcal{D}$
* another possiblity for diversity: random sample $d'$ features from $x$

1. When sampling index $i_1, i_2, \dots, i_{d'}: \phi(x) = (x_{i_1}, x_{i_2}, \dots, x_{i_{d'}})$
2. $\mathcal{Z} \in R^{d'}$: a random subspace of $\mathcal{X} \in R^{d'}$
3. offten $d' << d$, efficient for large d, can be generally applied on other models.
4. original RF re-sample new subspace for each b(x) in C&RT.
5. RF = bagging + random-subspace C&RT

### Diversifying by Feature Expansion

* Random sample d' features from $x: \phi(x) = P \cdot x$ with row i of P sampled randomly $\in$ natural basis.
    * 给个例子如下：$d = 3, d' = 2$
    
```{r}
P <- matrix(c(0,0,1,0,0,1), 2, 3)
P
```

* more powerful features for diversity: row i other than nature basis
* projection (combination) with random row $p_i$ of $P$: $\phi_i(x) = p_i^Tx$
* often consider low-dimensional projection: only $d''$ non-zero components in $p_i$
* include random - subspace as special case: $d'' = 1, p_i \in$ nature basis
* original RF consider $d'$ random low-dimensional projections for each $b(x)$ in C&RT
* RF = bagging + random-combination C&RT - randomness everywhere
* 可以问一下老师：注意这里，树生成了之后，在预测的时候也要做相同的projectoin.

## Out-Of-Bag Estimate

### Bagging Revisited

* Bagging: function Bag(D, A)
* For $t = 1, 2, \dots, T$

1. request size-$N'$ data $\tilde{D}_t$ by bootstrapping with $D$
2. obtain base $g_t$ by $A(\tilde{D}_t)$

* return G = uniform({$g_t$})

* if $(x_i, y_i)$ is not used for obtaining $g_t$, then it's called out-of-bag (OOB) examples
  of $g_t$

### Number of OOB Examples

* OOB $\Longleftrightarrow$ not sampled after $N'$ drawings.

if $N' = N$

1. probability for $(x_n, y_n)$ to be OOB for $g_t$: $(1-1/N)^N$
2. if N large:

$$
(1-1/N)^N
= \frac{1}{(\frac{N}{N-1})^N}
= \frac{1}{(1+\frac{1}{N-1})^N}
\approx \frac{1}{e}
$$

3. OOB size per $g_t \approx 1/e \cdot N$ which is about 1/3 .

### OOB versus Validation

* OOB examples are like $D_{val}$: enough random examples unused during training.
* Use OOB examples to validate $g_t$? YES, but not needed, because we don't care about
  $g_t$. We care more about $G$.
* Use OOB examples to validate G?
* With $G_n^-$ contains only trees that $x_n$ is OOB of, say, $x_n$ is OOB for $g_2, g_3, g_T$ .
Then $x_n$ can be the validation for $G_n^- = 1/3 \cdot (g_2 + g_3 + g_T)$

$$
E_{oob}(G) = \frac{1}{N} \sum_{n=1}^N err(y_n, G_n^-(x_n))
$$

* $E_{oob}$: self-validation of bagging and RF.

### Model selection by OOB error

* RF: by Best $E_{oob}$
    * use $E_{oob}$ for self-validation - of RF parameters such as $d''$
    * no re-training needed, as we did in validation
* $E_{oob}$ often accurate in practice

## Feature selection

for $x = (x_1, \dots, x_d)$, want to remove 

* redundant features: like keeping one of 'age' and 'full birthday'.
* irrelevant features: like insurance type for cancer prediction.

and only 'learn' subset-transform $\phi(x) = (x_{i_1}, \dots, x_{i_{d'}})$
with $d' < d$ for $g(\phi(x))$

* advantages:

1. efficient: simpler hypothesis and shorter prediction time
2. generalization: feature noisy removed
3. interpretability

* disadvantages:

1. computation: 'combinational' optimization in training
2. overfit: 'combinational' selection
3. mis-interpretability, 只发现关联性，没发现因果性。

* decision tree：a rare model with built-in feature selection

### Feature selection by importance

* idea: calculate the importance for each feature, select the most important $d'$ of them.

importance by linear model:

$$
score = w^T x = \sum_{i=1}^d w_i x_i
$$

* intuitive estimate: importance(i) = $|w_i|$ with some 'good' w.
* getting 'good' w: learned from data, just use linear model.
* non-linear models? often much harder.

### Feature importance by Permutation Test

* idea: random test -  if feature i needed, 'random' values of $x_{n,i}$ degrades performance
* which random values?
    * uniform, Gaussian, ... $P(x_i)$ changed
    * bootstrap, permutation: $P(x_i)$ approximately remained.
* permutation test: importance(i) = performance($D$) - performance($D^{(p)}$)
* It's a general statistical tool for arbitrary non-linear models like RF

### Feature importance in Original RF

1. performance($D^{(P)}$): needs re-training and validation in general
2. 'escaping' validation? OOB in RF
3. Original RF solution: importance(i) = $E_{oob}(G) - E_{oob}^{(p)}(G)$, where
    $E_{oob}^{(p)}(G)$ comes from replacing each request of $x_{n_i}$ by a permuted OOB value.
* 老师也说，这个介绍比较简单。可以参考原始文献[here](https://link.springer.com/article/10.1023%2FA%3A1010933404324)。

## Random Forest in Action

* With 1000 trees, smooth and large-margin-like boundary with many trees.
* Easy yet robust nonlinear model
* For a complicated and noisy data set, single decision tree trys to overfit.
* 可以学习一下老师这样做一个这样的动画，应该可以假设理解。

### How many trees needed?

* almost every theory: the more, the 'better', assuming good

$$
\bar{g} = \text{lim}_{T \rightarrow \infty} G
$$

* KDDCup 2013: predicting author-paper relation
* Depending on random seed, for thousands of trees, $E_{val} \in [0.015, 0.019]$
* $E_{out} \in [0.014, 0.019]$ for top 20 teams.
* decision: take 12000 trees with seed 1.
* cons of RF: may need lots of trees if the whole random process too unstable. Should
  double check stability of G to ensure enough trees.
  
## Summary

* Random Forest Algorithm: bag of trees on randomly projected subspaces
* Out-of-bag Estimate: self-validation with OOB examples
* Feature selection: permutation test for feature importance
* Random Forest in Action: 'smooth' boundary with many trees

# Lecture 11 Adaptive Boosted Decision Tree

## Adaptive Boosted Decision Tree

### From Random Forest to AdaBoost-DTree

* function RandomForest(D)
* For $t = 1, 2, \dots, T$

1. request size-$N'$ data $\tilde{D}_t$ by bootstrapping with $D$
2. obtain base $g_t$ by DTree($\tilde{D}_t$)

* return G = uniform({$g_t$})

* function AdaBoost-DTree(D)
* For $t = 1, 2, \dots, T$

1. re-weight data by $u^{(t)}$
2. obtain tree $g_t$ by Dtree($D, u^{(t)}$)
3. calculate vote $\alpha_t$ of $g_t$

* return G = LinearHypo($g_t, \alpha_t$)

* 我的问题是decision tree的算法好像和weight没什么关系。

### Weighted Decision Tree Algorithm

* Weighted Algorithm

$$
\text{minimize (regularized) }
E_{in}^u(h) = 1/N \sum_{n=1}^N u_n \cdot \text{err} (y_n, h(x_n)) \\
$$

1. If using existing algorithm as black-box (no modifications), to get $E_{in}^u$
   approximately optimized
2. "weighted" algorithm in bagging
    1. weights u expressed by bootstrap-sampled copies.
    2. request size-N' data $\tilde{D_t}$ by bootstrapping with $D$
3. A General Randomized Base Algorithm
    1. weights u expressed by sampling proportional to $u_n$
    2. request size-N' data $\tilde{D_t}$ by sampling $\propto u$ on D

* Adaboost-Dtree: often via AdaBoost + sampling $\propto u^{(t)}$ + Dtree($\tilde{D_t}$)
  without modifying DTree.
  
### Weak Decision Tree Algorithm

* Adaboost:

$$
\alpha_t = ln \diamondsuit_t = ln\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}
$$

With weighted error rate $\epsilon_t$

* If fully grown tree, we will get $\alpha_t = \infty$. That's autocracy.
* Need: pruned tree trained on some $x_n$ to be weak.
    * pruned: usual pruning, or just limiting tree height
    * some: sampling $\propto u$
* Adaboost-Dtree: often via AdaBoost + sampling $\propto u^{(t)}$ + pruned Dtree($\tilde{D_t}$)

### AdaBoost with Extremely-Pruned Tree

1. What if DTree with hight $\le 1$ (extremely pruned)?

* DTree (C&RT) with height $\le$ 1, learn branching criteria

$$
b(x) = \underset{\text{decision stump } h(x)}{\text{argmin}}
\sum_{c=1}^2
|D_c \text{ with } h| \cdot \text{impurity}(D_c \text{ with } h)
$$

* If impurity = binary classification error, it becames a decision stump.
* So AdaBoost-Stump = special case of AdaBoost-DTree.

## Optimization View of AdaBoost

### Example Weights of AdaBoost

$$
\begin{align*}
u_{n}^{(t+1)} & = 
\begin{cases}
u_{n}^{(t)} \cdot \diamondsuit_t & \quad \text{if incorrect} \\
u_{n}^{(t)} / \diamondsuit_t & \quad \text{if correct} \\
\end{cases} \\
& = u_n \diamondsuit_t ^ {-y_n g_t (x_n)} = u_n \cdot exp(-y_n \alpha_t g_t (x_n))
\end{align*} \\
u_{n}^{(T+1)} = u_{n}^{(1)} \cdot \prod_{t=1}^T exp (-y_n \alpha_t g_t (x_n))
= \frac{1}{N} \cdot exp \left( -y_n \sum_{t=1}^T \alpha_t g_t (x_n) \right)
\\
G(x) = \text{sign} \left( \sum_{t=1}^T \alpha_t g_t (x_n) \right) \\
\sum_{t=1}^T \alpha_t g_t (x_n) \text{ : voting score of } \{g_t\} \text{ on } x
$$

1. Adaboost: $u_{n}^{(T+1)} \propto exp (-y_n (\text{ voting score on } x_n))$

### voting score and margin

* linear blending = LinModel + hypothesis transform + without constraints

$$
G(x_n) = \text{sign} \left( 
\sum_{t=1}^T 
\underbrace{\alpha_t}_{w_i}
\underbrace{g_t (x_n)}_{\phi_i(x_n)} \right)
$$

* and hard-margin SVM margin

$$
\frac
{y_n \cdot (w^T \phi_i(x_n) + b)}
{||w||}
$$

* $y_n$ (voting score) = signed & unnormalized margin
* $\Leftrightarrow$ want $y_n$ (voting score) positive & large
* $\Leftrightarrow$ exp( -$y_n$(voting score) ) small
* $\Leftrightarrow u_n^{(T+1)}$ small
* claim: AdaBoost decreases $\sum_{n=1}^N u_n^{(t)}$

### AdaBoost Error Function

1. claim: AdaBoost decreases $\sum_{n=1}^N u_n^{(t)}$ and thus somewhat minimizes

$$
\sum_{n=1}^{N} u_{n}^{(T+1)}
= \frac{1}{N} \cdot \sum_{n=1}^{N} exp \left( -y_n \sum_{t=1}^T \alpha_t g_t (x_n) \right)
\\
$$

2. linear score $s = \sum_{t=1}^T \alpha_t g_t (x_n)$
3. $\text{err}_{0/1}(s, y) = [ys \le 0]$
4. $\text{err}_{ADA}(s, y) = exp(-ys)$: upper bound of $\text{err}_{0/1}$
    , called exponential error measure.

```{r, echo=FALSE}
library(tidyverse)
x <- seq(-3, 3, 0.01)
err01 <- as.numeric((x < 0))
errada <- exp(-x)
ggplot() +
  geom_line(mapping = aes(x = x, y = err01)) +
  geom_line(mapping = aes(x = x, y = errada), colour = "red") +
  ylim(c(0, 6)) + ylab("err") + xlab("ys")
```

5. $\widehat{err}_{ADA}$: algorithmic error measure by convex upper bound of 
   $\text{err}_{0/1}$
   
### Gradient Descent on AdaBoost Error Function

1. recall: gradient desecent, at iteration t

$$
\underset{||v|| = 1}{\text{min}} \quad
E_{in}(w_t + \eta v)
\approx
\underbrace{E_{in}(w_t)}_{\text{known}} +
\underbrace{\eta}_{\text{given position}}
v^T
\nabla
\underbrace{E_{in}(w_t)}_{\text{known}}
$$

2. at iteration t, to find $g_t$, solve

$$
\begin{align*}
\underset{h}{\text{min}} \quad
\widehat{E}_{ADA} \quad
&= \frac{1}{N} \cdot \sum_{n=1}^{N} exp \left( -y_n \sum_{\tau =1}^{t-1} \alpha_{\tau} g_{\tau} (x_n) + \eta h(x_n) \right) \\
&= \sum_{n=1}^{N} u_{n}^{(t)} exp (-y_n \eta h(x_n)) \\
& \overset{\text{taylor}}{\approx} \sum_{n=1}^{N} u_{n}^{(t)} (1 -y_n \eta h(x_n) )
= \sum_{n=1}^{N} u_{n}^{(t)} - \eta \sum_{n=1}^{N} u_{n}^{(t)} y_n h(x_n)
\end{align*}
$$

3. good $h$: minimize $\sum_{n=1}^{N} u_{n}^{(t)} (-y_n h(x_n))$

### Learning Hypothesis as Optimization

1. finding good $h$ (function direction) $\Leftrightarrow$ minimize
    $\sum_{n=1}^{N} u_{n}^{(t)} ( -y_n h(x_n) )$
    
2. for binary classification, where $y_n$ and $h(x_n)$ both $\in \{-1,+1\}$

$$
\begin{align*}
\sum_{n=1}^{N} u_{n}^{(t)} (-y_n h(x_n)) &=
\sum_{n=1}^{N} u_{n}^{(t)} \cdot
\begin{cases}
-1 & \text{if } y_n = h(x_n)\\
+1 & \text{if } y_n \ne h(x_n)\\
\end{cases}
\\
&= -\sum_{n=1}^{N} u_{n}^{(t)} +
\sum_{n=1}^{N} u_{n}^{(t)} \cdot
\begin{cases}
0 & \text{if } y_n = h(x_n)\\
2 & \text{if } y_n \ne h(x_n)\\
\end{cases}
\\
&= -\sum_{n=1}^{N} u_{n}^{(t)} + 2 \cdot E_{in}^{u(t)}(h)
\end{align*}
$$

3. Who minimizes $E_{in}^{u(t)}(h)$ ? $\mathcal{A}$ in AdaBoost!
4. $\mathcal{A}: \text{good } g_t = h$ for 'gradient descent'

### Deciding blending weight as optimization

1. AdaBoost finds $g_t$ by approximately
    $\underset{h}{\text{min}} \widehat{E}_{ADA} = \sum_{n=1}^{N} u_{n}^{(t)} exp (-y_n \eta h(x_n))$
2. After finding $g_t$, how about
    $\underset{\eta}{\text{min}} \widehat{E}_{ADA} = \sum_{n=1}^{N} u_{n}^{(t)} exp (-y_n \eta g_t(x_n))$
3. Optimal $\eta_t$ somewhat 'greedily faster' than fixed (small) $\eta$ - called steepest
    descent for optimization
4. Two cases inside summation:
    * $y_n = g_t(x_n): u_n^{t}exp(-\eta)$ (correct)
    * $y_n = g_t(x_n): u_n^{t}exp(+\eta)$ (incorrect)

5. $\widehat{E}_{ADA} = (\sum_{n=1}^{N} u_{n}^{(t)}) \cdot ((1-\epsilon_t) exp(-\eta) + \epsilon_t exp(+\eta))$
6. $\frac{\partial \widehat{E}_{ADA}}{\partial \eta} = 0$, steepest
$\eta = \ln(\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}) = \alpha_t$
7. Adaboost: Steepest descent with approximate functional gradient

## Gradient Boosting

### Gradient Boosting for Arbitrary Error Function

* AdaBoost with binary-output hypothesis

$$
\underset{\eta}{\text{min}}
\underset{h}{\text{min}}
\frac{1}{N} \cdot \sum_{n=1}^{N} \exp \left( -y_n 
\left( 
\sum_{\tau =1}^{t-1} \alpha_{\tau} g_{\tau} (x_n) + \eta h(x_n)
\right)
\right)
$$

* 同样的概念能不能对不同的error function做最佳化？
* Gradient boost, as long as the error function is smooth.

$$
\underset{\eta}{\text{min}}
\underset{h}{\text{min}}
\frac{1}{N} \cdot \sum_{n=1}^{N} \text{err} 
\left( 
\sum_{\tau =1}^{t-1} \alpha_{\tau} g_{\tau} (x_n) + \eta h(x_n)
, y_n
\right)
$$

* with any hypothesis $h$, (usually real-output hypothesis)
* allows extension to different err for regression / soft classification / etc.

### Gradient Boost for Regression

$$
\underset{\eta}{\text{min}}
\underset{h}{\text{min}}
\frac{1}{N} \cdot \sum_{n=1}^{N} \text{err} 
\left( 
\underbrace{\sum_{\tau =1}^{t-1} \alpha_{\tau} g_{\tau} (x_n)}_{s_n} + \eta h(x_n)
, y_n
\right)
\text{ with err} (s,y) = (s-y)^2
$$

* 

$$
\begin{align*}
\underset{h}{\text{min}} \dots
 & \overset{\text{taylor}}{\approx} 
 \frac{1}{N} \cdot \sum_{n=1}^{N} 
\underbrace{\text{err} (s_n, y_n)}_{\text{constant}}
+
\frac{1}{N} \sum_{n=1}^{N} \eta h(x_n)
\left. \frac{\partial \text{err} (s, y_n) }{ \partial s } \right|_{s = s_n} \\
& = \underset{h}{\text{min}} \text{ constants } + \frac{\eta}{N}
\sum_{n=1}^{N} h(x_n) \cdot 2(s_n - y_n)
\end{align*}
$$

1. Naive solution $h(x_n) = -\infty(s_n - y_n)$ if no constraint on $h$

### Learning Hypothesis as Optimization

$$
\underset{h}{\text{min}} \text{ constants } + \frac{\eta}{N}
\sum_{n=1}^{N} h(x_n) \cdot 2(s_n - y_n)
$$

1. magnitude of h does not matter: because $\eta$ will be optimized next
2. penalize large magnitude to avoid naive solution.

$$
\begin{align*}
\underset{h}{\text{min}}
& \quad \text{ constants } + \frac{\eta}{N}
\sum_{n=1}^{N} h(x_n) \cdot 2(s_n - y_n) + (h(x_n))^2 \\
& = \text{ constants } + \frac{\eta}{N}
\sum_{n=1}^{N} \text{ constant } + (h(x_n) - (y_n - s_n))^2
\end{align*}
$$

3. solution of penalized approximate functional gradient: squared-error regression on
  $\{(x_n, \underbrace{y_n-s_n}_{residual})\}$
4. GradientBoost for regression: find $g_t = h$ by regression with residuals.

### Deciding Blending Weight as Optimization

1. After finding $g_t=h$,
$$
\underset{\eta}{\text{min}}
\frac{1}{N} \cdot \sum_{n=1}^{N} \text{err} 
\left( 
\sum_{\tau =1}^{t-1} \alpha_{\tau} g_{\tau} (x_n) + \eta g_t(x_n)
, y_n
\right)
\text{ with err}(s,y) = (s-y)^2
$$

$$
\underset{\eta}{\text{min}}
\frac{1}{N} \cdot \sum_{n=1}^{N} 
( 
s_n + \eta g_t(x_n) - y_n
)^2
= \frac{1}{N} \sum_{n=1}^{N}
((y_n-s_n) - \eta g_t(x_n))^2
$$

2. one-variable linear regression on {($g_t$-transformed input, residual)}
3. GradientBoost for regression: $\alpha_t$ = optimal $\eta$ by $g_t$-transformed linear
  regression.
 
## Summary of Aggregation Models

### Map of Blending Models

1. blending: aggregate after getting diverse $g_t$

* uniform: simple voting/averaging of $g_t$
* non-uniform: linear model on $g_t$-transformed inputs
* conditional: nonlinear model on $g_t$-transformed inputs
* uniform for 'stability'
* non-uniform/conditional carefully for 'complexity'

### Map of Aggregation-Learning Models

Learning: aggregate as well as getting diverse $g_t$

* Bagging: diverse $g_t$ by bootstrapping; uniform vote by nothing
* AdaBoost: diverse $g_t$ by reweighting; linear vote by steepest search
* Decision Tree: diverse $g_t$ by data splitting; conditional vote by branching
* GradientBoost: diverse $g_t$ by residual fitting; linear vote by steepest search
* Boosting-like algorithms most popular 

### Map of Aggregation of Aggregation Models

* Random forest: Randomized bagging + 'strong' DTree
* AdaBoost Dtree: AdaBoost + 'weak' Dtree
* GradientBoost + 'weak' Dtree
* All three frequently used in practice

### Specialty of Aggregation Models

1. cure underfitting: $G(x)$ 'strong'.
2. aggregation => feature transform
3. cure overfitting: $G(x)$ 'moderate'.
4. aggregation => regularization
5. proper aggregation (a.k.a ensemble) => better performance

### Summary

* Adaptive Boosted Decision Tree: sampling and pruning for weak trees
* Optimization View of AdaBoost: functional gradient descent on exponential error
* Gradient Boosting: iterative steepest residual fitting
* Summary of Aggregation Models: some cure underfitting; some cure overfitting.
