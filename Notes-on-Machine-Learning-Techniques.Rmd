---
title: "Notes on Machine Learning Techniques"
author: "Yang Ge"
date: "November 13, 2015"
output:
  html_document:
    keep_md: false
    toc: true
---

```{r setoptions, echo=TRUE}
library(knitr)
opts_chunk$set(echo = TRUE, cache = TRUE)
library(dplyr)
```

# Lecture 1

## Course Introduction

**Course Design**

* From foundations to techniques

## Large-Margin Separating Hyperplane

**Linear Classification Revisited**

* PLA/pocket: $h(x) = \text{sign}(s)$
    * plausible err = 0/1 (small flipping noise) minimize specially

linear (hyperplane) classifiers:

$$
h(x) = \text{sign} (w^T x)
$$

**Which Line Is Best**

* PLA? depending on randomness
* VC bound? whichever you like!
    * $E_{out}(w) \le \underbrace{E_{in}(w)}_{0} + \underbrace{\Omega(\mathcal{H})}_{d_{VC} = d + 1}$
* You? rightmost? Possibly?

**Why Rightmost Hyperplane?**

* informal argument
    * if (Gaussian-like) noise on future $x \approx x_n$ (可能因为测量误差):
    * leftmost: a tiny noise will make $x$ cross decision boundary.
    * Difference between left and right figure: 测量误差容忍度.
* $x_n$ further from hyperplane $\iff$ tolerate more noise
    * In machine learning foundations, we know noise is the cause of overfitting.
    * Then if the line can tolerate more noise, the line is better.
* $\iff$ more robust to overfitting.
* distance to closest $x_n$ $\iff$ amount of noise tolerance $\iff$ robustness of
  hyperplane

* rightmost one: more robust because of larger distance to closest $x_n$.

**Fat Hyperplane**

* robust separating hyperplane: fat
    * far from both sides of examples
* robustness $\equiv$ fatness: distance to closest $x_n$

goal: find fattest separating hyperplane.

**Large-Margin Separating Hyperplane**

* $\underset{w}{\text{max}}$ fatness(w)
* subject to:
    * $w$ classifies every $(x_n, y_n)$ correctly
    * fastness(w) = $\underset{n = 1, \dots, N}{\text{distance}(x_n, w)}$

* fatness: formally called *margin*
* correctness: $y_{n} = \text{sign} (w^T x_{n})$

So we have the following:

* $\underset{w}{\text{max}}$ fatness(w)
* subject to:
    * every $y_{n} w^T x_{n} > 0$
    * margin(w) = $\underset{n = 1, \dots, N}{\text{distance}(x_n, w)}$

goal: find *Large-Margin* *separating* hyperplane.

## Standard Large-Margin Problem

**Distance to Hyperplane: Preliminary**

'shorten' $x$ and $w$

* distance needs $w_{0}$ and $(w_1, \dots, w_d)$ differently (to be derived)
* $b = w_0$, $w = [w_1, \dots, x_d]$; $x = [x_1, \dots, x_d]$

**Distance to Hyperplane**

want: distance $(x, b, w)$, with hyperplane $w^T x' + b = 0$

consider $x', x''$ on hyperplane:
1. $w^Tx' = -b$, $w^Tx'' = -b$
2. $w \perp$ hyperplane
$$
w^T (x'' - x') = 0
$$
3. distance = project $(x - x')$ to $\perp$ hyperplane.

$$
\text{distance}(x, b, w)
= \begin{vmatrix}
\frac{w^T}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}
} (x - x')
\end{vmatrix}
= \frac{1}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}
} \begin{vmatrix}
w^T x + b
\end{vmatrix}
$$

**Distance to Separating Hyperplane**

$$
\text{distance}(x, b, w)
= \frac{1}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}
} \begin{vmatrix}
w^T x + b
\end{vmatrix}
$$

* separating hyperplane: for every $n$

$$
y_{n}(w^T x_n + b) > 0
$$

* distance to separating hyperplane:

$$
\text{distance}(x, b, w)
= \frac{1}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}
} w^T x + b
$$

* $\underset{b,w}{\text{max}}$ \text{margin}(b, w)
* subject to:
    * every $y_{n}(w^T x_n + b) > 0$
    * $\text{margin}(b, w) = \underset{n=1, \dots, N}{min}\frac{1}{
      \begin{Vmatrix}
      w
      \end{Vmatrix}
    } y_n (w^T x_n + b)$

**Margin of Special Separating Hyperplane**

* $\underset{b,w}{\text{max}} \text{margin}(b, w)$
* subject to:
    * every $y_{n}(w^T x_n + b) > 0$
    * $\text{margin}(b, w) = \underset{n=1, \dots, N}{min}\frac{1}{
      \begin{Vmatrix}
      w
      \end{Vmatrix}
    } y_n (w^T x_n + b)$

* $w^Tx + b = 0$ same as $3w^Tx + 3b = 0$: scaling does not matter
* special scaling: only consider separating $(b, w)$ such that

$$
\underset{n=1, \dots, N}{\text{min}} y_n (w^T x_n + b) = 1 \implies
\text{margin}(b, w) = \frac{1}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}}
$$

We have new formula:

* $\underset{b,w}{\text{max}} \frac{1}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}}$
* subject to:
    * every $y_{n}(w^T x_n + b) > 0$
    * $\underset{n=1, \dots, N}{\text{min}} y_n (w^T x_n + b) = 1$

We could further simplify to:

* $\underset{b,w}{\text{max}} \frac{1}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}}$
* subject to:
    * $\underset{n=1, \dots, N}{\text{min}} y_n (w^T x_n + b) = 1$

**Standard Large-Margin Hyperplane Problem**

* original constraints: $\underset{n=1, \dots, N}{\text{min}} y_n (w^T x_n + b) = 1$
    * want: optimal $(b, w)$ here (inside)
* necessary constraints: $y_n (w^T x_n + b) \ge 1$ for all $n$

* if optimal $(b, w)$ outside, e.g. $y_n (w^T x_n + b) > 1.126$ for all $n$
    * can scale $(b, w)$ to "more optimal" $(\frac{b}{1.126}, \frac{w}{1.126})$
      (contradiction!)

We have another new constraints:

* $\underset{b,w}{\text{max}} \frac{1}{
  \begin{Vmatrix}
  w
  \end{Vmatrix}}$
* subject to:
    * $\underset{n=1, \dots, N}{\text{min}} y_n (w^T x_n + b) \ge 1$ for all $n$

final change: max $\implies$ min, remove $\sqrt{}$

* $\underset{b,w}{\text{min}} \frac{1}{2} w^T w$
* subject to:
    * $\underset{n=1, \dots, N}{\text{min}} y_n (w^T x_n + b) \ge 1$ for all $n$

## Support Vector Machine

**Solving a Particular Standard Problem**

* $\underset{b,w}{\text{min}} \frac{1}{2} w^T w$
* subject to:
    * $y_n (w^T x_n + b) \ge 1$ for all $n$

```{r}
data <- data.frame(x0=c(0, 2, 2, 3), x1=c(0, 2, 0, 0), y=c(-1, -1, 1, 1))
plot(data$x0, data$x1, type = "n", xlim = c(-1, 4), ylim = c(-1, 4), asp = 1, xlab = "x_1", ylab = "x_2")
points(data$x0[1:2], data$x1[1:2], pch = 4, col = "red")
points(data$x0[3:4], data$x1[3:4], pch = 1, col = "blue")
abline(a = -1, b = 1)
abline(a = 0, b = 1, lty = 2)
abline(a = -2, b = 1, lty = 2)
```

* $g_{SVM} = \text{sign}(x_1 - x_2 - 1)$

**Support Vector Machine (SVM)**

* optimal solution: $(w_1 = 1, w_2 = -1, b = -1)$
* $\text{margin}(b, w) = \frac{1}{\begin{Vmatrix}w\end{Vmatrix}} = \frac{1}{\sqrt{2}}$
* examples on boundary: 'locates' fattest hyperplane. Other examples: not needed.
* call boundary example *Support Vector*

*Support vector* machine (SVM): learn fattest hyperplanes (with help of support vectors)

**Solving General SVM**

$$
\begin{split}
\underset{b,w}{\text{min}} &\quad \frac{1}{2} w^T w \\
\text{subject to } &\quad y_n (w^T x_n + b) \ge 1
\end{split}
$$

* not easy manually, of course.
* gradient descent? not easy with constraints
* luckily:
    * (convex) quadratic objective function of $(b, w)$
    * linear constraints of $(b, w)$
* quadratic programming (QP): 'easy' optimization problem.

**Quadratic Programming**

$$
\begin{split}
\text{optimal}(b, w) &=? \\
\underset{b,w}{\text{min}} \quad & \frac{1}{2} w^T w \\
\text{subject to } \quad & y_n (w^T x_n + b) \ge 1 \\
& \text{for } n = 1, 2, \dots, N \\
\end{split}
$$

* Generic form of quadratic programming

$$
\begin{split}
\text{optimal } u \rightarrow & QP(Q, p, A, c) \\
\underset{u}{\text{min}} \quad & \frac{1}{2} u^T Q u + p^T u \\
\text{subject to } \quad & a_m^T u \ge c_m, \\
& \text{for } m = 1, 2, \dots, M \\
\end{split}
$$

* objective function:
    * $u = \begin{bmatrix}
      b \\
      w
      \end{bmatrix}$
    * $Q = \begin{bmatrix}
      0 & 0^T \\
      0 & I
    \end{bmatrix}$
    * $p = 0_{d+1}$
* constraints:
    * $a_m = y_n[1, x_n^T]$
    * $c_n  = 1$
    * $M = N$

* SVM with general QP solver: easy if you've read the manual.

**SVM with QP Solver**

* Linear Hard-Margin SVM Algorithm:
    1. $Q, p, a^T_n, c_n$ as above
    2. $\begin{bmatrix}
      b \\
      w
    \end{bmatrix} \leftarrow \text{QP}(Q, p, A, c)$
    3. return $b \& w$ as $g_{svm}$
* Hard-Margin: nothing violate 'fat boundary'
* linear $x_n$
* want non-linear? $z_n = \Phi(x_n)$ - remember?

## Reasons Behind Large-Margin Hyperplane

**Why Large-Margin Hyperplane**

$$
\begin{split}
\underset{b,w}{\text{min}} \quad & \frac{1}{2} w^T w \\
\text{subject to } \quad & y_n (w^T x_n + b) \ge 1 \text{ for all } n
\end{split}
$$

|  | minimize | constraint |
| :-------------: | :-------------: | :-------------: |
| regularization | $E_{in}$ | $w^Tw \le C$ |
| SVM | $w^Tw$ | $E_{in} = 0$ [and more] |

SVM (Large-Margin hyperplane):
'weight-decay regularization' within $E_{in} = 0$

**Large-Margin Restricts Dichotomies**

consider 'Large-Margin algorithm' $\mathcal{A}_{\rho}$: either returns $g$ with
margin($g$) $\ge \rho$, or 0 otherwise.

* $\mathcal{A}_0$: like PLA $\implies$ shatter 'general' 3 inputs
* $\mathcal{A}_{1.126}$: more strict than SVM $\implies$ cannot shatter any 3 inputs

fewer dichotomies $\implies$ smaller VC dim $\implies$ better generalization.

**VC Dimenson of Large-Margin Algorithm**

fewer dichotomies $\implies$ smaller 'VC dim.'

* considers $d_{VC}(\mathcal{A}_{\rho})$ [data dependent, need more than VC]
* instead of $d_{VC}(\mathcal{H})$ [data independent, covered VC]

* $d_{VC}(\mathcal{A}_{\rho})$ when $\mathcal{X}$ = unit circle in $\mathbb{R}^2$
* $\rho = 0$: just perceptrons ($d_{VC} = 3$)
* $\rho = \frac{\sqrt{3}}{2}$: cannot shatter any 3 inputs ($d_{VC} < 3$)
    * some inputs must be of distance $\le \sqrt{3}$

generally, when $\mathcal{X}$ in radius-R hyperball:

$$
d_{VC}(\mathcal{A}_{\rho}) \le \text{min} (\frac{R^2}{\rho^2}, d) + 1
\le \underbrace{d+1}_{d_{VC}(\text{perceptron})}
$$

**Benefits of Large-Margin Hyperplanes**

|   | Large-Margin Hyperplanes | Hyperplanes | Hyperplanes + feature transform $\Phi$
| :-------------: | :-------------: | :-------------: | :-------------: |
| #       | even fewer | not many | many |
| boundary | simple | simple | sophisticated |

* not many: good for $d_{VC}$ and generalization
* sophisticated: good for possibly better $E_{in}$

* a new possiblility: non-linear SVM

|   | Large-Margin Hyperplanes + numerous feature transform $\Phi$ |
| :-------------: | :-------------: |
| # | not many       |
| boundary | sophisticated |

## Conclusions

* Large-Margin Separating Hyperplane
    * intuitively more robust against noise
* Standard Large-Margin Problem
    * minimize 'length of w' at special separating scale
* Support Vector Machine
    * 'easy' via quadratic programming
* Reasons Behind Large-Margin Hyperplane
    * fewer dichotomies and better generalization


# Lecture 2 Dual Support Vector Machine

## Motivation of Dual SVM

**Non-Linear Support Vector Machine Revisited**

$$
\begin{split}
\underset{b,w}{\text{min}} \quad & \frac{1}{2} w^T w \\
\text{subject to } \quad & y_n (w^T \underbrace{z_n}_{\Phi(x_n)} + b) \ge 1 \text{ for all } n
\end{split}
$$

Non-Linear Hard-Margin SVM

1. $Q = \begin{bmatrix}
      0 & 0^T_{\tilde{d}} \\
      0_{\tilde{d}} & I_{\tilde{d}}
    \end{bmatrix}$; $p = 0_{\tilde{d}+1}$; $a_n = y_n[1, z_n^T]$; $c_n  = 1$
2. $\begin{bmatrix}
b \\
w
\end{bmatrix} \leftarrow \text{QP}(Q, p, A, c)$
3. return $b \in \mathbb{R}$ & $w \in \mathbb{R}^{\tilde{d}}$ with
   $g_{svm}(x) = \text{sign}(w^T \Phi(x) + b)$

* demanded: not many (Large-Margin), but sophisticated boundary (feature transform)
* QP with $\tilde{d}+1$ variables and $N$ constraints - challenging if $\tilde{d}$,
  or infinite?!

Goal: SVM without dependence on $\tilde{d}$

**TODO: SVM without $\tilde{d}$**

* Original SVM
    * $\tilde{d} + 1$ variables
    * $N$ constraints
* 'Equivalent' SVM
    * $N$ variables
    * $N+1$ constraints

Warning: Heavy Math!!!

* introduce some necessary math without rigor to help understand SVM deeper
* 'claim' some results if details unnecessary - like how we 'claimed' Hoeffding

'Equivalent' SVM: based on some dual problem of Original SVM

**Key Tool: Lagrange Multipliers**

| Regularization by Constrained-Minimizing $E_{in}$ | Regularization by Minimizing E_{aug} |
| :-------------: | :-------------: |
| $\underset{w}{\text{min}} E_{in}(w) \text{s.t. } w^Tw \le C$ | $\underset{w}{\text{min}} E_{aug}(w) =E_{in}(w) + \frac{\lambda}{N} w^T w$ |

* $C$ equivalent to some $\lambda \ge 0$ by checking optimality condition

$$
\nabla E_{in}(w) + \frac{2\lambda}{N} w = 0
$$

* regularization: view $\lambda$ as given parameter instead of $C$, and solve 'easily'
* dual SVM: view $\lambda$ as unknown given the constraints, and solve them as
variables instead.

how many $\lambda$'s as variables? $N$.

**Starting Point: Constrained to 'Unconstrained'**

$$
\begin{split}
\underset{b,w}{\text{min}} \quad & \frac{1}{2} w^T w \\
\text{subject to } \quad & y_n (w^T \underbrace{z_n}_{\Phi(x_n)} + b) \ge 1 \text{ for all } n
\end{split}
$$

Lagrange Function: with Lagrange Multipliers $\alpha_n$

$$
\mathcal{L}(b, w, \alpha)
= \underbrace{\frac{1}{2} w^T w}_{\text{objective}}
+ sum_{n=1}^{N} \alpha \underbrace{(1 - y_n(w^T z_n + b))}_{\text{constraint}}
$$

Claim

$$
\text{SVM } \equiv \underset{b,w}{\text{min}}
(\underset{\text{all } \alpha_n \ge 0}{\text{max}} \mathcal{L}(b, w, \alpha))
= \underset{b,w}{\text{min}} ( \infty \text{if violate }; \frac{1}{2} w^T w \text{if feasible})
$$

* any violating $(b, w)$:
  $\underset{\text{all } \alpha_n \ge 0}{\text{max}}
  (\square + \sum_n \alpha_n (\text{some positive})) \rightarrow \infty$
* any feasible $(b, w)$:
  $\underset{\text{all } \alpha_n \ge 0}{\text{max}}
  (\square + \sum_n \alpha_n (\text{al non-positive})) = \square$

Constraint now hidden in $max$.

## Lagrange Dual SVM
## Solving Dual SVM
## Messages behind Dual SVM
## Conclusions
* Motivation of Dual SVM
* Lagrange Dual SVM
* Solving Dual SVM
* Messages behind Dual SVM
* Conclusions
