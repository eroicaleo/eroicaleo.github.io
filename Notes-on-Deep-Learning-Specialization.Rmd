---
title: "DeepLearningSpecialization"
author: "Yang Ge"
date: "10/23/2017"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Course 1

## Week 1

### Welcome

* What you will learn:

1. Neural networks and Deep Learning
2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.
3. Structuring your Machine Learning Project:
    * Train/dev/test model has changed
    * What if your training set and testing set come from a different distrbution?
      How to deal with it?
    * end to end deep learning, when to use and when not to use?
    * unique: share hard one lessons.
4. Convolutional Neural Networks (CNN)
    * Often apply to images
5. Natural Language Processing: Building Sequence models
    * RNN
    * LSTM (Long short term memory model)
    * Apply to speech recognition

### What is a neural network?

* 'Deep learning' refers to train (sometimes very large) neural networks
* House price prediction
    * Linear regression: Straight line
    * Very simple NN: Size(x) -> Cycle -> Price(y)
    * Cycle is neuron: Input the size, compute the linear function, output the price.
    * ReLu function: Rectified Linear Unit, takes maximum or 0
    * Large neural is taking many single neurons and stacking together
* House price prediction 2
    * size and # bdrm -> family size
    * zip code -> walkability
    * zip code + wealth -> school quality
    * family size + walkability + school quality -> price
    * The power and magic of NN: just need to give X and y, all the things in the middle
      will be figured out by itself.
    * Given enough training examples, NN good at finding functions
    
### Supervised learning with NN

* Supervised Learning

| Input(x)     | Output(y)     | Application |
| :------------- | :------------- | :--------|
| Home features | Price   | Real Estate (Standard NN) |
| Ad, user info | Click on ad? (0/1) | Online Advertising (Standard NN) |
| Image | Object(1, ..., 1000) | Photo tagging (CNN) |
| Audio | Text Transcript | Speech recognition (RNN) |
| English | Chinese | Machine translation (sequence data: complex RNN) |
| Image, Radar info | Position of a car | Autonomous driving (custom/hybrid architecture) |

* Needs to cleverly select what needs to be X and y
* Neural network examples
* Structured Data (database of data, housing, AD)
* Unstructured Data (raw audio, images, text)
    * Hard to computer to understand historically
    * Human is good to understand this
    * Thanks to deep learning, computers are much better than years ago
    
### Why is deep learning taking off

* Scale drives deep learning progress
    * SVM, logistical regression: Performance saturated even data scales
    * Train very large NN: performance keeps getting better and better
* Need 2 things to get very good performance:
    * Very big NN
    * Do need a lot of data
* Use `m` in lower case to denote amount of data
* For small training set, SVM might do better than NN.
* Large training set, large NN dominate performance.
* At the beginning, large training set + faster machines help.
* Recent years, algorithm innovation helps.
    * A lot of algorithm helps NN run faster.
    * One example: sigmoid function to ReLu function. Because at the two end of sigmoid function,
      the slop is flat, the gradient decent is slow.
* Idea -> Code -> Experiment -> Idea, so fast computation is really important.
    * Now can get results in 1day/10min, before it can be 1mon
    * This way can do much more experiments
    
### About this course

* Week1 Introduction
* Week2 Basic NN programming
    * Will code backward/forward propogation
* Week3 One single layer NN
* Week4 Deep NN

### Course resources

* Discussion forums
    * Question, tech details, bugs.
* [feedback,enterprise,academic]@deeplearning.ai

## Week2 Logistic Regression as a NN

### 2.1 Binary Classification

* In NN implementation, we don't want iterate the `m` training set with an explicit for loop.
* We will learn why computation can be organized as forward propogation + backward propogation
* How Computer store image: red/green/color channel
    * 64x64 picture, then 3 64x64 matrices.
    * Unroll all the pixels, input vector dimension: 12288 $n= n_x = 12288$

#### Notation

$$
(x,y), x \in R^{n_x}, y \in \{0,1\} \\
\{(x^{(1)}, y^{(1)}),\dots\} \\
M_{\text{train}}, M_{\text{test}} \\
X = [x^{(1)}, \dots, x^{(m)}], n_x \text{ by } m, \in R^{n_x \times m} \\
Y = [y^{(1)}, \dots, y^{(m)}] \in R^{1 \times m}
$$

* python cmd: `X.shape, Y.shape`

### 2.2 Logistic Regression

1. Given $x \in R^{n_x}$, want $\hat{y} = P(y=1|x)$
2. Parameters: $w \in R^{n_x}, b \in R$
3. Doesn't work output: $\hat{y} = w^Tx + b$
4. Output: $\hat{y} = \sigma(w^Tx + b)$, sigmoid function

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

5. When z large, it's 1, when z large negative, then it's 0.
6. Note: In programming NN, we usually keep w and b separate. It's easier for NN implementation.

### 2.3 Logistic Regression Cost Function

1. Given $\{(x^{(1)}, y^{(1)}),\dots\, (x^{(m)}, y^{(m)})\}$, want $\hat{y}^{(i)} \approx y^{(i)}$
2. Loss error function: $L(\hat{y}, y) = 0.5(\hat{y} - y)^2$ Not working well
3. Working well: $L(\hat{y}, y) = -(y \log \hat{y} + (1-y) \log (1-\hat{y}))$

$$
y =1 \implies L(\hat{y}, y) = -\log \hat{y} \implies \log \hat{y} \text{ large }
\implies \hat{y} \text{ large } \\
y = 0 \implies L(\hat{y}, y) = -\log (1-\hat{y}) \implies \log (1- \hat{y}) \text{ large }
\implies \hat{y} \text{ small }
$$
4. Loss functino is defined in a single training example, but cost function is defined for
   measuring how well it's doing for entire training functions.

$$
J(w, b) = \frac{1}{M} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)}) \\
= -\frac{1}{M} \sum_{i=1}^m 
y^{(i)} \log \hat{y}^{(i)} + 
(1-y^{(i)}) \log (1-\hat{y}^{(i)})
$$

5. Logistic regression can be viewed as very small neural network.
   
### 2.3 Gradient Descent

* Recap

$$
\hat{y} = \sigma(w^Tx + b) \\
\sigma(z) = \frac{1}{1+e^{-z}} \\
J(w, b) = \frac{1}{m} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})
= -\frac{1}{m} \sum_{i=1}^m 
y^{(i)} \log \hat{y}^{(i)} + 
(1-y^{(i)}) \log (1-\hat{y}^{(i)})
$$

1. Want to find w, b to minimize J(w, b), which is a convex function, one of huge reason to use
    such function as cost function.
2. Initialize $w,b = 0$


1. Repeat {
2.   $w := w - \alpha \frac{\partial J(w,b)}{\partial w}$
3.   $b := b - \alpha \frac{\partial J(w,b)}{\partial b}$
4. }


1. alpha is learning rate
2. `dw, db` to be variable for derivative term in coding python.
3. $ \frac{\partial J(w)}{\partial w}$ slope of the function

### 2.4 Derivatives

1. Think derivatives as slope of the function
2. height / width.

### 2.5 More Derivative Examples

### 2.6 Computation graph

$$
J(a, b, c) = 3a + bc
$$

1. Computation graph comes into handy when you have a function to optimize
2. Computation graph organize the computation with the blue arrow from left to right

### 2.7 Derivatives with a Computation Graph

1. Chain rule: a -> v -> J
2. dFinalOutputVar / dVar, but in code: `dvar`, like `dv, da`

### 2.8 Logistic Regression Gradient Descent

$$
da = \frac{dL(a, y)}{da} = -\frac{y}{a} + \frac{1-y}{1-a} \\
dz = a - y \\
dw1 = x_1 \cdot dz, dw2 = x_2 \cdot dz, db = dz
$$

### 2.9 Gradient Descent on m Examples

1. J = 0, dw1 = 0, dw2 = 0, db = 0
2. for i = 1 to m
3.     $z^{(i)} = w^Tx^{(i)}+b$
4.     $a^{(i)} = \sigma(z^{(i)})$
5.     $J += -[y^{(i)} \log a^{(i)} + (1- y^{(i)}) \log (1 - a^{(i)})]$
6.     $dz^{(i)} = a^{(i)} - y^{(i)}$
7.     $dw_1 += x_1^{(i)} dz^{(i)}$
8.     $dw_2 += x_2^{(i)} dz^{(i)}$
9.     $db += dz^{(i)}$
10. J/=m, dw1 /= m, dw2 /= m, db /= m
11. w1 := w1 - alpha dw1 and so on

* Two weakness:

1. For loop training ex
2. For loop for features.
3. Without using for loop helps scaling to bigger data set
4. In deep-learning era, vectorization is a must.

### 2.10 Vectorization

1. What is vectorization?
2. $z = w^Tx+b$
3. python: `z = np.dot(w, x) + b` 100X faster than for loop in python
4. both GPU and CPU have SIMD
5. Whenever possible, avoid use explicit for loop

### 2.11 More Vectorization Examples

1. Rule of Thumb: whenever possible, avoid explicit for-loops
2. `u = np.dot(a, v)`
3.

```
import numpy as np
u = np.exp(v)
np.log(v)
np.abs(v)
np.maximum(v, 0)
v**2
1/v
```

4. Logistic regression derivatives

```
dw = np.zeros((n_x, 1))
dw += x * dz
dw /= m
```

### 2.12 Vectorizing Logistic Regression

$$
z^{(1)} = w^Tx^{(1)}+b \\
a^{(1)} = \sigma(z^{(1)}) \\
z^{(2)} = w^Tx^{(2)}+b \\
a^{(2)} = \sigma(z^{(2)}) \\
z^{(3)} = w^Tx^{(3)}+b \\
a^{(3)} = \sigma(z^{(3)}) \\
$$

$$
[z^{(1)}, z^{(2)}, \cdots, z^{(m)}] = w^T X + [b, b, \cdots, b]
$$

* python code

```
Z = np.dot(w.T, X) + b
```

* Compute a leave to programming assignment.

### 2.13 Vectorizing Logistic Regression's Gradient Output

$$
dZ = [dz^{(1)}, dz^{(2)}, \cdots, dz^{(m)}]
dZ = A - Y
$$

* python code for db and dw

```
db = 1/m * np.sum(dZ)
dw = 1/m * np.dot(X, dZ.T)
```

* Implementing Logistic Regression

$$
Z = w^T X + b \\
A = \sigma (Z) \\
dZ = A - Y \\
dw = 1 / m  X (dZ)^T \\
db = 1 / m \sum dZ
$$

* Still need to do for loop over iterations and no way to get rid of it.

### 2.14 Broadcasting in Python

1. Broadcasting example:

```
cal = A.sum(axis=0)
lala = cal.reshape(1, 4)
percentage = 100 * A / lala
```

2. `axis=0` means sum vertically.
3. `A / cal.reshape(1, 4)` is broadcasting, although redundant, 
    `reshape` is constant cheap call, and make the code clear.
4. `(m,n) + (1,n)`: copy `(1,n)` m times, so the results are `(m,n)`
5. `(m,n) + (m,1)`: copy `(m,1)` n times, so the results are `(m,n)`

* General principle

1. `(m,n) +-*/ (1,n)`: copy `(1,n)` m times, so the results are `(m,n)`
2. `(m,n) +-*/ (m,1)`: copy `(m,1)` n times, so the results are `(m,n)`
3. more general, look for numpy doc, broadcasting. [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

### 2.15 A note on python/numpy vectors

1. weekness: easily introducing subtle bugs
2. Commit to Use row/column explicitly: `a = np.random.rand(5,1)`
3. `a = np.random.rand(5)` creates rank 1 array. DON'T USE.
4. If not sure the dimension, do this `assert(A.shape == (5,1))`, inexpensive.
5. reshape rank 1 array by `a = a.reshape(1,5)`

### 2.16 Quick tour of Jupyter/iPython Notebooks

### 2.17 Python Basics With Numpy v3

<font color='blue'>
**What you need to remember:**
- np.exp(x) works for any np.array x and applies the exponential function to every coordinate
- the sigmoid function and its gradient
- image2vector is commonly used in deep learning
- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. 
- numpy has efficient built-in functions
- broadcasting is extremely useful

**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and the `*` operator (which is equivalent to  `.*` in Matlab/Octave), which performs an element-wise multiplication.

<font color='blue'>
**What to remember:**
- Vectorization is very important in deep learning. It provides computational efficiency and clarity.
- You have reviewed the L1 and L2 loss.
- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...

### 2.18 Logistic Regression with a Neural Network mindset

<font color='blue'>
**What you need to remember:**

Common steps for pre-processing a new dataset are:
- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
- Reshape the datasets such that each example is now a vector of size (num_px \* num_px \* 3, 1)
- "Standardize" the data

<font color='blue'>
**What to remember:**
You've implemented several functions that:
- Initialize (w,b)
- Optimize the loss iteratively to learn parameters (w,b):
    - computing the cost and its gradient 
    - updating the parameters using gradient descent
- Use the learned (w,b) to predict the labels for a given set of examples

**Interpretation**: 
- Different learning rates give different costs and thus different predictions results.
- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). 
- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.
- In deep learning, we usually recommend that you: 
    - Choose the learning rate that better minimizes the cost function.
    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) 

<font color='blue'>
**What to remember from this assignment:**
1. Preprocessing the dataset is important.
2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().
3. Tuning the learning rate (which is an example of a "hyperparameter") can make a big difference to the algorithm. You will see more examples of this later in this course!

## Week 3 Shallow NN

### 3-1 Neural Networks Overview

* What is a NN ?

$$
z^{[1]} = W^{[1]}x + b^{[1]} \\
a^{[1]} = \sigma(z^{[1]}) \\
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\
a^{[2]} = \sigma(z^{[2]}) \\
L(a^{[2]}, y)
$$

* superscript bracket refers to layer
* Then do backward propogation

$$
da^{[2]},dz^{[2]},dW^{[2]},db^{[2]},da^{[1]},dz^{[1]}, \cdots
$$

### 3-2 Neural Network Representation

1. input layer $x_1, \dots, x_{n_x}$
2. Hidden layer: the value is not seen in training set
3. Output layer: $y$
4. Notation: a: activation

$$
a^{[0]} \\
a^{[1]} =
\begin{bmatrix}
a^{[1]}_{1} \\
a^{[1]}_{2} \\
a^{[1]}_{3} \\
a^{[1]}_{4} \\
\end{bmatrix}
\\
a^{[2]} = \hat{y}
$$

5. When we count layer, we don't count input layer
6. Hidden layer and output layer are associated with $W^{[1]}, b^{[1]}; W^{[2]}, b^{[2]}$.

### 3-3 Computing a Neural Network's Output

1. The cycle in NN represents 2 steps of computation

$$
\text{logistic regression:} \\
z = w^Tx+b \\
a = \sigma(z) \\
\text{NN:} \\
z^{[1]}_1 = w^{[1]T}_1 x^{[1]}_1+b^{[1]}_1 \\
a^{[1]}_1 = \sigma(z^{[1]}_1) \\
W = \begin{bmatrix}
-w^{[1]}_{1}- \\
-w^{[1]}_{2}- \\
-w^{[1]}_{3}- \\
-w^{[1]}_{4}- \\
\end{bmatrix} \\
z^{[1]} = W^{[1]} x + b^{[1]} \\
a^{[1]} = \sigma(z^{[1]})
$$

### 3-4 Vectorizing across multiple examples

1. Notation: $x^{(1)} \rightarrow a^{[2](1)}$
2. Taking the training examples, stack them columnly

$$
X = [x^{(1)}, x^{(2)}, \dots, x^{(m)}] \\
Z^{[1]} = W^{[1]} X + b^{[1]} \\
Z^{[1]} = [z^{[1](1)}, \dots, z^{[1](m)}] \\
A^{[1]} = \sigma(Z^{[1]}) \\
A^{[1]} = [a^{[1](1)}, \dots, a^{[1](m)}] \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

3. The column of $Z^{[1]}, A^{[1]}$ corresponds to training examples, the row corresponds to
  hidden unit.
  
### 3-5 Explanation for Vectorized Implementation

* Justification for vectorized implementation

$$
z^{[1](1)} = W^{[1]}x^{(1)}, z^{[1](2)} = W^{[1]}x^{(2)}, z^{[1](3)} = W^{[1]}x^{(3)} \\
W^{[1]}
\begin{bmatrix}
| & | & | \\
x^{(1)} & x^{(2)} & x^{(3)}\\
| & | & | \\
\end{bmatrix} =
\begin{bmatrix}
| & | & | \\
z^{[1](1)} &z^{[1](2)} & z^{[1](3)}\\
| & | & | \\
\end{bmatrix} = Z^{[1]}
$$

* Recap of vectorization across multiple examples

```
for i = 1 to m
```
$$
z^{[1](i)} = W^{[1](i)} x + b^{[1]} \\
a^{[1](i)} = \sigma(z^{[1](i)}) \\
z^{[2](i)} = W^{[2](i)} x + b^{[2]} \\
a^{[2](i)} = \sigma(z^{[2](i)})
$$

* Vectorization version

$$
Z^{[1]} = W^{[1]} X + b^{[1]} \\
A^{[1]} = \sigma(Z^{[1]}) \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

### 3-6 Activation functions

1. Don't have to use sigmoid function, can use
    $\tanh = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ function
2. $\tanh$ almost always works better, because centering @ 0.0 instead of 0.5
3. exception is the output layer, because $\hat{y} \in \{0, 1\}$
4. When z is very large or small, the gradient is close to 0, then make the gradient descent slow.
5. Relu $a = \max (0,z)$
6. Rule of thumb
    1. binary classification: sigmoid for output layer.
    2. For hidden layer, Relu becomes more popular.
7. Pros and Cons
    1. sigmoid: only use for output layer
    2. Relu: default
    3. Leaky Relu: maybe $a = \max (0.01z, z)$
8. Always have a lot choices: # of hidden unit, choice functions, hard to know what might be best.

### 3-7 Why do you need non-linear activation functions?

1. Why not just use linear activation function: $a^{[1]} = z^{[1]}, a^{[2]} = z^{[2]}$
2. If so, we just have linear model: $a^{[2]} = W^{[2]}W^{[1]}x + (W^{[2]}b^{[1]}+b^{[2]})$,
   then many layer won't help.
3. One place to use linear activation function is the output layer for regressison function.

### 3-8 Derivatives of activation functions

1. Sigmoid activation function

$$
g(z) = \frac{1}{1+e^{-z}} \\
\frac{dg(z)}{dz} = -\frac{1}{(1+e^{-z})^2} (-e^{-z}) = g(z)\frac{e^{-z}}{1+e^{-z}} \\
= g(z)(1-g(z)) \\
a = g(z) \\
g'(z) = a(1-a)
$$

2. Tanh activation function

$$
g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
\frac{dg(z)}{dz} = -\frac{(e^z - e^{-z})^2}{(e^z + e^{-z})^2} + 1 = 1-g^2(z) \\
a = g(z) \\
\frac{dg(z)}{dz} = 1 - a^2
$$

3. ReLU: $g'(z) = 1$, for z >= 0, g'(z) = 0, for z < 0
4. Leaky ReLU: g'(z) = 1, for z >= 0, g'(z) = 0.01, for z < 0

### 3-9 Gradient descent for Neural Networks

$$
W^{[1]} (n^{[1]},n^{[0]}), b^{[1]}(n^{[1]},1),
W^{[2]} (n^{[2]}, n^{[1]}), b^{[2]} (n^{[2]}, 1) \\
n_x = n^{[0]}, n^{[1]}, n^{[2]} \\
\text{cost function:} \\
J = \frac{1}{m} = \sum_{i=1}^m L(\hat{y}, y)
$$

* Forward propogation:

$$
Z^{[1]} = W^{[1]} X + b^{[1]} \\
A^{[1]} = \sigma(Z^{[1]}) \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

* Backward propogation:

$$
dZ^{[2]} = A^{[2]} - Y \\
dW^{[2]} = \frac{1}{m} dZ^{[2]} A^{[1]T} \\
db^{[2]} = \frac{1}{m} np.sum(dZ^{[2]}, axis=1,keepdims = True) \\
dZ^{[1]} = W^{[2]T} dZ^{[2]} \cdot g'^{[1]} (Z^{[1]}) 
(\text{element-wise, both sizes are }(n^{[1]},m)) \\
dW^{[1]} = \frac{1}{m} dZ^{[1]}X^T \\
db^{[1]} = \frac{1}{m} \cdot np.sum(dZ^{[1]}, axis=1, keepdims = True)
$$

### 3-10 Backpropagation Intuition

### 3-11 Random Initialization

1. For NN, initialization to all 0 won't work

$$
a_1^{[1]} = a_2^{[1]} \\
dz_1^{[1]} = dz_2^{[1]}
$$

2. Symmetric, so every row is the same
3. Not helpful because want different hidden units to compute different hidden functions.

$$
W^{[1]} = np.random.rand((2,2)) \times 0.01 \\
b^{[1]} = np.zeros((2, 1)) \\
$$

4. why the 0.01 comes from? We want usually very small random numbers. Because if we use
  very big number, $a^{[1]}, \dots$ will be in the flat part, gradient descent will be slow.
5. In deep NN, we might want to choose different constant than 0.01.

## Week 4 Deep NN

### 4-1 Deep L-layer neural network

1. What is a DNN
2. View the # of layers as hyperparameters
3. $L$ # of layers
4. $n^{[l]}$ # of units in layer l
5. $a^{[l]}$ activation in layer l, $a^{[l]} = g^{[l]}(z^{[l]})$
6. $W^{[l]}$ weights for $z^{[l]}$, $b^{[l]}$
7. $x = a^{[0]}, \hat{y} = a^{[L]}$

### 4-2 Forward Propagation in a Deep Network

1. single example:

$$
z^{[1]} = W^{[1]} x + b^{[1]} \\
a^{[1]} = g^{[1]} (z^{[1]}) \\
z^{[2]} = W^{[2]} a^{[1]} + b^{[2]} \\
a^{[2]} = g^{[2]} (z^{[2]}) \\
\dots \\
z^{[4]} = W^{[4]} a^{[3]} + b^{[4]} \\
\hat{y} = a^{[4]} = g^{[4]} (z^{[4]}) \\
\text{In general: } \\
z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} = g^{[l]} (z^{[l]})
$$

2. vectorization version:

$$
Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]} \\
A^{[1]} = g^{[1]}(Z^{[1]}) \\
\text{In general: } \\
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]})
$$

3. There is one for loop iterate through layer 1 to layer L, no way to get rid of.

### 4-3 Getting your matrix dimensions right

1. Useful tool to debug, piece of paper to walk through dimensions of the Matrix.

$$
\text{In general: } \\
W^{[l]}, dW^{[l]}: (n^{[l]},n^{[l-1]}) \text{ matrix }\\
b^{[l]}, db^{[l]}: (n^{[l]}, 1) \text{ vector } \\
Z^{[l]}, A^{[l]}, dZ^{[l]}, dA^{[l]}: (n^{[l]}, m) \text{ matrix }\\
$$

2. Once make sure the dims are correct, it can eliminate some of the bugs.

### 4-4 Why deep representations?

1. Face detection:
2. First layer: detect edges
3. 2nd layer: detect parts of the face
4. 3nd layer: put parts of faces together then can detect
5. early layer detects simpler funcitons
6. composing to later layers.
7. Audio clip
8. First layer: low level audio waveform features.
9. 2nd layer: phonemes
10. 3rd layer: words
11. 4th layer: sentence/phrases
12. When NN gets deeper, it can do suprisingly complex things

* Circuit theory and deep learning

1. small L layer deep NN that can do exponential bigger shallower NN.
2. Andrew found this explanation less intuitive.

* "Deep learning" is great branding
* When starts, starts with something simple, e.g. logistic regression.

### 4-5 Building blocks of deep neural networks

* 1st focus on one layer

$$
\text{Forward: } \\
input: a^{[l-1]}, output: a^{[l]} \\
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]}) \\
cache: Z^{[l]}\\
\text{Backward: } \\
input: da^{[l]}, cached Z^{[l]},output:da^{[l-1]}, dW^{[l]}, db^{[l]}
$$

2. The block diagram is really good. **NEED to REMEMBER**!!

3. Implementatino notes: cache not just $Z^{[l]}$, but also $W^{[l]}, b^{[l]}$

### 4-6 Forward and Backward Propagation

* Forward propagation for layer l

1. Input $a^{[l-1]}$
2. output $a^{[l]}$, cache $z^{[l]}$

$$
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]}) \\
$$

* Backward propagation for layer l

1. Input $da^{[l]}$
2. output $da^{[l-1]}$, cache $z^{[l]}$

$$
dZ^{[l]} = dA^{[l]} \cdot g'^{[l]} (Z^{[l]}) \\
dW^{[l]} = \frac{1}{m} dZ^{[l]} A^{[l-1]T} \\
db^{[l]} = \frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims = True) \\
dA^{[l-1]} = W^{[l]T} \cdot dZ^{[l]} \\
dA^{[L]} = (-\frac{y^{(1)}}{a^{(1)}}+\frac{1-y^{(1)}}{1-a^{(1)}}, \dots, 
-\frac{y^{(m)}}{a^{(m)}}+\frac{1-y^{(m)}}{1-a^{(m)}})
$$

3. complexity comes from the data rather than from the code
4. When you done programming exercises, it will become more concrete.

### 4-7 Parameters vs Hyperparameters

1. parameters: $W, b$
2. hyper-parameters: learning rate, iterations, # of hidden layer, # of hidden units.
3. choice of activation functions.
4. Later: momentum, mini-batch size, regularization parameter forms.

* Apply deep learning is a very empirical process
* Idea -> Experiment -> code
* Vision, speech, NLP, Ad.
* Try a range of values to start with.
* The best hyper-parameters today might not be the best one 1 year later.
    * The computing infrastructure might change
    * Try every several months

### 4-8 What does this have to do with the brain?

# Course 2

## Week 1 Practical aspects of Deep Learning

### 1-1 Train / Dev / Test sets

1. Applied ML is a highly iterative process: # of layers, # of hidden units, learning rate
    activation functions
2. Idea -> Code -> Experiment
3. Intuition from one domain, often do not transfer to another.
4. Almost impossible to correctly guess all the parameters very first time.
5. Train/dev/test sets
6. Previous example: 60/20/20 for 100/1000/10000 samples.
7. Big data: 1M samples, 10000 might be more than enough for dev and test, 98/1/1%

* Mismatched train/test distribution

1. Training set: Cat pictures from web, HD, clear
2. Dev/test set: from uploaded users, low resolution, etc
3. Rule of Thumb: dev and test come from same distribution
4. Not having a test set might be OK.

### 1-2 Bias / Variance

1. High bias: underfitting
2. High variance: overfitting
3. Train set / Dev set error: 
    * 1/11% high variance
    * 15/16% high bias
    * 15/30% both high variance + bias
4. But it depends also on optimal (bayes) error, blurry image.
5. Assumptions: 1. bayes error is small 2. draw from the same distribution
6. What does a high variance + bias look like?
    * Don't have flexibility for most data, e.g. a straight line
    * Have great flexibility for outliers.

### 1-3 Basic Recipe for Machine Learning

1. Does it has high bias? training set performance?
    1. Bigger network (always work better)
    2. Train longer (not always, but doesn't hurt)
    3. Neural network architecture (might not work)
2. Does it have high variance (after Q1 is solved)? Dev set performance?
    1. Get more data (pretty much always work)
    2. Regularization
    3. Neural network architecture (might not work)
3. Hopefully, find something low bias + low variance, DONE :)
4. high bias, not gonna help
5. Earlier ML, discussion about "Bias variance tradeoff".
6. Modern deep learning big data era, we can reduce without increasing other one.
7. Regularization might increase bias a little bit, but won't be too much.

### 1-4 Regularization

* Logistic regression

$$
\underset{w,b}{\text{min } } J(w, b) = 
\frac{1}{m} \sum_{i = 1}^{m} L(\hat{y}^{(i)}, y^{(i)}) +
\frac{\lambda}{2m} ||w||^2_2 \\
\text{L2 regularization: } ||w||^2_2 = w^Tw \\
\text{L1 regularization: } ||w||_1 = \sum_{i = 1}^{n_x} |w|
$$

1. In practise, $b$ is usually not included.
2. L1 will make $w$ sparse, not used that much.
3. L2 is much much often.
3. $\lambda$ regularization parameter, determined by dev set.
4. In programming exercises, use `lambd`

* Neural network

$$
J(W^{[1]}, b^{[1]}, \dots, W^{[L]}, b^{[L]}) =
\frac{1}{m} \sum_{i = 1}^{m} L(\hat{y}^{(i)}, y^{(i)}) +
\frac{\lambda}{2m} \sum_{i = 1}^{m} ||W^{[L]}||^2_F \\
dW^{[L]} = \text{( from back propogation )} + \frac{\lambda}{m} W^{[L]}
$$

1. It's also called weight decay.

### 1-5 Why regularization reduces overfitting?

1. When $\lambda$ is big, $W^{[L]} \approx 0$, NN becomes simpler and reduce overfitting and
  close to linear model.
2. $z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}$, $W^{[L]}$ is small then $z^{[L]}$ is small and will
  be in the linear region. Then every neoron is linear, the whole model is linear.
3. implementation tip for debugging, plot the cost function with the iteration.
  When you include regularization, needs to plot the whole cost function, not just the loss 
  function.

### 1-6 Dropout Regularization

1. For each training example, remove some nodes with certain probability.

* Implementing dropout ("Invert dropout")

```
# keep_prob = 0.8 in this example
# d3 will be a boolean array
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
a3 = np.multiply(a3, d3)
a3 /= keep_prob # bump 
```

* 问题：看起来以上的implementation，for different training examples, it has different
  dropout. How to do backward propogation??

1. The reason for division: 50 units -> 10 units shut off, want to keep the expectation
  of $a^{[4]}$ the same

$$
z^{[4]} = W^{[4]} \cdot a^{[3]} + b^{[4]}
$$

* Making predictions at test time: There is no drop out.

### 1-7 Understanding Dropout

1. Intuition: can't rely on any one feature, so have to spread out weights.
2. So if won't put too much weight on any one input.
3. Shrink the squared norm weight.
4. Dropout has a similar effect as the L2 regularization.
5. Errata: 2:50 the size of the matrix
6. For matrix with big size, apply higher drop out.
7. The input layer can also have dropout, but usually don't do that.
8. Computer vision has successful application. Input is so big, never have enough data.
9. Downside: $L$ is no longer well defined. So loosing the debugging tool.
10: Andrew recommend: first turn off drop out, make sure bug-free. Then turn on drop out.

### 1-8 Other regularization methods

* Data augmentation, because we want to get more data.
    * flipping image
    * randomly distortion and transformation, e.g. zooming in, rotation
* Early stopping
    * plot training error or J.
    * Also plot dev set, decrease first than increase.
    * Reason: at beginning, $|W|$ tends to be small, in later interation, $|W|$ gets bigger
      and bigger.
    * stop half way, $||W^{[L]}||^2_F$ won't be too big.
* Andrew's experience, orthogonalization, divide ML to 2 tasks, focus one at a time
    * Optimize cost function J
    * Not overfit
    * Use early stopping cannot decouple the 2 tasks because trying to do the 2 together
    * L2 regularization can decouple the 2 tasks, but need to try different lambda

### 1-9 Normalizing inputs

1. substract mean

$$
\text{ mean: } \\
\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)} \\
x = x - \mu \\
\text{ Normalize variance: } \\
\sigma^2 = \frac{1}{m} \sum_{i=1}^m x^{(i)} ** 2 \\
x = x / \sigma^2
$$

2. Needs to use same training set and test set

* Why normalize inputs?

1. Not normalized, conture will be elongated, gradient descent will go back and forth.
2. Normalized, conture will be more symmetric, gradient descent will go straight to minimum.

* Never does any harm

### 1-10 Vanishing / Exploding gradients

1. Sometimes the gradient can be very very big or very very small.
2. If $w > 1$, then very deep network $w^L$ will be exponential big. The same thing will be when
    $w < 1$
3. The same argument can be said for the derivatives.
4. This was a major barrier before.

### 1-11 Weight Initialization for Deep Networks

* Single neuron example

$$
z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n
$$

1. We would like to have the larger n, the smaller $w$. What we can do is to set $var(w) = \frac{1}{n}$ for sigmoid and tanh activation, and $var(w) = \frac{2}{n}$ for relu .
2. Python code as follows

```
WL = np.random.randn(shape) * np.sqrt(1/n[l-1]) # for sigmoid and tanh
WL = np.random.randn(shape) * np.sqrt(2/n[l-1]) # for Relu function
```

### 1-12 Numerical approximation of gradients

1. Check your derivative computation
2. Use 2 sides difference, error is $O(\epsilon^2)$
3. Use 1 side difference, error is $O(\epsilon)$

### 1-13 Gradient checking

1. Helps Andrew a lot to find bugs.
2. Take $W^{[1]}, b^{[1]}, \dots, W^{[L]}, b^{[L]}$ into a giant vector $\theta$
3. Take $dW^{[1]}, db^{[1]}, \dots, dW^{[L]}, db^{[L]}$ into a giant vector $d\theta$

* Gradient checking (Grad check)

$$
\text{for each } i: \\
d\theta_{approx}[i] = \frac
{J(\theta_1, \theta_2, \dots, \theta_i + \epsilon, \dots)
- J(\theta_1, \theta_2, \dots, \theta_i - \epsilon, \dots)}
{2 \epsilon} \approx
d\theta[i] = 
\frac{\partial J}
{\partial\theta_i} \\
\text{ check } \\
\frac{||d\theta_{approx} - d\theta||_{2}}
{||d\theta_{approx}||_2 + ||d\theta||_2}
\approx \epsilon = 10^{-7}
$$

1. If the $\epsilon >= 10^{-3}$, most likely there is a bug.
2. Help Andrew a lot in finding bugs.

### 1-14 Gradient Checking Implementation Notes

1. Don't use in training - only to debug
2. If algorithm fails grad check, look at components to try to identify bug.
    1. E.g. $db^{[l]}$ is off, but $dW^{[l]}$ is close, then check how to compute $db^{[l]}$
3. Remember regularization term: include it in gradient descent if use regularization.
4. Doesn't work with dropout
    1. Turn on grad check without drop out.
    2. Turn off grad and turn on drop out, hope for the best.
    3. Could do but Andrew don't usually do: Fix pattern of drop out.
5. Run a random initialization. run it again after some training.

### 1-15 Programming assignment

* Initialization:

<font color='blue'>
**What you should remember**:
- The weights $W^{[l]}$ should be initialized randomly to break symmetry. 
- It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly.

**Observations**:
- The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $\log(a^{[3]}) = \log(0)$, the loss goes to infinity.
- Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. 
- If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.

<font color='blue'>
**In summary**:
- Initializing weights to very large random values does not work well. 
- Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part! 

<font color='blue'>
**What you should remember from this notebook**:
- Different initializations lead to different results
- Random initialization is used to break symmetry and make sure different hidden units can learn different things
- Don't intialize to values that are too large
- He initialization works well for networks with ReLU activations. 

* Regularization

**Observations**:
- The value of $\lambda$ is a hyperparameter that you can tune using a dev set.
- L2 regularization makes your decision boundary smoother. If $\lambda$ is too large, it is also possible to "oversmooth", resulting in a model with high bias.

**What is L2-regularization actually doing?**:

L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. 

<font color='blue'>
**What you should remember** -- the implications of L2-regularization on:
- The cost computation:
    - A regularization term is added to the cost
- The backpropagation function:
    - There are extra terms in the gradients with respect to weight matrices
- Weights end up smaller ("weight decay"): 
    - Weights are pushed to smaller values.

**Note**:
- A **common mistake** when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. 
- Deep learning frameworks like [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) or [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) come with a dropout layer implementation. Don't stress - you will soon learn some of these frameworks.

<font color='blue'>
**What you should remember about dropout:**
- Dropout is a regularization technique.
- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.
- Apply dropout both during forward and backward propagation.
- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.  

<font color='blue'>
**What we want you to remember from this notebook**:
- Regularization will help you reduce overfitting.
- Regularization will drive your weights to lower values.
- L2 regularization and Dropout are two very effective regularization techniques.
# Python functions used through the courses

## Week 2 Optimization algorithms

### 2-1 Mini-batch gradient descent

1. Vectorization allows you to efficiently compute on m examples.
2. When m = 5000000, vectorization can still be very slow.
3. Take mini-batch of 1000 examples. Then we have 5000 mini-batches.

$$
X = [x^{(1)}, \dots, x^{(5000000)}] \\
X^{\{1\}} = [x^{(1)}, \dots, x^{(1000)}] (n_x, 1000) \\
X^{\{2\}} = [x^{(1001)}, \dots, x^{(2000)}] (n_x, 1000) \\
\dots \\
X^{\{5000\}} = [x^{(4995000)}, \dots, x^{(5000000)}] (n_x, 1000) \\
$$

* Mini-batch gradient descent

$$
\text{ for } t = 1, ..., 5000 \\
    \quad \text{ Forward prop on } X^{\{t\}} \\
    Z^{[1]} = W^{[1]} X^{\{t\}} + b^{[1]} \\
    A^{[1]} = g^{[1]}(Z^{[1]}) \\
    \dots \\
    A^{[L]} = g^{[L]}(Z^{[L]}) \\
    J^{\{t\}} = \frac{1}{1000} \sum_{i=1}^{1000} L(\hat{y}^{(i)}, y^{(i)})
    + \frac{\lambda}{2\cdot1000} \sum_{l} ||W^{[l]}||_F^2 \\
    \text{ Backward prop on wrt } J^{\{t\}} (X^{\{t\}}, Y^{\{t\}}) \\
    W^{[l]} := W^{[l]} - \alpha dW^{[l]} \\
    b^{[l]} := b^{[l]} - \alpha db^{[l]}
$$

1. This is called "1 epoch" passing through training set.

### 2-2 Understanding mini-batch gradient descent

* Training with mini batch gradient descent

1. The cost goes down every iteration, even one goes up then something is wrong, e.g.
  learning rate is too big.
2. It maynot decrease in every iteration, but the trend should go down.

* Choosing your mini-batch size

1. size = m, batch gradient descent
2. size = 1, stochastic gradient descent: every example is it's own mini batch
3. stochastic gradient descent can be extremely noisy can take to wrong direction, won't ever
  converge.
4. In practice: somewhere in between.
5. batch: too long iteration, stochastic, lose speedup from vectorization
6. In between: fastest learning
    * vectorization
    * Make progress

* Choosing your mini-batch size

1. Small training set (< 2000): Just use batch
2. Typical mini-batch sizes: 64, 128, 256, 512
3. Make sure mini-batch fit in (CPU/GPU) memory
4. This is another hyper-parameter

### 2-3 Exponentially weighted averages

* Temperature in London

$$
V_0 = 0 \\
V_1 = 0.9 V_0 + 0.1 \theta_1 \\
V_2 = 0.9 V_1 + 0.1 \theta_2 \\
\cdots \\
V_t = 0.9 V_{t-1} + 0.1 \theta_t \\
V_1 = \beta V_0 + (1- \beta) \theta_1 \\
V_t \text{ as approximately average over } \approx \frac{1}{1-\beta} \text{ days temperature }
$$
1. $\beta = 0.9 \approx $ 10 days temperature
2. $\beta = 0.98 \approx $ 50 days temperature, less wigglely but adapt slowly.
3. $\beta = 0.5 \approx $ 2 days

### 2-4 Understanding exponentially weighted averages

$$
V_1 = \beta V_0 + (1- \beta) \theta_1 \\
V_{t} = (1- \beta) V_{t-1} + (1- \beta)\beta V_{t-2} + (1- \beta)\beta^2 V_{t-2} + \cdots \\
(1 - \epsilon) ^ {\frac{1}{\epsilon}} = \frac{1}{e}
$$

* Implementing exponentially weighted averages

1. requires very less memory than the regular average: `vtheta = beta * vtheta + (1-beta) * thetat`

### 2-5 Bias correction in exponentially weighted averages

1. Bias correction: change from $V_t$ to $V_t / (1 - \beta^t)$

### 2-6 Gradient descent with momentum

1. Almost always works faster than normal gradient descent
2. On each iteration t: compute dW and db on current mini-batch
3. compute $V_{dW} = \beta V_{dW} + (1- \beta)dW$, similar for $V_{db}$
4. $W = W - \alpha V_{dW}, b = b - \alpha V_{db}$
5. The steps moving more horizontally toward minimum.

* Implementation details

$$
v_{dw} = \beta v_{dw} + (1-\beta)dW \\
v_{db} = \beta v_{db} + (1-\beta)db \\
W = W - \alpha v_{dw} \\
b = b - \alpha v_{db} \\
\text{In practice, someone does the following, Andrew does not prefer:} \\
v_{dw} = \beta v_{dw} + dW
$$

1. Hyperparameters: $\alpha, \beta$, $\beta = 0.9$ works very well.
2. In practice, don't see people use bias correction, because after 10 iterations,
    The average warms up.

### 2-7 RMSprop

1. Root mean square prop
2. On iteration t: compute dW, db on current mini-batch

$$
S_{dW} = \beta S_{dW} + (1-\beta) dW^2 \text{ (element-wise) } \\
S_{db} = \beta S_{db} + (1-\beta) db^2 \text{ (element-wise) } \\
W = W - \alpha \frac{dW}{\sqrt{S_{dW}} + \epsilon} \\
b = b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon} \\
$$

3. $\epsilon$ is for numerical stability.

### 2-8 Adam optimization algorithm

1. A lot of optimization algorithm proposed not well generalized.

$$
V_{dW} = 0, S_{dW} = 0, V_{db} = 0, S_{db} = 0 \\
V_{dW} = \beta_1 V_{dW} + (1 - \beta_1) dW, V_{db} = \beta_1 V_{db} + (1-\beta_1) db \\
S_{dW} = \beta_2 S_{dW} + (1-\beta_2) dW^2 \text{ (element-wise) } \\
S_{db} = \beta_2 S_{db} + (1-\beta_2) db^2 \text{ (element-wise) } \\
V_{dW}^{corrected} = V_{dW} / (1 - \beta^t_1), V_{db}^{corrected} = V_{db} / (1 - \beta^t_1) \\
S_{dW}^{corrected} = S_{dW} / (1 - \beta^t_1), S_{db}^{corrected} = S_{db} / (1 - \beta^t_1) \\
W = W - \alpha \frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}} + \epsilon} \\
b = b - \alpha \frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}} + \epsilon} \\

$$

* Hyperparameter choice

1. $\alpha$
2. $\beta_1 = 0.9$
3. $\beta_2 = 0.999$
4. $\epsilon = 10^{-8}$

* Adam: Adaptive moment estimation

### 2-9 Learning rate decay

1. 1 epoch is 1 pass through the data

$$
\alpha = \frac{1}{1+\text{decay rate} \times \text{epoch_num}} \alpha_0 \\
\alpha = \frac{k}{\sqrt{\text{epoch_num}}} \alpha_0 \\
\alpha = 0.95^{\text{epoch_num}} \alpha_0
$$

2. manual decay: only works for training small # of models.
3. Learning rate can really help.

## 2-10 The problem of local optima

1. The intuition about local optima previously is not correct
2. Most local optima are settle points
3. A lot of intuition we learnt from low dimension doesn't apply to high dimension.

* Problem of plateaus

1. unlikely to get stuck in a bad local optimal
2. Plateaus can make learning slow

## 2-11 Programming assignment

<font color='blue'>
**What you should remember**:
- The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.
- You have to tune a learning rate hyperparameter $\alpha$.
- With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).

<font color='blue'>
**What you should remember**:
- Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.
- You have to tune a momentum hyperparameter $\beta$ and a learning rate $\alpha$.


# numpy functions

```
np.sum
np.square
np.zeros
np.random.randn
np.random.rand
np.multiply
np.linalg.norm
np.copy
```

