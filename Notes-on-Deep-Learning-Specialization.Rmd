---
title: "DeepLearningSpecialization"
author: "Yang Ge"
date: "10/23/2017"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Course 1

## Week 1

### Welcome

* What you will learn:

1. Neural networks and Deep Learning
2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.
3. Structuring your Machine Learning Project:
    * Train/dev/test model has changed
    * What if your training set and testing set come from a different distrbution?
      How to deal with it?
    * end to end deep learning, when to use and when not to use?
    * unique: share hard one lessons.
4. Convolutional Neural Networks (CNN)
    * Often apply to images
5. Natural Language Processing: Building Sequence models
    * RNN
    * LSTM (Long short term memory model)
    * Apply to speech recognition

### What is a neural network?

* 'Deep learning' refers to train (sometimes very large) neural networks
* House price prediction
    * Linear regression: Straight line
    * Very simple NN: Size(x) -> Cycle -> Price(y)
    * Cycle is neuron: Input the size, compute the linear function, output the price.
    * ReLu function: Rectified Linear Unit, takes maximum or 0
    * Large neural is taking many single neurons and stacking together
* House price prediction 2
    * size and # bdrm -> family size
    * zip code -> walkability
    * zip code + wealth -> school quality
    * family size + walkability + school quality -> price
    * The power and magic of NN: just need to give X and y, all the things in the middle
      will be figured out by itself.
    * Given enough training examples, NN good at finding functions
    
### Supervised learning with NN

* Supervised Learning

| Input(x)     | Output(y)     | Application |
| :------------- | :------------- | :--------|
| Home features | Price   | Real Estate (Standard NN) |
| Ad, user info | Click on ad? (0/1) | Online Advertising (Standard NN) |
| Image | Object(1, ..., 1000) | Photo tagging (CNN) |
| Audio | Text Transcript | Speech recognition (RNN) |
| English | Chinese | Machine translation (sequence data: complex RNN) |
| Image, Radar info | Position of a car | Autonomous driving (custom/hybrid architecture) |

* Needs to cleverly select what needs to be X and y
* Neural network examples
* Structured Data (database of data, housing, AD)
* Unstructured Data (raw audio, images, text)
    * Hard to computer to understand historically
    * Human is good to understand this
    * Thanks to deep learning, computers are much better than years ago
    
### Why is deep learning taking off

* Scale drives deep learning progress
    * SVM, logistical regression: Performance saturated even data scales
    * Train very large NN: performance keeps getting better and better
* Need 2 things to get very good performance:
    * Very big NN
    * Do need a lot of data
* Use `m` in lower case to denote amount of data
* For small training set, SVM might do better than NN.
* Large training set, large NN dominate performance.
* At the beginning, large training set + faster machines help.
* Recent years, algorithm innovation helps.
    * A lot of algorithm helps NN run faster.
    * One example: sigmoid function to ReLu function. Because at the two end of sigmoid function,
      the slop is flat, the gradient decent is slow.
* Idea -> Code -> Experiment -> Idea, so fast computation is really important.
    * Now can get results in 1day/10min, before it can be 1mon
    * This way can do much more experiments
    
### About this course

* Week1 Introduction
* Week2 Basic NN programming
    * Will code backward/forward propogation
* Week3 One single layer NN
* Week4 Deep NN

### Course resources

* Discussion forums
    * Question, tech details, bugs.
* [feedback,enterprise,academic]@deeplearning.ai

## Week2 Logistic Regression as a NN

### 2.1 Binary Classification

* In NN implementation, we don't want iterate the `m` training set with an explicit for loop.
* We will learn why computation can be organized as forward propogation + backward propogation
* How Computer store image: red/green/color channel
    * 64x64 picture, then 3 64x64 matrices.
    * Unroll all the pixels, input vector dimension: 12288 $n= n_x = 12288$

#### Notation

$$
(x,y), x \in R^{n_x}, y \in \{0,1\} \\
\{(x^{(1)}, y^{(1)}),\dots\} \\
M_{\text{train}}, M_{\text{test}} \\
X = [x^{(1)}, \dots, x^{(m)}], n_x \text{ by } m, \in R^{n_x \times m} \\
Y = [y^{(1)}, \dots, y^{(m)}] \in R^{1 \times m}
$$

* python cmd: `X.shape, Y.shape`

### 2.2 Logistic Regression

1. Given $x \in R^{n_x}$, want $\hat{y} = P(y=1|x)$
2. Parameters: $w \in R^{n_x}, b \in R$
3. Doesn't work output: $\hat{y} = w^Tx + b$
4. Output: $\hat{y} = \sigma(w^Tx + b)$, sigmoid function

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

5. When z large, it's 1, when z large negative, then it's 0.
6. Note: In programming NN, we usually keep w and b separate. It's easier for NN implementation.

### 2.3 Logistic Regression Cost Function

1. Given $\{(x^{(1)}, y^{(1)}),\dots\, (x^{(m)}, y^{(m)})\}$, want $\hat{y}^{(i)} \approx y^{(i)}$
2. Loss error function: $L(\hat{y}, y) = 0.5(\hat{y} - y)^2$ Not working well
3. Working well: $L(\hat{y}, y) = -(y \log \hat{y} + (1-y) \log (1-\hat{y}))$

$$
y =1 \implies L(\hat{y}, y) = -\log \hat{y} \implies \log \hat{y} \text{ large }
\implies \hat{y} \text{ large } \\
y = 0 \implies L(\hat{y}, y) = -\log (1-\hat{y}) \implies \log (1- \hat{y}) \text{ large }
\implies \hat{y} \text{ small }
$$
4. Loss functino is defined in a single training example, but cost function is defined for
   measuring how well it's doing for entire training functions.

$$
J(w, b) = \frac{1}{M} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)}) \\
= -\frac{1}{M} \sum_{i=1}^m 
y^{(i)} \log \hat{y}^{(i)} + 
(1-y^{(i)}) \log (1-\hat{y}^{(i)})
$$

5. Logistic regression can be viewed as very small neural network.
   
### 2.3 Gradient Descent

* Recap

$$
\hat{y} = \sigma(w^Tx + b) \\
\sigma(z) = \frac{1}{1+e^{-z}} \\
J(w, b) = \frac{1}{m} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})
= -\frac{1}{m} \sum_{i=1}^m 
y^{(i)} \log \hat{y}^{(i)} + 
(1-y^{(i)}) \log (1-\hat{y}^{(i)})
$$

1. Want to find w, b to minimize J(w, b), which is a convex function, one of huge reason to use
    such function as cost function.
2. Initialize $w,b = 0$


1. Repeat {
2.   $w := w - \alpha \frac{\partial J(w,b)}{\partial w}$
3.   $b := b - \alpha \frac{\partial J(w,b)}{\partial b}$
4. }


1. alpha is learning rate
2. `dw, db` to be variable for derivative term in coding python.
3. $ \frac{\partial J(w)}{\partial w}$ slope of the function

### 2.4 Derivatives

1. Think derivatives as slope of the function
2. height / width.

### 2.5 More Derivative Examples

### 2.6 Computation graph

$$
J(a, b, c) = 3a + bc
$$

1. Computation graph comes into handy when you have a function to optimize
2. Computation graph organize the computation with the blue arrow from left to right

### 2.7 Derivatives with a Computation Graph

1. Chain rule: a -> v -> J
2. dFinalOutputVar / dVar, but in code: `dvar`, like `dv, da`

### 2.8 Logistic Regression Gradient Descent

$$
da = \frac{dL(a, y)}{da} = -\frac{y}{a} + \frac{1-y}{1-a} \\
dz = a - y \\
dw1 = x_1 \cdot dz, dw2 = x_2 \cdot dz, db = dz
$$

### 2.9 Gradient Descent on m Examples

1. J = 0, dw1 = 0, dw2 = 0, db = 0
2. for i = 1 to m
3.     $z^{(i)} = w^Tx^{(i)}+b$
4.     $a^{(i)} = \sigma(z^{(i)})$
5.     $J += -[y^{(i)} \log a^{(i)} + (1- y^{(i)}) \log (1 - a^{(i)})]$
6.     $dz^{(i)} = a^{(i)} - y^{(i)}$
7.     $dw_1 += x_1^{(i)} dz^{(i)}$
8.     $dw_2 += x_2^{(i)} dz^{(i)}$
9.     $db += dz^{(i)}$
10. J/=m, dw1 /= m, dw2 /= m, db /= m
11. w1 := w1 - alpha dw1 and so on

* Two weakness:

1. For loop training ex
2. For loop for features.
3. Without using for loop helps scaling to bigger data set
4. In deep-learning era, vectorization is a must.

### 2.10 Vectorization

1. What is vectorization?
2. $z = w^Tx+b$
3. python: `z = np.dot(w, x) + b` 100X faster than for loop in python
4. both GPU and CPU have SIMD
5. Whenever possible, avoid use explicit for loop

### 2.11 More Vectorization Examples

1. Rule of Thumb: whenever possible, avoid explicit for-loops
2. `u = np.dot(a, v)`
3.

```
import numpy as np
u = np.exp(v)
np.log(v)
np.abs(v)
np.maximum(v, 0)
v**2
1/v
```

4. Logistic regression derivatives

```
dw = np.zeros((n_x, 1))
dw += x * dz
dw /= m
```

### 2.12 Vectorizing Logistic Regression

$$
z^{(1)} = w^Tx^{(1)}+b \\
a^{(1)} = \sigma(z^{(1)}) \\
z^{(2)} = w^Tx^{(2)}+b \\
a^{(2)} = \sigma(z^{(2)}) \\
z^{(3)} = w^Tx^{(3)}+b \\
a^{(3)} = \sigma(z^{(3)}) \\
$$

$$
[z^{(1)}, z^{(2)}, \cdots, z^{(m)}] = w^T X + [b, b, \cdots, b]
$$

* python code

```
Z = np.dot(w.T, X) + b
```

* Compute a leave to programming assignment.

### 2.13 Vectorizing Logistic Regression's Gradient Output

$$
dZ = [dz^{(1)}, dz^{(2)}, \cdots, dz^{(m)}]
dZ = A - Y
$$

* python code for db and dw

```
db = 1/m * np.sum(dZ)
dw = 1/m * np.dot(X, dZ.T)
```

* Implementing Logistic Regression

$$
Z = w^T X + b \\
A = \sigma (Z) \\
dZ = A - Y \\
dw = 1 / m  X (dZ)^T \\
db = 1 / m \sum dZ
$$

* Still need to do for loop over iterations and no way to get rid of it.

### 2.14 Broadcasting in Python

1. Broadcasting example:

```
cal = A.sum(axis=0)
lala = cal.reshape(1, 4)
percentage = 100 * A / lala
```

2. `axis=0` means sum vertically.
3. `A / cal.reshape(1, 4)` is broadcasting, although redundant, 
    `reshape` is constant cheap call, and make the code clear.
4. `(m,n) + (1,n)`: copy `(1,n)` m times, so the results are `(m,n)`
5. `(m,n) + (m,1)`: copy `(m,1)` n times, so the results are `(m,n)`

* General principle

1. `(m,n) +-*/ (1,n)`: copy `(1,n)` m times, so the results are `(m,n)`
2. `(m,n) +-*/ (m,1)`: copy `(m,1)` n times, so the results are `(m,n)`
3. more general, look for numpy doc, broadcasting. [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

### 2.15 A note on python/numpy vectors

1. weekness: easily introducing subtle bugs
2. Commit to Use row/column explicitly: `a = np.random.rand(5,1)`
3. `a = np.random.rand(5)` creates rank 1 array. DON'T USE.
4. If not sure the dimension, do this `assert(A.shape == (5,1))`, inexpensive.
5. reshape rank 1 array by `a = a.reshape(1,5)`

### 2.16 Quick tour of Jupyter/iPython Notebooks

### 2.17 Python Basics With Numpy v3

<font color='blue'>
**What you need to remember:**
- np.exp(x) works for any np.array x and applies the exponential function to every coordinate
- the sigmoid function and its gradient
- image2vector is commonly used in deep learning
- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. 
- numpy has efficient built-in functions
- broadcasting is extremely useful

**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and the `*` operator (which is equivalent to  `.*` in Matlab/Octave), which performs an element-wise multiplication.

<font color='blue'>
**What to remember:**
- Vectorization is very important in deep learning. It provides computational efficiency and clarity.
- You have reviewed the L1 and L2 loss.
- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...

### 2.18 Logistic Regression with a Neural Network mindset

<font color='blue'>
**What you need to remember:**

Common steps for pre-processing a new dataset are:
- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
- Reshape the datasets such that each example is now a vector of size (num_px \* num_px \* 3, 1)
- "Standardize" the data

<font color='blue'>
**What to remember:**
You've implemented several functions that:
- Initialize (w,b)
- Optimize the loss iteratively to learn parameters (w,b):
    - computing the cost and its gradient 
    - updating the parameters using gradient descent
- Use the learned (w,b) to predict the labels for a given set of examples

**Interpretation**: 
- Different learning rates give different costs and thus different predictions results.
- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). 
- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.
- In deep learning, we usually recommend that you: 
    - Choose the learning rate that better minimizes the cost function.
    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) 

<font color='blue'>
**What to remember from this assignment:**
1. Preprocessing the dataset is important.
2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().
3. Tuning the learning rate (which is an example of a "hyperparameter") can make a big difference to the algorithm. You will see more examples of this later in this course!

## Week 3 Shallow NN

### 3-1 Neural Networks Overview

* What is a NN ?

$$
z^{[1]} = W^{[1]}x + b^{[1]} \\
a^{[1]} = \sigma(z^{[1]}) \\
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\
a^{[2]} = \sigma(z^{[2]}) \\
L(a^{[2]}, y)
$$

* superscript bracket refers to layer
* Then do backward propogation

$$
da^{[2]},dz^{[2]},dW^{[2]},db^{[2]},da^{[1]},dz^{[1]}, \cdots
$$

### 3-2 Neural Network Representation

1. input layer $x_1, \dots, x_{n_x}$
2. Hidden layer: the value is not seen in training set
3. Output layer: $y$
4. Notation: a: activation

$$
a^{[0]} \\
a^{[1]} =
\begin{bmatrix}
a^{[1]}_{1} \\
a^{[1]}_{2} \\
a^{[1]}_{3} \\
a^{[1]}_{4} \\
\end{bmatrix}
\\
a^{[2]} = \hat{y}
$$

5. When we count layer, we don't count input layer
6. Hidden layer and output layer are associated with $W^{[1]}, b^{[1]}; W^{[2]}, b^{[2]}$.

### 3-3 Computing a Neural Network's Output

1. The cycle in NN represents 2 steps of computation

$$
\text{logistic regression:} \\
z = w^Tx+b \\
a = \sigma(z) \\
\text{NN:} \\
z^{[1]}_1 = w^{[1]T}_1 x^{[1]}_1+b^{[1]}_1 \\
a^{[1]}_1 = \sigma(z^{[1]}_1) \\
W = \begin{bmatrix}
-w^{[1]}_{1}- \\
-w^{[1]}_{2}- \\
-w^{[1]}_{3}- \\
-w^{[1]}_{4}- \\
\end{bmatrix} \\
z^{[1]} = W^{[1]} x + b^{[1]} \\
a^{[1]} = \sigma(z^{[1]})
$$

### 3-4 Vectorizing across multiple examples

1. Notation: $x^{(1)} \rightarrow a^{[2](1)}$
2. Taking the training examples, stack them columnly

$$
X = [x^{(1)}, x^{(2)}, \dots, x^{(m)}] \\
Z^{[1]} = W^{[1]} X + b^{[1]} \\
Z^{[1]} = [z^{[1](1)}, \dots, z^{[1](m)}] \\
A^{[1]} = \sigma(Z^{[1]}) \\
A^{[1]} = [a^{[1](1)}, \dots, a^{[1](m)}] \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

3. The column of $Z^{[1]}, A^{[1]}$ corresponds to training examples, the row corresponds to
  hidden unit.
  
### 3-5 Explanation for Vectorized Implementation

* Justification for vectorized implementation

$$
z^{[1](1)} = W^{[1]}x^{(1)}, z^{[1](2)} = W^{[1]}x^{(2)}, z^{[1](3)} = W^{[1]}x^{(3)} \\
W^{[1]}
\begin{bmatrix}
| & | & | \\
x^{(1)} & x^{(2)} & x^{(3)}\\
| & | & | \\
\end{bmatrix} =
\begin{bmatrix}
| & | & | \\
z^{[1](1)} &z^{[1](2)} & z^{[1](3)}\\
| & | & | \\
\end{bmatrix} = Z^{[1]}
$$

* Recap of vectorization across multiple examples

```
for i = 1 to m
```
$$
z^{[1](i)} = W^{[1](i)} x + b^{[1]} \\
a^{[1](i)} = \sigma(z^{[1](i)}) \\
z^{[2](i)} = W^{[2](i)} x + b^{[2]} \\
a^{[2](i)} = \sigma(z^{[2](i)})
$$

* Vectorization version

$$
Z^{[1]} = W^{[1]} X + b^{[1]} \\
A^{[1]} = \sigma(Z^{[1]}) \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

### 3-6 Activation functions

1. Don't have to use sigmoid function, can use
    $\tanh = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ function
2. $\tanh$ almost always works better, because centering @ 0.0 instead of 0.5
3. exception is the output layer, because $\hat{y} \in \{0, 1\}$
4. When z is very large or small, the gradient is close to 0, then make the gradient descent slow.
5. Relu $a = \max (0,z)$
6. Rule of thumb
    1. binary classification: sigmoid for output layer.
    2. For hidden layer, Relu becomes more popular.
7. Pros and Cons
    1. sigmoid: only use for output layer
    2. Relu: default
    3. Leaky Relu: maybe $a = \max (0.01z, z)$
8. Always have a lot choices: # of hidden unit, choice functions, hard to know what might be best.

### 3-7 Why do you need non-linear activation functions?

1. Why not just use linear activation function: $a^{[1]} = z^{[1]}, a^{[2]} = z^{[2]}$
2. If so, we just have linear model: $a^{[2]} = W^{[2]}W^{[1]}x + (W^{[2]}b^{[1]}+b^{[2]})$,
   then many layer won't help.
3. One place to use linear activation function is the output layer for regressison function.

### 3-8 Derivatives of activation functions

1. Sigmoid activation function

$$
g(z) = \frac{1}{1+e^{-z}} \\
\frac{dg(z)}{dz} = -\frac{1}{(1+e^{-z})^2} (-e^{-z}) = g(z)\frac{e^{-z}}{1+e^{-z}} \\
= g(z)(1-g(z)) \\
a = g(z) \\
g'(z) = a(1-a)
$$

2. Tanh activation function

$$
g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
\frac{dg(z)}{dz} = -\frac{(e^z - e^{-z})^2}{(e^z + e^{-z})^2} + 1 = 1-g^2(z) \\
a = g(z) \\
\frac{dg(z)}{dz} = 1 - a^2
$$

3. ReLU: $g'(z) = 1$, for z >= 0, g'(z) = 0, for z < 0
4. Leaky ReLU: g'(z) = 1, for z >= 0, g'(z) = 0.01, for z < 0

### 3-9 Gradient descent for Neural Networks

$$
W^{[1]} (n^{[1]},n^{[0]}), b^{[1]}(n^{[1]},1),
W^{[2]} (n^{[2]}, n^{[1]}), b^{[2]} (n^{[2]}, 1) \\
n_x = n^{[0]}, n^{[1]}, n^{[2]} \\
\text{cost function:} \\
J = \frac{1}{m} = \sum_{i=1}^m L(\hat{y}, y)
$$

* Forward propogation:

$$
Z^{[1]} = W^{[1]} X + b^{[1]} \\
A^{[1]} = \sigma(Z^{[1]}) \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

* Backward propogation:

$$
dZ^{[2]} = A^{[2]} - Y \\
dW^{[2]} = \frac{1}{m} dZ^{[2]} A^{[1]T} \\
db^{[2]} = \frac{1}{m} np.sum(dZ^{[2]}, axis=1,keepdims = True) \\
dZ^{[1]} = W^{[2]T} dZ^{[2]} \cdot g'^{[1]} (Z^{[1]}) 
(\text{element-wise, both sizes are }(n^{[1]},m)) \\
dW^{[1]} = \frac{1}{m} dZ^{[1]}X^T \\
db^{[1]} = \frac{1}{m} \cdot np.sum(dZ^{[1]}, axis=1, keepdims = True)
$$

### 3-10 Backpropagation Intuition

### 3-11 Random Initialization

1. For NN, initialization to all 0 won't work

$$
a_1^{[1]} = a_2^{[1]} \\
dz_1^{[1]} = dz_2^{[1]}
$$

2. Symmetric, so every row is the same
3. Not helpful because want different hidden units to compute different hidden functions.

$$
W^{[1]} = np.random.rand((2,2)) \times 0.01 \\
b^{[1]} = np.zeros((2, 1)) \\
$$

4. why the 0.01 comes from? We want usually very small random numbers. Because if we use
  very big number, $a^{[1]}, \dots$ will be in the flat part, gradient descent will be slow.
5. In deep NN, we might want to choose different constant than 0.01.

## Week 4 Deep NN

### 4-1 Deep L-layer neural network

1. What is a DNN
2. View the # of layers as hyperparameters
3. $L$ # of layers
4. $n^{[l]}$ # of units in layer l
5. $a^{[l]}$ activation in layer l, $a^{[l]} = g^{[l]}(z^{[l]})$
6. $W^{[l]}$ weights for $z^{[l]}$, $b^{[l]}$
7. $x = a^{[0]}, \hat{y} = a^{[L]}$

### 4-2 Forward Propagation in a Deep Network

1. single example:

$$
z^{[1]} = W^{[1]} x + b^{[1]} \\
a^{[1]} = g^{[1]} (z^{[1]}) \\
z^{[2]} = W^{[2]} a^{[1]} + b^{[2]} \\
a^{[2]} = g^{[2]} (z^{[2]}) \\
\dots \\
z^{[4]} = W^{[4]} a^{[3]} + b^{[4]} \\
\hat{y} = a^{[4]} = g^{[4]} (z^{[4]}) \\
\text{In general: } \\
z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} = g^{[l]} (z^{[l]})
$$

2. vectorization version:

$$
Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]} \\
A^{[1]} = g^{[1]}(Z^{[1]}) \\
\text{In general: } \\
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]})
$$

3. There is one for loop iterate through layer 1 to layer L, no way to get rid of.

### 4-3 Getting your matrix dimensions right

1. Useful tool to debug, piece of paper to walk through dimensions of the Matrix.

$$
\text{In general: } \\
W^{[l]}, dW^{[l]}: (n^{[l]},n^{[l-1]}) \text{ matrix }\\
b^{[l]}, db^{[l]}: (n^{[l]}, 1) \text{ vector } \\
Z^{[l]}, A^{[l]}, dZ^{[l]}, dA^{[l]}: (n^{[l]}, m) \text{ matrix }\\
$$

2. Once make sure the dims are correct, it can eliminate some of the bugs.

### 4-4 Why deep representations?

1. Face detection:
2. First layer: detect edges
3. 2nd layer: detect parts of the face
4. 3nd layer: put parts of faces together then can detect
5. early layer detects simpler funcitons
6. composing to later layers.
7. Audio clip
8. First layer: low level audio waveform features.
9. 2nd layer: phonemes
10. 3rd layer: words
11. 4th layer: sentence/phrases
12. When NN gets deeper, it can do suprisingly complex things

* Circuit theory and deep learning

1. small L layer deep NN that can do exponential bigger shallower NN.
2. Andrew found this explanation less intuitive.

* "Deep learning" is great branding
* When starts, starts with something simple, e.g. logistic regression.

### 4-5 Building blocks of deep neural networks

* 1st focus on one layer

$$
\text{Forward: } \\
input: a^{[l-1]}, output: a^{[l]} \\
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]}) \\
cache: Z^{[l]}\\
\text{Backward: } \\
input: da^{[l]}, cached Z^{[l]},output:da^{[l-1]}, dW^{[l]}, db^{[l]}
$$

2. The block diagram is really good. **NEED to REMEMBER**!!

3. Implementatino notes: cache not just $Z^{[l]}$, but also $W^{[l]}, b^{[l]}$

### 4-6 Forward and Backward Propagation

* Forward propagation for layer l

1. Input $a^{[l-1]}$
2. output $a^{[l]}$, cache $z^{[l]}$

$$
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]}) \\
$$

* Backward propagation for layer l

1. Input $da^{[l]}$
2. output $da^{[l-1]}$, cache $z^{[l]}$

$$
dZ^{[l]} = dA^{[l]} \cdot g'^{[l]} (Z^{[l]}) \\
dW^{[l]} = \frac{1}{m} dZ^{[l]} A^{[l-1]T} \\
db^{[l]} = \frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims = True) \\
dA^{[l-1]} = W^{[l]T} \cdot dZ^{[l]} \\
dA^{[L]} = (-\frac{y^{(1)}}{a^{(1)}}+\frac{1-y^{(1)}}{1-a^{(1)}}, \dots, 
-\frac{y^{(m)}}{a^{(m)}}+\frac{1-y^{(m)}}{1-a^{(m)}})
$$

3. complexity comes from the data rather than from the code
4. When you done programming exercises, it will become more concrete.

### 4-7 Parameters vs Hyperparameters

1. parameters: $W, b$
2. hyper-parameters: learning rate, iterations, # of hidden layer, # of hidden units.
3. choice of activation functions.
4. Later: momentum, mini-batch size, regularization parameter forms.

* Apply deep learning is a very empirical process
* Idea -> Experiment -> code
* Vision, speech, NLP, Ad.
* Try a range of values to start with.
* The best hyper-parameters today might not be the best one 1 year later.
    * The computing infrastructure might change
    * Try every several months

### 4-8 What does this have to do with the brain?
