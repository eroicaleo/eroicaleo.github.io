---
title: "DeepLearningSpecialization"
author: "Yang Ge"
date: "10/23/2017"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Course 1

## Week 1

### Welcome

* What you will learn:

1. Neural networks and Deep Learning
2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.
3. Structuring your Machine Learning Project:
    * Train/dev/test model has changed
    * What if your training set and testing set come from a different distrbution?
      How to deal with it?
    * end to end deep learning, when to use and when not to use?
    * unique: share hard one lessons.
4. Convolutional Neural Networks (CNN)
    * Often apply to images
5. Natural Language Processing: Building Sequence models
    * RNN
    * LSTM (Long short term memory model)
    * Apply to speech recognition

### What is a neural network?

* 'Deep learning' refers to train (sometimes very large) neural networks
* House price prediction
    * Linear regression: Straight line
    * Very simple NN: Size(x) -> Cycle -> Price(y)
    * Cycle is neuron: Input the size, compute the linear function, output the price.
    * ReLu function: Rectified Linear Unit, takes maximum or 0
    * Large neural is taking many single neurons and stacking together
* House price prediction 2
    * size and # bdrm -> family size
    * zip code -> walkability
    * zip code + wealth -> school quality
    * family size + walkability + school quality -> price
    * The power and magic of NN: just need to give X and y, all the things in the middle
      will be figured out by itself.
    * Given enough training examples, NN good at finding functions
    
### Supervised learning with NN

* Supervised Learning

| Input(x)     | Output(y)     | Application |
| :------------- | :------------- | :--------|
| Home features | Price   | Real Estate (Standard NN) |
| Ad, user info | Click on ad? (0/1) | Online Advertising (Standard NN) |
| Image | Object(1, ..., 1000) | Photo tagging (CNN) |
| Audio | Text Transcript | Speech recognition (RNN) |
| English | Chinese | Machine translation (sequence data: complex RNN) |
| Image, Radar info | Position of a car | Autonomous driving (custom/hybrid architecture) |

* Needs to cleverly select what needs to be X and y
* Neural network examples
* Structured Data (database of data, housing, AD)
* Unstructured Data (raw audio, images, text)
    * Hard to computer to understand historically
    * Human is good to understand this
    * Thanks to deep learning, computers are much better than years ago
    
### Why is deep learning taking off

* Scale drives deep learning progress
    * SVM, logistical regression: Performance saturated even data scales
    * Train very large NN: performance keeps getting better and better
* Need 2 things to get very good performance:
    * Very big NN
    * Do need a lot of data
* Use `m` in lower case to denote amount of data
* For small training set, SVM might do better than NN.
* Large training set, large NN dominate performance.
* At the beginning, large training set + faster machines help.
* Recent years, algorithm innovation helps.
    * A lot of algorithm helps NN run faster.
    * One example: sigmoid function to ReLu function. Because at the two end of sigmoid function,
      the slop is flat, the gradient decent is slow.
* Idea -> Code -> Experiment -> Idea, so fast computation is really important.
    * Now can get results in 1day/10min, before it can be 1mon
    * This way can do much more experiments
    
### About this course

* Week1 Introduction
* Week2 Basic NN programming
    * Will code backward/forward propogation
* Week3 One single layer NN
* Week4 Deep NN

### Course resources

* Discussion forums
    * Question, tech details, bugs.
* [feedback,enterprise,academic]@deeplearning.ai

## Week2 Logistic Regression as a NN

### 2.1 Binary Classification

* In NN implementation, we don't want iterate the `m` training set with an explicit for loop.
* We will learn why computation can be organized as forward propogation + backward propogation
* How Computer store image: red/green/color channel
    * 64x64 picture, then 3 64x64 matrices.
    * Unroll all the pixels, input vector dimension: 12288 $n= n_x = 12288$

#### Notation

$$
(x,y), x \in R^{n_x}, y \in \{0,1\} \\
\{(x^{(1)}, y^{(1)}),\dots\} \\
M_{\text{train}}, M_{\text{test}} \\
X = [x^{(1)}, \dots, x^{(m)}], n_x \text{ by } m, \in R^{n_x \times m} \\
Y = [y^{(1)}, \dots, y^{(m)}] \in R^{1 \times m}
$$

* python cmd: `X.shape, Y.shape`

### 2.2 Logistic Regression

1. Given $x \in R^{n_x}$, want $\hat{y} = P(y=1|x)$
2. Parameters: $w \in R^{n_x}, b \in R$
3. Doesn't work output: $\hat{y} = w^Tx + b$
4. Output: $\hat{y} = \sigma(w^Tx + b)$, sigmoid function

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

5. When z large, it's 1, when z large negative, then it's 0.
6. Note: In programming NN, we usually keep w and b separate. It's easier for NN implementation.

### 2.3 Logistic Regression Cost Function

1. Given $\{(x^{(1)}, y^{(1)}),\dots\, (x^{(m)}, y^{(m)})\}$, want $\hat{y}^{(i)} \approx y^{(i)}$
2. Loss error function: $L(\hat{y}, y) = 0.5(\hat{y} - y)^2$ Not working well
3. Working well: $L(\hat{y}, y) = -(y \log \hat{y} + (1-y) \log (1-\hat{y}))$

$$
y =1 \implies L(\hat{y}, y) = -\log \hat{y} \implies \log \hat{y} \text{ large }
\implies \hat{y} \text{ large } \\
y = 0 \implies L(\hat{y}, y) = -\log (1-\hat{y}) \implies \log (1- \hat{y}) \text{ large }
\implies \hat{y} \text{ small }
$$
4. Loss functino is defined in a single training example, but cost function is defined for
   measuring how well it's doing for entire training functions.

$$
J(w, b) = \frac{1}{M} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)}) \\
= -\frac{1}{M} \sum_{i=1}^m 
y^{(i)} \log \hat{y}^{(i)} + 
(1-y^{(i)}) \log (1-\hat{y}^{(i)})
$$

5. Logistic regression can be viewed as very small neural network.
   
### 2.3 Gradient Descent

* Recap

$$
\hat{y} = \sigma(w^Tx + b) \\
\sigma(z) = \frac{1}{1+e^{-z}} \\
J(w, b) = \frac{1}{m} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})
= -\frac{1}{m} \sum_{i=1}^m 
y^{(i)} \log \hat{y}^{(i)} + 
(1-y^{(i)}) \log (1-\hat{y}^{(i)})
$$

1. Want to find w, b to minimize J(w, b), which is a convex function, one of huge reason to use
    such function as cost function.
2. Initialize $w,b = 0$


1. Repeat {
2.   $w := w - \alpha \frac{\partial J(w,b)}{\partial w}$
3.   $b := b - \alpha \frac{\partial J(w,b)}{\partial b}$
4. }


1. alpha is learning rate
2. `dw, db` to be variable for derivative term in coding python.
3. $ \frac{\partial J(w)}{\partial w}$ slope of the function

### 2.4 Derivatives

1. Think derivatives as slope of the function
2. height / width.

### 2.5 More Derivative Examples

### 2.6 Computation graph

$$
J(a, b, c) = 3a + bc
$$

1. Computation graph comes into handy when you have a function to optimize
2. Computation graph organize the computation with the blue arrow from left to right

### 2.7 Derivatives with a Computation Graph

1. Chain rule: a -> v -> J
2. dFinalOutputVar / dVar, but in code: `dvar`, like `dv, da`

### 2.8 Logistic Regression Gradient Descent

$$
da = \frac{dL(a, y)}{da} = -\frac{y}{a} + \frac{1-y}{1-a} \\
dz = a - y \\
dw1 = x_1 \cdot dz, dw2 = x_2 \cdot dz, db = dz
$$

### 2.9 Gradient Descent on m Examples

1. J = 0, dw1 = 0, dw2 = 0, db = 0
2. for i = 1 to m
3.     $z^{(i)} = w^Tx^{(i)}+b$
4.     $a^{(i)} = \sigma(z^{(i)})$
5.     $J += -[y^{(i)} \log a^{(i)} + (1- y^{(i)}) \log (1 - a^{(i)})]$
6.     $dz^{(i)} = a^{(i)} - y^{(i)}$
7.     $dw_1 += x_1^{(i)} dz^{(i)}$
8.     $dw_2 += x_2^{(i)} dz^{(i)}$
9.     $db += dz^{(i)}$
10. J/=m, dw1 /= m, dw2 /= m, db /= m
11. w1 := w1 - alpha dw1 and so on

* Two weakness:

1. For loop training ex
2. For loop for features.
3. Without using for loop helps scaling to bigger data set
4. In deep-learning era, vectorization is a must.

### 2.10 Vectorization

1. What is vectorization?
2. $z = w^Tx+b$
3. python: `z = np.dot(w, x) + b` 100X faster than for loop in python
4. both GPU and CPU have SIMD
5. Whenever possible, avoid use explicit for loop

### 2.11 More Vectorization Examples

1. Rule of Thumb: whenever possible, avoid explicit for-loops
2. `u = np.dot(a, v)`
3.

```
import numpy as np
u = np.exp(v)
np.log(v)
np.abs(v)
np.maximum(v, 0)
v**2
1/v
```

4. Logistic regression derivatives

```
dw = np.zeros((n_x, 1))
dw += x * dz
dw /= m
```

### 2.12 Vectorizing Logistic Regression

$$
z^{(1)} = w^Tx^{(1)}+b \\
a^{(1)} = \sigma(z^{(1)}) \\
z^{(2)} = w^Tx^{(2)}+b \\
a^{(2)} = \sigma(z^{(2)}) \\
z^{(3)} = w^Tx^{(3)}+b \\
a^{(3)} = \sigma(z^{(3)}) \\
$$

$$
[z^{(1)}, z^{(2)}, \cdots, z^{(m)}] = w^T X + [b, b, \cdots, b]
$$

* python code

```
Z = np.dot(w.T, X) + b
```

* Compute a leave to programming assignment.

### 2.13 Vectorizing Logistic Regression's Gradient Output

$$
dZ = [dz^{(1)}, dz^{(2)}, \cdots, dz^{(m)}]
dZ = A - Y
$$

* python code for db and dw

```
db = 1/m * np.sum(dZ)
dw = 1/m * np.dot(X, dZ.T)
```

* Implementing Logistic Regression

$$
Z = w^T X + b \\
A = \sigma (Z) \\
dZ = A - Y \\
dw = 1 / m  X (dZ)^T \\
db = 1 / m \sum dZ
$$

* Still need to do for loop over iterations and no way to get rid of it.

### 2.14 Broadcasting in Python

1. Broadcasting example:

```
cal = A.sum(axis=0)
lala = cal.reshape(1, 4)
percentage = 100 * A / lala
```

2. `axis=0` means sum vertically.
3. `A / cal.reshape(1, 4)` is broadcasting, although redundant, 
    `reshape` is constant cheap call, and make the code clear.
4. `(m,n) + (1,n)`: copy `(1,n)` m times, so the results are `(m,n)`
5. `(m,n) + (m,1)`: copy `(m,1)` n times, so the results are `(m,n)`

* General principle

1. `(m,n) +-*/ (1,n)`: copy `(1,n)` m times, so the results are `(m,n)`
2. `(m,n) +-*/ (m,1)`: copy `(m,1)` n times, so the results are `(m,n)`
3. more general, look for numpy doc, broadcasting. [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

### 2.15 A note on python/numpy vectors

1. weekness: easily introducing subtle bugs
2. Commit to Use row/column explicitly: `a = np.random.rand(5,1)`
3. `a = np.random.rand(5)` creates rank 1 array. DON'T USE.
4. If not sure the dimension, do this `assert(A.shape == (5,1))`, inexpensive.
5. reshape rank 1 array by `a = a.reshape(1,5)`

### 2.16 Quick tour of Jupyter/iPython Notebooks

### 2.17 Python Basics With Numpy v3

<font color='blue'>
**What you need to remember:**
- np.exp(x) works for any np.array x and applies the exponential function to every coordinate
- the sigmoid function and its gradient
- image2vector is commonly used in deep learning
- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. 
- numpy has efficient built-in functions
- broadcasting is extremely useful

**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and the `*` operator (which is equivalent to  `.*` in Matlab/Octave), which performs an element-wise multiplication.

<font color='blue'>
**What to remember:**
- Vectorization is very important in deep learning. It provides computational efficiency and clarity.
- You have reviewed the L1 and L2 loss.
- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...

### 2.18 Logistic Regression with a Neural Network mindset

<font color='blue'>
**What you need to remember:**

Common steps for pre-processing a new dataset are:
- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
- Reshape the datasets such that each example is now a vector of size (num_px \* num_px \* 3, 1)
- "Standardize" the data

<font color='blue'>
**What to remember:**
You've implemented several functions that:
- Initialize (w,b)
- Optimize the loss iteratively to learn parameters (w,b):
    - computing the cost and its gradient 
    - updating the parameters using gradient descent
- Use the learned (w,b) to predict the labels for a given set of examples

**Interpretation**: 
- Different learning rates give different costs and thus different predictions results.
- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). 
- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.
- In deep learning, we usually recommend that you: 
    - Choose the learning rate that better minimizes the cost function.
    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) 

<font color='blue'>
**What to remember from this assignment:**
1. Preprocessing the dataset is important.
2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().
3. Tuning the learning rate (which is an example of a "hyperparameter") can make a big difference to the algorithm. You will see more examples of this later in this course!

<font color='black'>

## Week 3 Shallow NN

### 3-1 Neural Networks Overview

* What is a NN ?

$$
z^{[1]} = W^{[1]}x + b^{[1]} \\
a^{[1]} = \sigma(z^{[1]}) \\
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\
a^{[2]} = \sigma(z^{[2]}) \\
L(a^{[2]}, y)
$$

* superscript bracket refers to layer
* Then do backward propogation

$$
da^{[2]},dz^{[2]},dW^{[2]},db^{[2]},da^{[1]},dz^{[1]}, \cdots
$$

### 3-2 Neural Network Representation

1. input layer $x_1, \dots, x_{n_x}$
2. Hidden layer: the value is not seen in training set
3. Output layer: $y$
4. Notation: a: activation

$$
a^{[0]} \\
a^{[1]} =
\begin{bmatrix}
a^{[1]}_{1} \\
a^{[1]}_{2} \\
a^{[1]}_{3} \\
a^{[1]}_{4} \\
\end{bmatrix}
\\
a^{[2]} = \hat{y}
$$

5. When we count layer, we don't count input layer
6. Hidden layer and output layer are associated with $W^{[1]}, b^{[1]}; W^{[2]}, b^{[2]}$.

### 3-3 Computing a Neural Network's Output

1. The cycle in NN represents 2 steps of computation

$$
\text{logistic regression:} \\
z = w^Tx+b \\
a = \sigma(z) \\
\text{NN:} \\
z^{[1]}_1 = w^{[1]T}_1 x^{[1]}_1+b^{[1]}_1 \\
a^{[1]}_1 = \sigma(z^{[1]}_1) \\
W = \begin{bmatrix}
-w^{[1]}_{1}- \\
-w^{[1]}_{2}- \\
-w^{[1]}_{3}- \\
-w^{[1]}_{4}- \\
\end{bmatrix} \\
z^{[1]} = W^{[1]} x + b^{[1]} \\
a^{[1]} = \sigma(z^{[1]})
$$

### 3-4 Vectorizing across multiple examples

1. Notation: $x^{(1)} \rightarrow a^{[2](1)}$
2. Taking the training examples, stack them columnly

$$
X = [x^{(1)}, x^{(2)}, \dots, x^{(m)}] \\
Z^{[1]} = W^{[1]} X + b^{[1]} \\
Z^{[1]} = [z^{[1](1)}, \dots, z^{[1](m)}] \\
A^{[1]} = \sigma(Z^{[1]}) \\
A^{[1]} = [a^{[1](1)}, \dots, a^{[1](m)}] \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

3. The column of $Z^{[1]}, A^{[1]}$ corresponds to training examples, the row corresponds to
  hidden unit.
  
### 3-5 Explanation for Vectorized Implementation

* Justification for vectorized implementation

$$
z^{[1](1)} = W^{[1]}x^{(1)}, z^{[1](2)} = W^{[1]}x^{(2)}, z^{[1](3)} = W^{[1]}x^{(3)} \\
W^{[1]}
\begin{bmatrix}
| & | & | \\
x^{(1)} & x^{(2)} & x^{(3)}\\
| & | & | \\
\end{bmatrix} =
\begin{bmatrix}
| & | & | \\
z^{[1](1)} &z^{[1](2)} & z^{[1](3)}\\
| & | & | \\
\end{bmatrix} = Z^{[1]}
$$

* Recap of vectorization across multiple examples

```
for i = 1 to m
```
$$
z^{[1](i)} = W^{[1](i)} x + b^{[1]} \\
a^{[1](i)} = \sigma(z^{[1](i)}) \\
z^{[2](i)} = W^{[2](i)} x + b^{[2]} \\
a^{[2](i)} = \sigma(z^{[2](i)})
$$

* Vectorization version

$$
Z^{[1]} = W^{[1]} X + b^{[1]} \\
A^{[1]} = \sigma(Z^{[1]}) \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

### 3-6 Activation functions

1. Don't have to use sigmoid function, can use
    $\tanh = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ function
2. $\tanh$ almost always works better, because centering @ 0.0 instead of 0.5
3. exception is the output layer, because $\hat{y} \in \{0, 1\}$
4. When z is very large or small, the gradient is close to 0, then make the gradient descent slow.
5. Relu $a = \max (0,z)$
6. Rule of thumb
    1. binary classification: sigmoid for output layer.
    2. For hidden layer, Relu becomes more popular.
7. Pros and Cons
    1. sigmoid: only use for output layer
    2. Relu: default
    3. Leaky Relu: maybe $a = \max (0.01z, z)$
8. Always have a lot choices: # of hidden unit, choice functions, hard to know what might be best.

### 3-7 Why do you need non-linear activation functions?

1. Why not just use linear activation function: $a^{[1]} = z^{[1]}, a^{[2]} = z^{[2]}$
2. If so, we just have linear model: $a^{[2]} = W^{[2]}W^{[1]}x + (W^{[2]}b^{[1]}+b^{[2]})$,
   then many layer won't help.
3. One place to use linear activation function is the output layer for regressison function.

### 3-8 Derivatives of activation functions

1. Sigmoid activation function

$$
g(z) = \frac{1}{1+e^{-z}} \\
\frac{dg(z)}{dz} = -\frac{1}{(1+e^{-z})^2} (-e^{-z}) = g(z)\frac{e^{-z}}{1+e^{-z}} \\
= g(z)(1-g(z)) \\
a = g(z) \\
g'(z) = a(1-a)
$$

2. Tanh activation function

$$
g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
\frac{dg(z)}{dz} = -\frac{(e^z - e^{-z})^2}{(e^z + e^{-z})^2} + 1 = 1-g^2(z) \\
a = g(z) \\
\frac{dg(z)}{dz} = 1 - a^2
$$

3. ReLU: $g'(z) = 1$, for z >= 0, g'(z) = 0, for z < 0
4. Leaky ReLU: g'(z) = 1, for z >= 0, g'(z) = 0.01, for z < 0

### 3-9 Gradient descent for Neural Networks

$$
W^{[1]} (n^{[1]},n^{[0]}), b^{[1]}(n^{[1]},1),
W^{[2]} (n^{[2]}, n^{[1]}), b^{[2]} (n^{[2]}, 1) \\
n_x = n^{[0]}, n^{[1]}, n^{[2]} \\
\text{cost function:} \\
J = \frac{1}{m} = \sum_{i=1}^m L(\hat{y}, y)
$$

* Forward propogation:

$$
Z^{[1]} = W^{[1]} X + b^{[1]} \\
A^{[1]} = \sigma(Z^{[1]}) \\
Z^{[2]} = W^{[2]} X + b^{[2]} \\
A^{[2]} = \sigma(Z^{[2]})
$$

* Backward propogation:

$$
dZ^{[2]} = A^{[2]} - Y \\
dW^{[2]} = \frac{1}{m} dZ^{[2]} A^{[1]T} \\
db^{[2]} = \frac{1}{m} np.sum(dZ^{[2]}, axis=1,keepdims = True) \\
dZ^{[1]} = W^{[2]T} dZ^{[2]} \cdot g'^{[1]} (Z^{[1]}) 
(\text{element-wise, both sizes are }(n^{[1]},m)) \\
dW^{[1]} = \frac{1}{m} dZ^{[1]}X^T \\
db^{[1]} = \frac{1}{m} \cdot np.sum(dZ^{[1]}, axis=1, keepdims = True)
$$

### 3-10 Backpropagation Intuition

### 3-11 Random Initialization

1. For NN, initialization to all 0 won't work

$$
a_1^{[1]} = a_2^{[1]} \\
dz_1^{[1]} = dz_2^{[1]}
$$

2. Symmetric, so every row is the same
3. Not helpful because want different hidden units to compute different hidden functions.

$$
W^{[1]} = np.random.rand((2,2)) \times 0.01 \\
b^{[1]} = np.zeros((2, 1)) \\
$$

4. why the 0.01 comes from? We want usually very small random numbers. Because if we use
  very big number, $a^{[1]}, \dots$ will be in the flat part, gradient descent will be slow.
5. In deep NN, we might want to choose different constant than 0.01.

## Week 4 Deep NN

### 4-1 Deep L-layer neural network

1. What is a DNN
2. View the # of layers as hyperparameters
3. $L$ # of layers
4. $n^{[l]}$ # of units in layer l
5. $a^{[l]}$ activation in layer l, $a^{[l]} = g^{[l]}(z^{[l]})$
6. $W^{[l]}$ weights for $z^{[l]}$, $b^{[l]}$
7. $x = a^{[0]}, \hat{y} = a^{[L]}$

### 4-2 Forward Propagation in a Deep Network

1. single example:

$$
z^{[1]} = W^{[1]} x + b^{[1]} \\
a^{[1]} = g^{[1]} (z^{[1]}) \\
z^{[2]} = W^{[2]} a^{[1]} + b^{[2]} \\
a^{[2]} = g^{[2]} (z^{[2]}) \\
\dots \\
z^{[4]} = W^{[4]} a^{[3]} + b^{[4]} \\
\hat{y} = a^{[4]} = g^{[4]} (z^{[4]}) \\
\text{In general: } \\
z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} = g^{[l]} (z^{[l]})
$$

2. vectorization version:

$$
Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]} \\
A^{[1]} = g^{[1]}(Z^{[1]}) \\
\text{In general: } \\
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]})
$$

3. There is one for loop iterate through layer 1 to layer L, no way to get rid of.

### 4-3 Getting your matrix dimensions right

1. Useful tool to debug, piece of paper to walk through dimensions of the Matrix.

$$
\text{In general: } \\
W^{[l]}, dW^{[l]}: (n^{[l]},n^{[l-1]}) \text{ matrix }\\
b^{[l]}, db^{[l]}: (n^{[l]}, 1) \text{ vector } \\
Z^{[l]}, A^{[l]}, dZ^{[l]}, dA^{[l]}: (n^{[l]}, m) \text{ matrix }\\
$$

2. Once make sure the dims are correct, it can eliminate some of the bugs.

### 4-4 Why deep representations?

1. Face detection:
2. First layer: detect edges
3. 2nd layer: detect parts of the face
4. 3nd layer: put parts of faces together then can detect
5. early layer detects simpler funcitons
6. composing to later layers.
7. Audio clip
8. First layer: low level audio waveform features.
9. 2nd layer: phonemes
10. 3rd layer: words
11. 4th layer: sentence/phrases
12. When NN gets deeper, it can do suprisingly complex things

* Circuit theory and deep learning

1. small L layer deep NN that can do exponential bigger shallower NN.
2. Andrew found this explanation less intuitive.

* "Deep learning" is great branding
* When starts, starts with something simple, e.g. logistic regression.

### 4-5 Building blocks of deep neural networks

* 1st focus on one layer

$$
\text{Forward: } \\
input: a^{[l-1]}, output: a^{[l]} \\
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]}) \\
cache: Z^{[l]}\\
\text{Backward: } \\
input: da^{[l]}, cached Z^{[l]},output:da^{[l-1]}, dW^{[l]}, db^{[l]}
$$

2. The block diagram is really good. **NEED to REMEMBER**!!

3. Implementatino notes: cache not just $Z^{[l]}$, but also $W^{[l]}, b^{[l]}$

### 4-6 Forward and Backward Propagation

* Forward propagation for layer l

1. Input $a^{[l-1]}$
2. output $a^{[l]}$, cache $z^{[l]}$

$$
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} = g^{[l]}(Z^{[l]}) \\
$$

* Backward propagation for layer l

1. Input $da^{[l]}$
2. output $da^{[l-1]}$, cache $z^{[l]}$

$$
dZ^{[l]} = dA^{[l]} \cdot g'^{[l]} (Z^{[l]}) \\
dW^{[l]} = \frac{1}{m} dZ^{[l]} A^{[l-1]T} \\
db^{[l]} = \frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims = True) \\
dA^{[l-1]} = W^{[l]T} \cdot dZ^{[l]} \\
dA^{[L]} = (-\frac{y^{(1)}}{a^{(1)}}+\frac{1-y^{(1)}}{1-a^{(1)}}, \dots, 
-\frac{y^{(m)}}{a^{(m)}}+\frac{1-y^{(m)}}{1-a^{(m)}})
$$

3. complexity comes from the data rather than from the code
4. When you done programming exercises, it will become more concrete.

### 4-7 Parameters vs Hyperparameters

1. parameters: $W, b$
2. hyper-parameters: learning rate, iterations, # of hidden layer, # of hidden units.
3. choice of activation functions.
4. Later: momentum, mini-batch size, regularization parameter forms.

* Apply deep learning is a very empirical process
* Idea -> Experiment -> code
* Vision, speech, NLP, Ad.
* Try a range of values to start with.
* The best hyper-parameters today might not be the best one 1 year later.
    * The computing infrastructure might change
    * Try every several months

### 4-8 What does this have to do with the brain?

# Course 2

## Week 1 Practical aspects of Deep Learning

### 1-1 Train / Dev / Test sets

1. Applied ML is a highly iterative process: # of layers, # of hidden units, learning rate
    activation functions
2. Idea -> Code -> Experiment
3. Intuition from one domain, often do not transfer to another.
4. Almost impossible to correctly guess all the parameters very first time.
5. Train/dev/test sets
6. Previous example: 60/20/20 for 100/1000/10000 samples.
7. Big data: 1M samples, 10000 might be more than enough for dev and test, 98/1/1%

* Mismatched train/test distribution

1. Training set: Cat pictures from web, HD, clear
2. Dev/test set: from uploaded users, low resolution, etc
3. Rule of Thumb: dev and test come from same distribution
4. Not having a test set might be OK.

### 1-2 Bias / Variance

1. High bias: underfitting
2. High variance: overfitting
3. Train set / Dev set error: 
    * 1/11% high variance
    * 15/16% high bias
    * 15/30% both high variance + bias
4. But it depends also on optimal (bayes) error, blurry image.
5. Assumptions: 1. bayes error is small 2. draw from the same distribution
6. What does a high variance + bias look like?
    * Don't have flexibility for most data, e.g. a straight line
    * Have great flexibility for outliers.

### 1-3 Basic Recipe for Machine Learning

1. Does it has high bias? training set performance?
    1. Bigger network (always work better)
    2. Train longer (not always, but doesn't hurt)
    3. Neural network architecture (might not work)
2. Does it have high variance (after Q1 is solved)? Dev set performance?
    1. Get more data (pretty much always work)
    2. Regularization
    3. Neural network architecture (might not work)
3. Hopefully, find something low bias + low variance, DONE :)
4. high bias, not gonna help
5. Earlier ML, discussion about "Bias variance tradeoff".
6. Modern deep learning big data era, we can reduce without increasing other one.
7. Regularization might increase bias a little bit, but won't be too much.

### 1-4 Regularization

* Logistic regression

$$
\underset{w,b}{\text{min } } J(w, b) = 
\frac{1}{m} \sum_{i = 1}^{m} L(\hat{y}^{(i)}, y^{(i)}) +
\frac{\lambda}{2m} ||w||^2_2 \\
\text{L2 regularization: } ||w||^2_2 = w^Tw \\
\text{L1 regularization: } ||w||_1 = \sum_{i = 1}^{n_x} |w|
$$

1. In practise, $b$ is usually not included.
2. L1 will make $w$ sparse, not used that much.
3. L2 is much much often.
3. $\lambda$ regularization parameter, determined by dev set.
4. In programming exercises, use `lambd`

* Neural network

$$
J(W^{[1]}, b^{[1]}, \dots, W^{[L]}, b^{[L]}) =
\frac{1}{m} \sum_{i = 1}^{m} L(\hat{y}^{(i)}, y^{(i)}) +
\frac{\lambda}{2m} \sum_{i = 1}^{m} ||W^{[L]}||^2_F \\
dW^{[L]} = \text{( from back propogation )} + \frac{\lambda}{m} W^{[L]}
$$

1. It's also called weight decay.

### 1-5 Why regularization reduces overfitting?

1. When $\lambda$ is big, $W^{[L]} \approx 0$, NN becomes simpler and reduce overfitting and
  close to linear model.
2. $z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}$, $W^{[L]}$ is small then $z^{[L]}$ is small and will
  be in the linear region. Then every neoron is linear, the whole model is linear.
3. implementation tip for debugging, plot the cost function with the iteration.
  When you include regularization, needs to plot the whole cost function, not just the loss 
  function.

### 1-6 Dropout Regularization

1. For each training example, remove some nodes with certain probability.

* Implementing dropout ("Invert dropout")

```
# keep_prob = 0.8 in this example
# d3 will be a boolean array
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
a3 = np.multiply(a3, d3)
a3 /= keep_prob # bump 
```

* 问题：看起来以上的implementation，for different training examples, it has different
  dropout. How to do backward propogation??

1. The reason for division: 50 units -> 10 units shut off, want to keep the expectation
  of $a^{[4]}$ the same

$$
z^{[4]} = W^{[4]} \cdot a^{[3]} + b^{[4]}
$$

* Making predictions at test time: There is no drop out.

### 1-7 Understanding Dropout

1. Intuition: can't rely on any one feature, so have to spread out weights.
2. So if won't put too much weight on any one input.
3. Shrink the squared norm weight.
4. Dropout has a similar effect as the L2 regularization.
5. Errata: 2:50 the size of the matrix
6. For matrix with big size, apply higher drop out.
7. The input layer can also have dropout, but usually don't do that.
8. Computer vision has successful application. Input is so big, never have enough data.
9. Downside: $L$ is no longer well defined. So loosing the debugging tool.
10: Andrew recommend: first turn off drop out, make sure bug-free. Then turn on drop out.

### 1-8 Other regularization methods

* Data augmentation, because we want to get more data.
    * flipping image
    * randomly distortion and transformation, e.g. zooming in, rotation
* Early stopping
    * plot training error or J.
    * Also plot dev set, decrease first than increase.
    * Reason: at beginning, $|W|$ tends to be small, in later interation, $|W|$ gets bigger
      and bigger.
    * stop half way, $||W^{[L]}||^2_F$ won't be too big.
* Andrew's experience, orthogonalization, divide ML to 2 tasks, focus one at a time
    * Optimize cost function J
    * Not overfit
    * Use early stopping cannot decouple the 2 tasks because trying to do the 2 together
    * L2 regularization can decouple the 2 tasks, but need to try different lambda

### 1-9 Normalizing inputs

1. substract mean

$$
\text{ mean: } \\
\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)} \\
x = x - \mu \\
\text{ Normalize variance: } \\
\sigma^2 = \frac{1}{m} \sum_{i=1}^m x^{(i)} ** 2 \\
x = x / \sigma^2
$$

2. Needs to use same training set and test set

* Why normalize inputs?

1. Not normalized, conture will be elongated, gradient descent will go back and forth.
2. Normalized, conture will be more symmetric, gradient descent will go straight to minimum.

* Never does any harm

### 1-10 Vanishing / Exploding gradients

1. Sometimes the gradient can be very very big or very very small.
2. If $w > 1$, then very deep network $w^L$ will be exponential big. The same thing will be when
    $w < 1$
3. The same argument can be said for the derivatives.
4. This was a major barrier before.

### 1-11 Weight Initialization for Deep Networks

* Single neuron example

$$
z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n
$$

1. We would like to have the larger n, the smaller $w$. What we can do is to set $var(w) = \frac{1}{n}$ for sigmoid and tanh activation, and $var(w) = \frac{2}{n}$ for relu .
2. Python code as follows

```
WL = np.random.randn(shape) * np.sqrt(1/n[l-1]) # for sigmoid and tanh
WL = np.random.randn(shape) * np.sqrt(2/n[l-1]) # for Relu function
```

### 1-12 Numerical approximation of gradients

1. Check your derivative computation
2. Use 2 sides difference, error is $O(\epsilon^2)$
3. Use 1 side difference, error is $O(\epsilon)$

### 1-13 Gradient checking

1. Helps Andrew a lot to find bugs.
2. Take $W^{[1]}, b^{[1]}, \dots, W^{[L]}, b^{[L]}$ into a giant vector $\theta$
3. Take $dW^{[1]}, db^{[1]}, \dots, dW^{[L]}, db^{[L]}$ into a giant vector $d\theta$

* Gradient checking (Grad check)

$$
\text{for each } i: \\
d\theta_{approx}[i] = \frac
{J(\theta_1, \theta_2, \dots, \theta_i + \epsilon, \dots)
- J(\theta_1, \theta_2, \dots, \theta_i - \epsilon, \dots)}
{2 \epsilon} \approx
d\theta[i] = 
\frac{\partial J}
{\partial\theta_i} \\
\text{ check } \\
\frac{||d\theta_{approx} - d\theta||_{2}}
{||d\theta_{approx}||_2 + ||d\theta||_2}
\approx \epsilon = 10^{-7}
$$

1. If the $\epsilon >= 10^{-3}$, most likely there is a bug.
2. Help Andrew a lot in finding bugs.

### 1-14 Gradient Checking Implementation Notes

1. Don't use in training - only to debug
2. If algorithm fails grad check, look at components to try to identify bug.
    1. E.g. $db^{[l]}$ is off, but $dW^{[l]}$ is close, then check how to compute $db^{[l]}$
3. Remember regularization term: include it in gradient descent if use regularization.
4. Doesn't work with dropout
    1. Turn on grad check without drop out.
    2. Turn off grad and turn on drop out, hope for the best.
    3. Could do but Andrew don't usually do: Fix pattern of drop out.
5. Run a random initialization. run it again after some training.

### 1-15 Programming assignment

* Initialization:

<font color='blue'>
**What you should remember**:
- The weights $W^{[l]}$ should be initialized randomly to break symmetry. 
- It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly.

**Observations**:
- The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $\log(a^{[3]}) = \log(0)$, the loss goes to infinity.
- Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. 
- If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.

<font color='blue'>
**In summary**:
- Initializing weights to very large random values does not work well. 
- Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part! 

<font color='blue'>
**What you should remember from this notebook**:
- Different initializations lead to different results
- Random initialization is used to break symmetry and make sure different hidden units can learn different things
- Don't intialize to values that are too large
- He initialization works well for networks with ReLU activations. 

* Regularization

**Observations**:
- The value of $\lambda$ is a hyperparameter that you can tune using a dev set.
- L2 regularization makes your decision boundary smoother. If $\lambda$ is too large, it is also possible to "oversmooth", resulting in a model with high bias.

**What is L2-regularization actually doing?**:

L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. 

<font color='blue'>
**What you should remember** -- the implications of L2-regularization on:
- The cost computation:
    - A regularization term is added to the cost
- The backpropagation function:
    - There are extra terms in the gradients with respect to weight matrices
- Weights end up smaller ("weight decay"): 
    - Weights are pushed to smaller values.

**Note**:
- A **common mistake** when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. 
- Deep learning frameworks like [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) or [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) come with a dropout layer implementation. Don't stress - you will soon learn some of these frameworks.

<font color='blue'>
**What you should remember about dropout:**
- Dropout is a regularization technique.
- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.
- Apply dropout both during forward and backward propagation.
- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.  

<font color='blue'>
**What we want you to remember from this notebook**:
- Regularization will help you reduce overfitting.
- Regularization will drive your weights to lower values.
- L2 regularization and Dropout are two very effective regularization techniques.
# Python functions used through the courses

<font color='black'>

## Week 2 Optimization algorithms

### 2-1 Mini-batch gradient descent

1. Vectorization allows you to efficiently compute on m examples.
2. When m = 5000000, vectorization can still be very slow.
3. Take mini-batch of 1000 examples. Then we have 5000 mini-batches.

$$
X = [x^{(1)}, \dots, x^{(5000000)}] \\
X^{\{1\}} = [x^{(1)}, \dots, x^{(1000)}] (n_x, 1000) \\
X^{\{2\}} = [x^{(1001)}, \dots, x^{(2000)}] (n_x, 1000) \\
\dots \\
X^{\{5000\}} = [x^{(4995000)}, \dots, x^{(5000000)}] (n_x, 1000) \\
$$

* Mini-batch gradient descent

$$
\text{ for } t = 1, ..., 5000 \\
    \quad \text{ Forward prop on } X^{\{t\}} \\
    Z^{[1]} = W^{[1]} X^{\{t\}} + b^{[1]} \\
    A^{[1]} = g^{[1]}(Z^{[1]}) \\
    \dots \\
    A^{[L]} = g^{[L]}(Z^{[L]}) \\
    J^{\{t\}} = \frac{1}{1000} \sum_{i=1}^{1000} L(\hat{y}^{(i)}, y^{(i)})
    + \frac{\lambda}{2\cdot1000} \sum_{l} ||W^{[l]}||_F^2 \\
    \text{ Backward prop on wrt } J^{\{t\}} (X^{\{t\}}, Y^{\{t\}}) \\
    W^{[l]} := W^{[l]} - \alpha dW^{[l]} \\
    b^{[l]} := b^{[l]} - \alpha db^{[l]}
$$

1. This is called "1 epoch" passing through training set.

### 2-2 Understanding mini-batch gradient descent

* Training with mini batch gradient descent

1. The cost goes down every iteration, even one goes up then something is wrong, e.g.
  learning rate is too big.
2. It maynot decrease in every iteration, but the trend should go down.

* Choosing your mini-batch size

1. size = m, batch gradient descent
2. size = 1, stochastic gradient descent: every example is it's own mini batch
3. stochastic gradient descent can be extremely noisy can take to wrong direction, won't ever
  converge.
4. In practice: somewhere in between.
5. batch: too long iteration, stochastic, lose speedup from vectorization
6. In between: fastest learning
    * vectorization
    * Make progress

* Choosing your mini-batch size

1. Small training set (< 2000): Just use batch
2. Typical mini-batch sizes: 64, 128, 256, 512
3. Make sure mini-batch fit in (CPU/GPU) memory
4. This is another hyper-parameter

### 2-3 Exponentially weighted averages

* Temperature in London

$$
V_0 = 0 \\
V_1 = 0.9 V_0 + 0.1 \theta_1 \\
V_2 = 0.9 V_1 + 0.1 \theta_2 \\
\cdots \\
V_t = 0.9 V_{t-1} + 0.1 \theta_t \\
V_1 = \beta V_0 + (1- \beta) \theta_1 \\
V_t \text{ as approximately average over } \approx \frac{1}{1-\beta} \text{ days temperature }
$$
1. $\beta = 0.9 \approx $ 10 days temperature
2. $\beta = 0.98 \approx $ 50 days temperature, less wigglely but adapt slowly.
3. $\beta = 0.5 \approx $ 2 days

### 2-4 Understanding exponentially weighted averages

$$
V_1 = \beta V_0 + (1- \beta) \theta_1 \\
V_{t} = (1- \beta) V_{t-1} + (1- \beta)\beta V_{t-2} + (1- \beta)\beta^2 V_{t-2} + \cdots \\
(1 - \epsilon) ^ {\frac{1}{\epsilon}} = \frac{1}{e}
$$

* Implementing exponentially weighted averages

1. requires very less memory than the regular average: `vtheta = beta * vtheta + (1-beta) * thetat`

### 2-5 Bias correction in exponentially weighted averages

1. Bias correction: change from $V_t$ to $V_t / (1 - \beta^t)$

### 2-6 Gradient descent with momentum

1. Almost always works faster than normal gradient descent
2. On each iteration t: compute dW and db on current mini-batch
3. compute $V_{dW} = \beta V_{dW} + (1- \beta)dW$, similar for $V_{db}$
4. $W = W - \alpha V_{dW}, b = b - \alpha V_{db}$
5. The steps moving more horizontally toward minimum.

* Implementation details

$$
v_{dw} = \beta v_{dw} + (1-\beta)dW \\
v_{db} = \beta v_{db} + (1-\beta)db \\
W = W - \alpha v_{dw} \\
b = b - \alpha v_{db} \\
\text{In practice, someone does the following, Andrew does not prefer:} \\
v_{dw} = \beta v_{dw} + dW
$$

1. Hyperparameters: $\alpha, \beta$, $\beta = 0.9$ works very well.
2. In practice, don't see people use bias correction, because after 10 iterations,
    The average warms up.

### 2-7 RMSprop

1. Root mean square prop
2. On iteration t: compute dW, db on current mini-batch

$$
S_{dW} = \beta S_{dW} + (1-\beta) dW^2 \text{ (element-wise) } \\
S_{db} = \beta S_{db} + (1-\beta) db^2 \text{ (element-wise) } \\
W = W - \alpha \frac{dW}{\sqrt{S_{dW}} + \epsilon} \\
b = b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon} \\
$$

3. $\epsilon$ is for numerical stability.

### 2-8 Adam optimization algorithm

1. A lot of optimization algorithm proposed not well generalized.

$$
V_{dW} = 0, S_{dW} = 0, V_{db} = 0, S_{db} = 0 \\
V_{dW} = \beta_1 V_{dW} + (1 - \beta_1) dW, V_{db} = \beta_1 V_{db} + (1-\beta_1) db \\
S_{dW} = \beta_2 S_{dW} + (1-\beta_2) dW^2 \text{ (element-wise) } \\
S_{db} = \beta_2 S_{db} + (1-\beta_2) db^2 \text{ (element-wise) } \\
V_{dW}^{corrected} = V_{dW} / (1 - \beta^t_1), V_{db}^{corrected} = V_{db} / (1 - \beta^t_1) \\
S_{dW}^{corrected} = S_{dW} / (1 - \beta^t_1), S_{db}^{corrected} = S_{db} / (1 - \beta^t_1) \\
W = W - \alpha \frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}} + \epsilon} \\
b = b - \alpha \frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}} + \epsilon} \\

$$

* Hyperparameter choice

1. $\alpha$
2. $\beta_1 = 0.9$
3. $\beta_2 = 0.999$
4. $\epsilon = 10^{-8}$

* Adam: Adaptive moment estimation

### 2-9 Learning rate decay

1. 1 epoch is 1 pass through the data

$$
\alpha = \frac{1}{1+\text{decay rate} \times \text{epoch_num}} \alpha_0 \\
\alpha = \frac{k}{\sqrt{\text{epoch_num}}} \alpha_0 \\
\alpha = 0.95^{\text{epoch_num}} \alpha_0
$$

2. manual decay: only works for training small # of models.
3. Learning rate can really help.

## 2-10 The problem of local optima

1. The intuition about local optima previously is not correct
2. Most local optima are settle points
3. A lot of intuition we learnt from low dimension doesn't apply to high dimension.

* Problem of plateaus

1. unlikely to get stuck in a bad local optimal
2. Plateaus can make learning slow

## 2-11 Programming assignment

<font color='blue'>
**What you should remember**:
- The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.
- You have to tune a learning rate hyperparameter $\alpha$.
- With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).

<font color='blue'>
**What you should remember**:
- Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.
- You have to tune a momentum hyperparameter $\beta$ and a learning rate $\alpha$.

## Week 3 Hyperparameter tuning, Batch Normalization and Programming Frameworks

### 3-1 Tuning process

* Hyperparameters

1. learning rate $\alpha$
2. momemtum $\beta = 0.9$
2. Adam $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$
2. # layers
3. # hidden units
4. learning rate decay
5. mini-batch

* In deep learning era, try random values, don't use grid.
  Because you don't know which parameter is important. Try random values can cover
  more values than grid.

* Another common practices: Coarse to fine

1. Find a region with good results.
2. Then focus more on this region.

### 3-2 Using an appropriate scale to pick hyperparameters

* Picking hyperparameters at random

1. $n^{[l]} = 50, \dots, 100$, then use random search.
2. # of layers $L$: 2 - 4, use grid search, try 2, 3, 4.

* Appropriate scale for hyperparameters

1. You suspect $\alpha = 0.0001, \dots, 1$
2. If sample uniformly, then 90% will be between 0.1 and 1, only 10% will be between
    0.0001 and 0.1
3. Then in this case, more reasonable to search in log scale.

* Hyperparameters for exponentially weighted averages

1. $\beta = 0.9, \dots, 0.999$, doesn't make sense to search uniformly.
2. So $1-\beta = 0.1 (10^{-1}), \dots, 0.001(10^{-3})$, so sample r uniformly between
  $[-3, -1]$
3. When beta changes from 0.9 to 0.9005, doesn't make too much difference, both are about
  averaging 10 last.
4. But if change from 0.999 to 0.9995, it's changing from 1000 to 2000.
5. forces us to sample more densely when $\beta$ is close to 1.

### 3-3 Hyperparameters tuning in practice: Pandas vs. Caviar

* Re-test hyperparameters occasionally

1. methods work well in one application domain can be applied to another.
2. Intuition do get scale.

* Babysitting one model (Panda approach, not enough resource)

1. May be have a lot of data, but less computing resources.
2. Can only afford to train one model, then babysit with this model.
3. check every day results and adjust the parameters accordingly. Nudging rate up and down.

* Training many models in parallel (Caviar approach, enough computing resource)

### 3-4 Normalizing activations in a network

1. It makes hyperparameter search much easier and more robust

* Normalizing inputs to speed up learning

1. Previously we normalize input $x$, can we also normalize $a^{[1]}, a^{[2]}, \dots$
  and so on, so as to train $W^{[3]}, b^{[3]}$ faster?
2. In practice, Andrew suggests to normalize $Z^{[2]}$ instead of $A^{[2]}$

* Implementing Batch Norm

1. Give some intermdiate values $z^{[l](i)} = \{z^{[1]}, \dots, z^{[m]}\}$

$$
\mu = \frac{1}{m} \sum_i z^{(i)} \\
\sigma ^2 =  \frac{1}{m} \sum_i (z_i - \mu)^2 \\
z_{\text{norm}}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma ^2 + \epsilon}} \\
\tilde{z^{(i)}} = \gamma  z_{\text{norm}}^{(i)} + \beta \text{ learnable parameters of model} \\
$$

2. if $\gamma = \sqrt{\sigma ^2 + \epsilon}$ and $\beta = \mu$,
  then $\tilde{z^{(i)}} = z^{(i)}$
3. Use $\tilde{z^{(i)}}$ instead of $z^{(i)}$

### 3-5 Fitting Batch Norm into a neural network

* Adding Batch Norm to a network

$$
X \xrightarrow{W^{[1]}, b^{[1]}} Z^{[1]}
\xrightarrow[\text{batch Norm (BN)}]{\beta^{[1]}, \gamma^{[1]}} \tilde{Z}^{[1]}
\rightarrow A^{[1]} = g^{[1]}(\tilde{Z}^{[1]})
\xrightarrow{W^{[2]}, b^{[2]}} Z^{[2]}
\xrightarrow[\text{batch Norm (BN)}]{\beta^{[2]}, \gamma^{[2]}} \tilde{Z}^{[2]}
$$

* Parameters

$$
W^{[1]}, b^{[1]}, \dots, W^{[L]}, b^{[L]} \\
\beta^{[1]}, \gamma^{[1]}, \dots \beta^{[L]}, \gamma^{[L]}
$$

1. Note that this $\beta$ is not the same as the in the momemtum and RMS drop.
2. Then you can use any optimization algorithm, like gradient descent, momentum, Adam.
3. If use a deep learning framework, usually don't have to implement yourself.

* Working with mini-batches

1. The parameter $b^{[l]}$ is no longer needed, since the mean will be substracted.
  It is replaced by $\beta^{[l]}$.
2. The size of $\beta^{[l]}, \gamma^{[l]}$ are all $n^{[l]}, 1$

* Implementing gradient descent 

$$
\text{ for } t = 1, ..., \# \\
    \quad \text{ Forward prop on } X^{\{t\}} \\
    \text{ In each hidden layer, use BN to replace } Z^{[l]} \text{ with } \tilde{Z}^{[l]} \\
    \text{ Use backward prop to compute } dW{[l]}, d\beta^{[l]}, d\gamma^{[l]}, (db^{[l]}
    \text{ is gone }) \\
    \text{updates parameters:} \\
    W := W - \alpha dW{[l]} \\
    \beta := \beta - \alpha d\beta^{[l]} \\
    \gamma := \gamma - \alpha d\gamma^{[l]}
$$

* It also works with momemtum, RMSprop, and Adam.

### 3-6 Why does Batch Norm work?

1. We have already seen normalize input features can speed up learning.

* Learning on shifting input distribution

1. Makes the deeper layer more robust to the changes in earlier layer.
2. Model trained based on one data set (black cat), might not work well
  for another data set (color cat), because the distribution is different.
3. This problem is called "Covariant shift"

* Why this is a problem with neural networks?

1. it limits the amount to which updating the parameters in the earlier layers
  can affect the distribution of values that the third layer now sees and therefore
  has to learn on.
2. Take away, from the perspective of later layers. The earlier layer doesn't shift much.

* Batch norm as regularization

1. Each mini-batch is scaled by the mean/variance of just that mini-batch
2. This adds noise to the values of $z^{[l]}$ within that mini-batch. So similar to dropout,
    it adds some noise to each hidden layer's activations.
3. This has a slight regularzation effect.
4. Using large mini-batch size, reduces noise then reduces regularization effect.

### 3-7 Batch Norm at test time

$$
\mu = \frac{1}{m} \sum_i z^{(i)} \\
\sigma ^ 2 = \frac{1}{m} \sum_i (z^{(i)} - \mu)^2 \\
z^{(i)}_{\text{norm}} = \frac{z^{(i)} - \mu}{\sqrt{\sigma ^ 2 + \epsilon}} \\
\tilde{z}^{(i)} = \gamma z^{(i)}_{\text{norm}} + \beta
$$

1. Then at test time, how do we choose $\mu, \sigma$? We do exponential average over
  $\mu^{\{1\}[l]}, \mu^{\{2\}[l]}, \mu^{\{3\}[l]}$,
  $\sigma^{\{1\}[l]}, \sigma^{\{2\}[l]}, \sigma^{\{3\}[l]}$, and take whatever is the latest
2. Although this is only an approximation, it's pretty robust

### 3-8 Softmax Regression

* Cats, dogs and baby chicks

1. C = # of classes $0, 1, \dots, C-1$
2. $n^{[L]} = 4$, $\hat{y}$ is 4 by 1 vector

* Softmax layer

$$
z^{[L]} = W^{[L]} a^{[L-1]} + b^{[L]} \\
\text{Activation function:} \\
t = e^{z^{[L]}} \\
a^{[L]} = \frac{e^{z^{[L]}}}{\sum_j^4 t_j} \\
a^{[L]} = g^{[L]}(z^{[L]})
$$

* Softmax examples

1. For a network with only one output layer, the decision boundary is all linear.

### 3-9 Training a softmax classifier

* Understanding softmax

$$
z^{[L]} = 
\begin{bmatrix}
5 \\
2 \\
-1 \\
3 \\
\end{bmatrix}
t =
\begin{bmatrix}
e^5 \\
e^2 \\
e^{-1} \\
e^{3} \\
\end{bmatrix}
a^{[L]} = g^{[L]}(z^{[L]}) =
\begin{bmatrix}
0.842 \\
0.042 \\
0.002 \\
0.114 \\
\end{bmatrix}
$$

1. Softmax regression generalizes logistic regression to C classes
2. If C = 2, softmax regression reduces to logistic regression.

* Loss function

$$
y =
\begin{bmatrix}
0 \\
1 \\
0 \\
0 \\
\end{bmatrix}

a^{[L]} = \hat{y} =
\begin{bmatrix}
0.3 \\
0.2 \\
0.1 \\
0.4 \\
\end{bmatrix}
\\
L(\hat{y}, y) = - \sum y_j \log \hat{y_j} \\
J = \frac{1}{m} \sum L(\hat{y}^{(i)}, y^{(i)}) \\
Y = [y^{(1)}, y^{(2)}, \dots, y^{(m)}] \text{ 4 by m matrix } \\
\hat{Y} = [\hat{y}^{(1)}, \hat{y}^{(2)}, \dots, \hat{y}^{(m)}] \text{ 4 by m matrix } \\
$$

* Gradient descent with softmax: back prop

$$
dz^{[L]} = \hat{y} - y
$$

### 3-10 Deep learning frameworks

* Caffe/Caffe2
* CNTK
* DL4J
* Keras
* Lasagne
* mxnet
* PaddlePaddle
* TensorFlow
* Theano
* Torch

* Criterion to choose
    * Ease of programming
    * Running speed
    * Truely open (open source with good governance)

### 3-11 TensorFlow

* Motivating problem

$$
f(w) = w^2 - 10w + 25
$$

### 3-12 Programming exercises

Great! To summarize, remember to initialize your variables, create a session and run the operations inside the session.

<font color='blue'>
**To summarize, you how know how to**:
1. Create placeholders
2. Specify the computation graph corresponding to operations you want to compute
3. Create the session
4. Run the session, using a feed dictionary if necessary to specify placeholder variables' values.

# Course 3 

## Week 1

### 1-1 Why ML Strategy?

* Motivating example

1. After a while, got 90% accuracy, but isn't good enough.
2. Ideas:
    * Collect more data
    * Collect more diverse training set
    * Train algorithm longer with gradient descent
    * Try Adam instead of gradient descent
    * Try bigger network
    * Try smaller network
    * Try dropout
    * Add L2 regularization
    * Network architecture
        * Activation functions
        * # hidden units
3. But could be wasting time.

### 1-2 Orthogonalization

1. Most effective machine learning people is very clear-eyed about what to tune
  in order to try to achieve one effect.
  
* TV tuning example

1. Orthogonalization refers TV designer designed each knob to change one one function.

* Car example

1. Stearing/Acceleration/Brake, interpretable

* Chain of assumptions in ML

1. Fit training set well on cost function, for some applications, this means comparable
  to human performance.
    * TV knob here: bigger network/better optimization algorithm (Adam)
2. Hope: Fit dev set well on cost function
    * Knob: regularization/bigger training set
3. Hope: Fit test set well on cost function
    * Knob: bigger dev set
4. performs well in real world
    * Knob: change dev set (distribution not set correctly) or cost function
      (isn't measuring the right thing)
5. Andrew personally tends not to use early stopping.
    * It affects the performance both on the training set and testing set, so
      less orthogonalized.
    * Doesn't mean it's a bad knob to use.
    * But having more orthogonalized one makes tuning much more easier.

### 1-3 Single number evaluation metric

1. building process will be much faster if have a single real number
  evaluation metric
2. Precision: recognized as cat, % is actually cat
3. Recall: for images that are really cat, what % correctly recognized.
4. Problem with two metrics is:
  if A does better on precision, B does better on recall, not sure
  which one is better.
5. Use F1 score: "Average" of P and R. Harmonic mean

$$
\frac{2}{\frac{1}{P} + \frac{1}{R}}
$$

6. A good Dev set + Single real number evaluation metric. Speed up iterating.

* Another example

| Algorithm | US  | China | India | Other |
| :-------: | :-: | :---: | :---: | :---: |
| A | 3% | 7% | 5% | 9% |
| B | 5% | 6% | 5% | 10% |

1. Then it's hard to keep tracking of 4 metrics, so it's better to do an average.
2. To have a single number can really improve efficiency.

### 1-4 Satisficing and Optimizing metric

* Another cat classification example

| Classifier | Accuracy  | Runtime |
| :-------: | :-: | :---: |
| A | 90% | 80ms |
| B | 92% | 95ms |
| C | 95% | 1500ms |

1. A little bit artificial way: cost = accuracy - 0.5 * running time
2. maximizing accuracy subject to running time <= 100ms
3. N metrics: 1 optimizing, N-1 satisficing

* wakewords / Trigger words

1. care both accuracy and false positive.
2. maximizing accuracy s.t. <= 1 false positive every 24 hours.

### 1-5 Train/dev/test distributions

1. Even large company team set up data sets slow down the distribution.

* Cat classification dev/test sets

1. This is a bad idea: dev set from (US/UK/Other EU/South America)
    test set from (India/China/Other Asia/Australia)
2. Team spends month on one bulls eye, but test set is another bulls eye.
3. Andrew recommends: take data from all regions and randomly shuffle into dev/test

* True story

1. Optimization on dev set on loan approvals for medium income zip code.
2. But test on low income zip code.

* Guide line: Choose a dev set and test set to reflect data you expect to get in the
  future and consider important to do well on
* The dev set and test set should be from same distribution.

### 1-6 Size of the dev and test sets

* Old way of splitting data: 70% train, 30% test. 60 train, 20 dev, 20 test.
* Reasonable for 0.1k/1k/10k/10k data.
* Modern era: 1M examples, 98% train/1% dev/1% test.
* Size of test set: set your test set to be big enough to give high confidence
  in the overall performance of your system
* No test set is probably OK, just train and dev set. But Andrew does not recommend.
* The trend is to use more data for training.
* Rule of thumb: set the dev set big enough to evaluate different ideas and pick one.
  test set is just to get final score.

### 1-7 When to change dev/test sets and metrics

* In the course of project, one can realize the target is wrong.
* Cat dataset examples

1. Metric: classification error
2. Alg A: 3% error, but let through porn images, which is totally unacceptalbe
3. Alg B: 5% error, doesn't have porn images
4. So from company and user acceptance point of view, B is better.
5. Metric prefers A, but users/company prefer B.

$$
\frac{1}{m_{dev}} = \sum_{i=1}^{m_{dev}}
I\{y^{(i)}_{pred} \ne y^{(i)} \}
$$

6. The above metric just treats porn and non-porn images equally.
7. May need to change to the following:

$$
\frac{1}{\sum w^{(i)}}
\frac{1}{m_{dev}} \sum_{i=1}^{m_{dev}}
w^{(i)} \cdot I\{y^{(i)}_{pred} \ne y^{(i)} \} \\
w^{(i)} =
\begin{cases}
1 & \text{if non-porn} \\
100 & \text{if porn}
\end{cases}
$$

* Orthogonalization for cat pictures: anti-porn

1. We have only discussed how to define a metric to evaluate classifiers, analogy:
    how to place the target.
2. Worry separately about how to do well on this metric, i.e. how to aim accurately,
    how to shoot at the target.

* Another example

1. Alg A: 3% error, 
2. Alg B: 5% error, but does better when deploying
    * Because during training, images are clearer, nice, HD images
    * During testing, cats are having funny face, more blurer
3. Guide line: if doing well on the metric + dev/test set doesn't corresponding to
  doing well on your application, change your metric and/or dev/test set
  
* Andrew's recommendation: even can't define a perfect evaluation metric and dev set,
  just set something quickly to drive your team iterating.
* Change it later when you have better idea.
* Not recommended: run for too long without evaluating metric

### 1-8 Why human-level performance

1. Suddenly, machine learning performance is improved.
2. The workflow is more efficient with something people can also do.

* Comparing to human-level performance

1. After suppassing human level performance, accuracy is slowing down.
2. performance never suppasses some theoretical limit. (Bayes optimal error)
    Best possible error.
3. 2 reasons for slowing down:
    1. Human error is not that far away from Bayes error
    2. Some tools can be used when performance worse than human, but not when better
        than human:
        1. Get labeled data from human
        2. Gain insight from manual error analysis: why did a person get this right.
        3. Better analysis of bias/variance.
4. Knowing how well humans can do on a task can help you understand better how much
  you should try to reduce bias and how much you should try to reduce variance.

### 1-9 Avoidable bias

* Cat classification

1. Humans: 1%
1. Training error: 8%
2. Dev error: 10%
3. In this case: Focus on bias, bigger network, run descent longer

* Another case

1. Humans: 7.5%
1. Training error: 8%
2. Dev error: 10%
3. In this case: Focus on variance, regularization, getting more training data.

* Human level error as a proxy for Bayes error, (reasonable assumption for computer
  vision)
* gap between human (bayes) error and training error is called avoidable bias.

### 1-10 Understanding human-level performance

* Human-level error as a proxy for Bayes error

1. Medical image classification example
2. Suppose typical human: 3% error
3. Suppose typical doctor: 1% error
4. Experienced doctor: 0.7% error
5. Team of experienced doctor: 0.5% error
6. How should you define human level error then?
7. Based on above, we know Bayes error <= 0.5%
8. In practice, if a system suppass typical doctor, then it's worth to deploy.

* Error analysis example

1. Training error: 5%
2. Dev error: 6%
3. Human error might be: 1%/0.7%/0.5%
4. So in this example, it doesn't matter which human error you use.
5. Another example: train 1%, Dev 5%, then it doesn't matter still.
6. 3rd example: train 0.7%, Dev 0.8%. This time matters.
7. This problem arose only when you are doing very good in your problem.

* Summarize of bias and variance with human-level performance.

1. Human level error:
2. Training error:
3. Dev error:
4. Between 1 & 2, it's avoidable bias. between 2 & 3, it's variance
5. Instead of comparing against 0 to get bias, we now compare against human error.
6. Speech recognition with noisy audio, the bayes error can't be 0.

### 1-11 Surpassing human level performance

1. Team of humans: 0.5, 0.5
2. one human: 1, 1
3. Training error: 0.6, 0.3
4. Dev error: 0.8, 0.4
5. Then in the 2nd case, don't have enough info to tell to reduce bias or variance.
6. If already better than human, then harder to use human intuition to improve algorithm.

* Problems where ML significantly surpasses human-level performance

1. Online advertising
2. Product recommendation
3. Logistics
4. Loan approval
5. All the above examples learn from structure data, not nature perception problem, which
  human is very good at.
6. Teams look at lots of data

### 1-12 Improving your model performance

* The 2 fundamental assumption of supervised learning

1. You can fit the training set pretty well: achieve low avoidable bias
2. The training set performance generalizes pretty well to the dev/test set: variance is
  not too bad
  
* Reducing (avoidable) bias and variance

1. Human-level
2. Training error
3. Dev error
4. Avoidable bias: 
    * train bigger model
    * train longer better optimization algorithms: momentum/RMSprop/Adam
    * NN architecture/hyperparameters search (e.g. changing activation function, RNN, CNN)
5. Variance:
    * More data
    * Regularization: e.g., L2, dropout, data augmentation
    * NN architecture/hyperparameters search

### 1-13 Andrej Karpathy interview

## Week 2 ML strategy (2)

### 2-1 Carrying out error analysis

* Look at dev examples to evaluate ideas

1. Your algorithm: 10% error on dev set.
2. Did poor on dog images.
3. Should you try to make your cat classifier do better on dogs?
4. It may take several months, is it worth to do it?
5. Error analysis:
    1. Get ~100 mislabeled dev set examples.
    2. Count up how many are dogs.
6. Suppose 5% is wrong, then even solves them completely, doesn't help much.
  error rate from 10% to 9.5%.
7. Give you a "ceiling".
8. Suppose 50% are dog images, error might go down from 10% to 5%.
9. Can potentially save a lot of time.

* Evaluate multiple ideas in parallel

1. Fix dog pictures
2. Fix great cats (lions, panthers)
3. Improve performance on blurry images

| Image | Dog | Great Cats | comments |
| :---: | :-: | :--------: | :------- |
| 1 | | | |
| 2 | | | |
| 3 | | | |
| ... | | | |
| % of total | 8% | 60% | |

4. Then we should focus on great cats.

### 2-2 Cleaning up incorrectly labeled data

* Incorrectly labeled examples

1. DL algorithms are quite robust to random errors in the training set.
2. If the errors are reasonablely random, then probably OK. 
3. Caveat: DL algorithm are less robust to systematic errors, e.g. consistently label
  dog as cat.
  
* Incorrectly labeled data in dev/test set.

1. Use error analysis, and add one column "incorrectly labeled".
2. Whether it makes significant difference in your test set.
3. Overall dev set error: 10% .
4. Errors due incorrect labels:  6% * 10% = 0.6%
5. errors due to other causes: 9.4%
6. Then in this case, probably not worth.

* Another case:

1. Overall dev set error: 2% .
4. Errors due incorrect labels:  0.6%
5. errors due to other causes: 1.4%
6. Then in this case, probably worth.

* Goal of dev set is to help you select between classifier A and B.

1. 2.1% error
2. 1.9% error
3. You don't trust the dev set, then need to fix up dev set.

* Correcting incorrect dev/test set examples

1. Apply same process to your dev and test sets to make sure they continue to
  come from the same distribution.
2. Consider examing examples your algorithm got right as well as ones it got wrong.
  This isn't always done, but something to consider.
3. Train and dev/test data may now come from slightly different distributions, but
  it's OK. It's super important to have dev/test set from the same distribution.

### 2-3 Build your first system quickly, then iterate

* Speech recognition example

* Noise background
    * Cafe noise
    * Car noise
* Accented speech
* Far from microphone
* Young children's speech
* Stuttering

* How do you pick which direction to focus on

* Set up dev/test set and metric
    * If the metric/target is wrong, still OK, can correct later.
* Build initial system quickly
    * quick and dirty
* Use Bias/Variance analysis & Error analysis to prioritize next steps.

* Guide line: build your first system quickly, then iterate

* Andrew saw a lot of teams overthink the problem.

### 2-4 Training and testing on different distributions

* DL is hunger for training data, so some team just get whatever training data
  they can get.
* More and more team use different train, test/dev distribution.

* Cat app example

1. Data from mobile app: blurry, amatuer, 200,000 images
2. Data from web: HD, professional, 10,000 images
3. Don't have a lot users for the mobile app.
4. Option 1: put them together and randomly shuffle them to train/dev/test set.
  205000 for train, 2500 for dev and 2500 for test.
    1. Advantage: all train/dev/test set comes from the same distribution
    2. Huge distribution: A lot of images come from web image rather than you care, i.e.
        mobile images. Spending most of time optimizing web images. Not aiming at the
        target. Not recommended.
5. Option 2: training: all 200000 from web + 5000 from mobile app. Dev and test set
  are all from mobile app.
    1. Advantage: you are aiming the target.
    
* Speech recognition example: speech activated rearview mirror.

1. Training: all the speech data you have, e.g. purchased data, smart speaker control,
    Voice keyboard, 500,000 utterance.
2. Dev/test: actually drawn from speech activated rearview mirror, 20,000 utterance.
3. Or 500,000 + 10,000 training, 5k dev, 5k test

### 2-5 Bias and Variance with mismatched data distributions

* Cat classifier example

1. Assume human get almost 0% error.
2. Train error 1%, dev error 10%, but train and dev comes from different distribution,
  can not say the same conclusion.
    1. Maybe the model is doing just fine in the dev set.
    2. Model sees the data in training but not dev set. The distributions are different.
3. Define a new training-dev set: same distribution as training set,
  but not used for training.
4. Shuffle the training set, leave one portion for train-dev set, which will not be used
  during training.
    1. train error 1%, but train-dev set error is 9%, dev error is 10%.
      Then we have a variance problem, because train/train-dev distributions are the same.
    2. train error 1%, but train-dev set error is 1.5%, dev error is 10%.
      Then we have a data mismatch problem.
    3. train error 10%, but train-dev set error is 11%, dev error is 12%.
      Then we have an avoidable bias problem, because human errors are 0%.
    4. train error 10%, but train-dev set error is 11%, dev error is 20%.
      Then we have both bias and data mismatch problem.

* Bias/variance on mismatched training and dev/test sets

1. Key quantities: Human level/training/training-dev/dev error: are 4%, 7%, 10%, 12%.
  The difference between them are: avoidable bias/variance/data mismatch.
    1. Also the difference between dev and test error are degree of overfitting to dev set.
      If this one is bigger then, then might need to consider getting more dev data.
2. Human level/training/training-dev/dev/test error: are 4%, 7%, 10%, 6%, 6%. Training
  set is harder, dev/test set is much easier.
  
* More general formulation:

1. General speech recognition, human/examples trained on/examples not trained on: 4/7/10
2. Rearview mirror data: human/examples trained on/examples not trained on: 6/6/6
    1. 4 <-> 7: avoidable bias
    2. 7 - 10: data mismatch
    3. 10 - 6: data mismatch
    4. Doing pretty good, because dev error is close to human error.

### 2-6 Addressing data mismatch

* There are not systematic way to address this

1. Carryout manual error analysis to try to understand difference between
    training and dev/test sets.
    1. dev set might be noisy - car noise, not getting street numbers right.
2. Make training data more similar; or collect more data similar to dev/test sets.
    1. Simulate noise in car data.
    2. hard to recognize street data, get people speaking numbers, add to training set.

* Artificial data synthesis

1. clean audio (classical example: "The quick brown fox jumps over the lazy dog", 
  contains all alphabat) + Car noise = Synthesized in-car audio
2. 10k hours audio, only has 1 hr of car noise. If duplicate noise 10k times.
3. Car recognition: overfitting to the 20 cars from gaming.

### 2-7 Transfer learning

* Apply the knowledge learned from one task to another. Cats recognition to X-ray image.
* Transfer learning

1. Take the last layer and weights
2. Initialize random weights.
3. Pre-training (image recognition) and fine tuning (radiology diagnosis).
4. Low level features can be helpful.

* Speech recognition

1. Audio in -> Transcripts out (speech recognition, wakeword/trigger detection)
2. When does transfer learning make sense?
    1. A lot of data transfering from, less data for the problem transfering to
      e.g. 1M image recognition, 100 radiology diagonosis or 10000 hrs speech recognition
      + 1h wakeword.
3. When does not make sense? The opposite is true.

* When transfer learning makes sense

1. Task A and B have the same input x
2. You have a lot more data for task A than task B.
3. low level features from A could be helpful for learning B.

### 2-8 Multi-task learning

* Simplified autonomous driving example

1. Detect pedestrians/cars/stop signs/Traffic lights, $y^{(i)} = [0, 1, 1, 0]$
2. Loss function:

$$
\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^4 L(\hat{y}_j^{(i)}, y_j^{(i)})
$$

3. Unlike softmax regression: One image can have multiple label.
4. Multi-task learning: low level features can be shared.
5. Even some labels are missing, then just need to skip those "?" in the above loss  
    function.

* When multi-task learning make sense?

1. Training on a set of tasks that could benefit from shared low-level features.
2. Usually: Amount of data you have for each task is quite similar
    1. A1, ..., A100 each task has 1000 data, but the other 99,000 can help A100 .
3. Can train a big enough NN to do well on all the tasks.

### 2-9 What is end-to-end deep learning

* Speech recognition example

1. X: audio -> (MFCC) -> features -> (ML) -> Phonemes -> words -> transcript (y)
2. End-to-end, audio -------------------> transcript

* Face recognition

1. One approach: Image (x) -> Identity (y)
2. Better approach:
    1. Detect person's face, zoom in the face part.
    2. Run algorithm on that part.
    3. Compare 2 images.
3. Split the problem to 2 simpler problems and get better performance.
4. Why is this 2 step approach works better?
    1. Each problem is simpler
    2. Have a lot of data for each of the 2 subtasks.
5. In contrast, if tries to learn everything at the same time, having much less data.

* Machine translation

1. English -> French

* Estimating child's age

1. multi steps: image -> bones -> age works well
2. end to end: image -> age, not enough data, so not working well.

### 2-10 Whether to use end-to-end deep learning

* Pros and Cons

1. Pros:
    1. Let the data speak: more able to capture statistics in the data than forcing
      human preconceptions.
    2. Less hand-designing of components needed.
2. Cons:
    1. May need large amount of data
    2. Excludes potentially useful hand-designed components, which can be very helpful.
    
* Applying end-to-end deep learning

1. Do you have sufficient data to learn a function of the complexity needed to map
  x to y?
    1. Image to find bones, find faces, not that hard.
    2. Image to child's age, much difficult and needs more data.

* Auto driving

1. image -> (DL) -> car/pedestrians -> (motion planning) -> route -> (control) -> steering
2. Use DL to learn individual components
3. Carefully choose X-> y depending what tasks you can get data for.
4. End to End, image -> steering approach is less promising.

# Course 4 Convolutional Neural Networks

## Week 1 Foundations of Convolutional Neural Networks

### 1-1 Computer Vision

* Self-driving cars / Face recognition
* Exciting parts:
    * Brand new applications.
    * Other fields can benefit from computer vision research community.

* Computer vision Problems

1. Image classification
2. Object detection: draw boxes around an object, e.g. cars.
3. Neural Style Transfer: normal picture to Picasso style

* Deep Learning on large images

1. Previous course image size: 64x64, 12288 vector
2. Large image: 1000x1000, 3M vector, even the 1st layer has only 1000 hidden units,
  the size of the matrix $W^{[1]}$ is $[1000, 3000000]$
3. Then cannot get enough data to prevent overfitting. Also the training time is very long.

### 1-2 Edge Detection Example

* Computer Vision Problem

1. Detect vertical edges
2. Detect horizontal edges

* Vertical edge detection

1. Given 6x6 matrix $M$, which is one RGB channel
2. Use 3x3 filter

$$
\begin{bmatrix}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1 \\
\end{bmatrix}
$$

3. Convolve $M$ with filter matrix, output will be 4x4 matrix by doing element-wise
  product and then addition.
4. In implementation:
    1. programming ex: conv_forward
    2. tensor-flow: tf.nn.conv2d
    3. keras: conv2D

* Why this method work?

1. one simple example:

$$
\begin{bmatrix}
10 & 10 & 10 & 0 & 0 & 0 \\
10 & 10 & 10 & 0 & 0 & 0 \\
10 & 10 & 10 & 0 & 0 & 0 \\
10 & 10 & 10 & 0 & 0 & 0 \\
10 & 10 & 10 & 0 & 0 & 0 \\
10 & 10 & 10 & 0 & 0 & 0 \\
\end{bmatrix}
*
\begin{bmatrix}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1 \\
\end{bmatrix}
= 
\begin{bmatrix}
0 & 30 & 30 & 0 \\
0 & 30 & 30 & 0 \\
0 & 30 & 30 & 0 \\
0 & 30 & 30 & 0 \\
\end{bmatrix}
$$

2. Then we get bright region in the middle.

### 1-3 More edge detection

1. What if color is flipped from previous example?

$$
\begin{bmatrix}
0 & 0 & 0 & 10 & 10 & 10  \\
0 & 0 & 0 & 10 & 10 & 10  \\
0 & 0 & 0 & 10 & 10 & 10  \\
0 & 0 & 0 & 10 & 10 & 10  \\
0 & 0 & 0 & 10 & 10 & 10  \\
0 & 0 & 0 & 10 & 10 & 10  \\
\end{bmatrix}
*
\begin{bmatrix}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1 \\
\end{bmatrix}
= 
\begin{bmatrix}
0 & -30 & -30 & 0 \\
0 & -30 & -30 & 0 \\
0 & -30 & -30 & 0 \\
0 & -30 & -30 & 0 \\
\end{bmatrix}
$$

2. The negative number means the dark to light transition.

* Vertical and Horizontal Edge Detection

$$
V =
\begin{bmatrix}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1 \\
\end{bmatrix}
,
H = 
\begin{bmatrix}
1 & 1 & 1 \\
0 & 0 & 0 \\
-1 & -1 & -1 \\
\end{bmatrix}
$$

2. One little bit complex image:

$$
\begin{bmatrix}
10 & 10 & 10 & 0 & 0 & 0 \\
10 & 10 & 10 & 0 & 0 & 0 \\
10 & 10 & 10 & 0 & 0 & 0 \\
0 & 0 & 0 & 10 & 10 & 10  \\
0 & 0 & 0 & 10 & 10 & 10  \\
0 & 0 & 0 & 10 & 10 & 10  \\
\end{bmatrix}
*
\begin{bmatrix}
1 & 1 & 1 \\
0 & 0 & 0 \\
-1 & -1 & -1 \\
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 0 & 0 \\
30 & 10 & -10 & -30 \\
30 & 10 & -10 & -30 \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

* Learning to detect edges

1. Ohter options for filter matrix
2. Sobel filter

$$
\begin{bmatrix}
1 & 0 & -1 \\
2 & 0 & -2 \\
1 & 0 & -1 \\
\end{bmatrix}
$$

3. Sharr filter

$$
\begin{bmatrix}
3 & 0 & -3 \\
10 & 0 & -10 \\
3 & 0 & -3 \\
\end{bmatrix}
$$

4. Maybe you can just learn them. Let all the weights be parameters.
  This is one of the most powerful ideas in computer vision.

$$
\begin{bmatrix}
w_1 & w_2 & w_3 \\
w_4 & w_5 & w_6 \\
w_7 & w_8 & w_9 \\
\end{bmatrix}
$$

### 1-4 padding

1. nxn matrix + fxf filter = (n-f+1)x(n-f+1) matrix
2. Downside 1: image shrinks every time
    1. You don't want this because for very deep NN, images shrinks every layer
      So you end up with very small image.
3. Downside 2: pixels in the corners are used only once, whereas pixels in the center
  are used multiple times.
    1. Throwing away a lot of edges in the image.
4. padding, convert 6x6 to 8x8, then after shrinkage, still is 6x6.
  The general equation is: (n+2p-f+1) x (n+2p-f+1)

* Valid and Same convolutions

1. Valid: no padding, nxn * fxf = (n-f+1)x(n-f+1)
2. Same: output size is the same as input size. p = (f-1)/2.
  f is usually odd. Andrew recommended 3/5/7

### 1-5 Strided Convolutions

1. If do stride = 2, then move the filter 2 pixels at a time.
2. 7x7 matrix, 3x3 filter, stride 2, get 3x3 matrix.

$$
n \times n \\
f \times f \\
p \\
s \\
(\lfloor \frac{n + 2p - f}{s} \rfloor + 1) \times
(\lfloor \frac{n + 2p - f}{s} \rfloor + 1) 
$$

* Techinical note on cross-correlation v.s. convolution

1. Convolution in math text book: flip filter horizontal and vertically first.
2. What we are doing is called cross-correlation, but in DL, we still call it convolution.
3. In signal processing, with mirror operation, the convolution is associative,
  but doesn't matter for DL.

### 1-6 Convolutions Over Volume

1. Previous 2-D images, this section 3-D volumns .

* Convolutions on RGB images

1. Images: 6x6x3, filter 3x3x3
2. No. of images must match no. of channels, 3rd dimension has to be the same.
3. Output is 4x4 image .
4. Element-wise multiplication of the 27 numbers and then add them together.

* Multiple filters

1. Use 2 filters at the same time, then can output 2 4x4 images.
2. Summary

$$
n \times n \times n_c \\
f \times f \times n_c \\
(n-f+1) \times (n-f+1) \times n_c' \ (\text{no of filters})
$$

3. No of channels is also called the depth in the literatrure, but Andrew found it's
  more confusing.
  
### 1-7 One layer of a Convolutional Network

* Example of a layer

1. Convolution + adding bias + apply relu operation, then from 6x6x3 $a^{[0]}$ to
  4x4x2 $a^{[1]}$.
  
* Number of parameters in one layer

1. 10 filters that are 3x3x3 in one layer, how many parameters?
2. 27 * 10 = 270 $w$ and 10 $b$.
3. So even apply to very large images, the no of parameters are still 280.

* Summary of notation

1. $f^{[l]}$ is the filter size
2. $p^{[l]}$ is the padding
3. $s^{[l]}$ is the stride
3. $n_c^{[l]}$ is the number of filters
3. Each filter is: $f^{[l]} \times f^{[l]} \times n^{[l-1]}_c$
3. Input: $n^{[l-1]}_H \times n^{[l-1]}_W \times n^{[l-1]}_c$
4. output: $n^{[l]}_H \times n^{[l]}_W \times n^{[l]}_c$
5. activations: $a^{[l]} \rightarrow n^{[l]}_H \times n^{[l]}_W \times n^{[l]}_c$
6. Vectorization: $A^{[l]} \rightarrow m \times n^{[l]}_H \times n^{[l]}_W \times n^{[l]}_c$
7. weights: $f^{[l]} \times f^{[l]} \times n^{[l-1]}_c \times n^{[l]}_c $
8. bias: $n^{[l]}_c - (1,1,1,n^{[l]}_c)$
9. Ordering may be different in different implementations and there is no standard.

$$
n^{[l]}_H = \lfloor
\frac{n^{[l-1]}_H + 2p^{[l]} - f^{[l]} }{ s^{[l]} } + 1
\rfloor \\
n^{[l]}_W = \lfloor
\frac{n^{[l-1]}_W + 2p^{[l]} - f^{[l]} }{ s^{[l]} } + 1
\rfloor \\
$$

### 1-8 Simple Convolutional Network Example

* Example ConvNet

1. Take away: starts with large images and gradually trend down, whereas the no of channels
  will gradually increase.
  
* Types of layer in a convnet

1. Convolution layer (CONV)
2. Pooling (Pool)
3. Fully connected (FC)

### 1-9 Pooling layers

* Pooling layer: Max pooling

1. 4x4 matrix, divided to 4 regions, pick the max in each region.
2. Parameters filter and stride: $f=2, s=2$

$$
\begin{bmatrix}
1 & 3 & 2 & 1 \\
2 & 9 & 1 & 1 \\
1 & 3 & 2 & 3 \\
5 & 6 & 1 & 2 \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
9 & 2 \\
6 & 3 \\
\end{bmatrix}
$$

3. A large number means a particular feature has been detected. Then the upper
  left quadrant has this particular feature, maybe a vertical line / whisker.
  If these features detected anywhere in this filter, then keep a high number.
  If these features doesn't exist, then the number is quite small.
4. It's working well. But don't know anyone who fully understand the reason.

* Pooling layer: Max pooling

1. input $5 \times 5 \times n_c$ and filter $3 \times 3 \times n_c$, output is
  $3 \times 3 \times n_c$

* Pooling layer: Average pooling

1. Not used often.
2. Very deep NN, use average pooling to collapse your representation.
  $7 \times 7 \times 1000$ to $1 \times 1 \times 1000$.
  
* Summary of pooling

1. Hyperparameters:
2. f filter size, f=2 and s=2 quite often. Or f = 3 and s = 2.
3. s stride
4. Max or average pooling
5. p: padding, max pooling usually doesn't use padding.

$$
\lfloor
\frac{n^{[l-1]}_H - f^{[l]} }{ s^{[l]} } + 1
\rfloor
\times
\lfloor
\frac{n^{[l-1]}_W - f^{[l]} }{ s^{[l]} } + 1
\rfloor \\
$$

### 1-10 CNN Example

* Neural network example (LeNet-5)

1. Input image $32 \times 32 \times 3$, apply 6 filters with $f=5,s=1$
2. The 2nd layer is conv 1 $28 \times 28 \times 6$, apply max pool $f=2,s=2$
3. The 3rd layer is pool 1 $14 \times 14 \times 6$, apply 16 filters with $f=5,s=1$
4. The 4th layer is conv 2 $10 \times 10 \times 16$, apply max pool $f=2,s=2$
5. The 5th layer is pool 2 $5 \times 5 \times 16$
6. Fully connected 3 layer, Then flaten it to $400 \times 1$,
  and $W^{[3]} (120 \times 400)$ and $b^{[3]} (120 \times 1)$
7. Fully connected 4 layer, $W^{[4]} (84 \times 120)$ and $b^{[4]} (84 \times 1)$
8. Final layer is softmax (10 outputs)
9. One common guide line, not to invent your own settings, refer to existing literature.
10. Usually the height and width decreases: $n_H, n_W \downarrow$ whereas the number of
  channels increase $n_c \uparrow$
11. (1 or more) conv layer - Pooling layer - (1 or more) conv layer - Pooling layer -
  (a couple of more) FC layer - softmax layer

### 1-11 Why Convolutions

1. Parameters sharing and sparsity connections
2. Example: 32x32x3, apply 6 filters with f = 5, we get 28x28x6 image.
  The initial image is 3072, and the 1st layer image is 4704
3. If fully connected, then 14M parameters.
4. But with convolution, it's just (5x5+1)x6 = 156 parameters.

* Parameter sharing:

1. A feature detector (such as vertical edge detector) that's useful in one part
  of the image is probably useful in another part of the image.
2. Remains true for low level features as well as high level features.
3. Sparsity of connections: In each layer, each output value depends only on a small
  number of inputs.
4. Good at capturing translation invariance: a picture of a cat shifted a couple of
  pixels to the right.
    1. The convolutional structure helps NN to encode the fact that an image shifted
      a few pixels should result in similar features.

## Week 2 Deep convolutional models: case studies

### 2-1 Why look at case studies?

* A convnet works well on somebody else's task might work well on your task as well.

* outline:

1. Classic networks:
    1. LeNet-5
    2. AlexNet
    3. VGG
2. ResNet (Residual Net) 152 layer.
3. Inception network.

### 2-2 Classic Networks

* LeNet 5: Gradient-based learning applied to document recognition.

$$
37 \times 37 \times 1
\xrightarrow{\substack{5 \times 5 \\s=1}}
28 \times 28 \times 6
\xrightarrow[\text{avg pool}]{\substack{f=2 \\ s=2}}
14 \times 14 \times 6
\xrightarrow{\substack{5 \times 5 \\s=1}}
10 \times 10 \times 16
\xrightarrow[\text{avg pool}]{\substack{f=2 \\ s=2}}
5 \times 5 \times 16 \\
\xrightarrow{\text{ FC }} 120
\xrightarrow{\text{ FC }} 84
\xrightarrow{} \hat{y}
$$

1. Small by modern standard, only 60K parameters.
2. Today usually see 10M to 100M .
3. As network goes deeper, $n_H, n_W \downarrow n_c \uparrow$
4. Advanced, people use sigmod/tanh, not ReLu back that time.
5. Also since computer was slow at that time, there were complicated way to compute
  filters. Not needed for today.
6. If read this paper, focus on section II and take a quick look at section III.

* AlexNet: ImageNet classification with deep convolutional neural networks

$$
227 \times 227 \times 3
\xrightarrow{\substack{11 \times 11 \\s=4}}
55 \times 55 \times 96
\xrightarrow[\text{Max pool}]{\substack{f=3 \\ s=2}}
27 \times 27 \times 96
\xrightarrow{\substack{5 \times 5 \\ \text{same}}}
27 \times 27 \times 256
\xrightarrow[\text{max pool}]{\substack{3 \times 3 \\ s=2}}
13 \times 13 \times 256 \\
\xrightarrow{\substack{3 \times 3 \\ \text{same}}}
13 \times 13 \times 384 \\
\xrightarrow{3 \times 3 }
13 \times 13 \times 384 \\
\xrightarrow{3 \times 3 }
13 \times 13 \times 256 \\
\xrightarrow[\text{max pool}]{\substack{3 \times 3 \\ s=2}}
6 \times 6 \times 256 \xrightarrow[\text{unroll}]{} 9216\\ 
\xrightarrow{\text{ FC }} 4096
\xrightarrow{\text{ FC }} 4096
\xrightarrow{\text{ softmax 1000 }} \hat{y}
$$

1. Similar to LeNet, but much bigger. Has 60M parameters.
2. Relu activation function.
3. If read the paper, having complicated way on training 2 GPUs.
4. If read the paper, local response normalization layer, didn't use much.
5. It was really this paper to convince computer vision community to seriously look into
  deep learning.
6. one of the Easier ones to read.

* VGG - 16: Very deep convolutional networks for large-scale image recognition

1. CONV = 3x3 filters, s = 1, same
2. Max-pool = 2x2, s = 2
3. 16 refers 16 layers have weights, pretty large: 138M parameters even by modern
  standard.
3. Doubling the filters on every step when you have new set of conv layers.
4. $n_H, n_W \downarrow n_c \uparrow$ very systematically, so it's very attractive.

$$
224 \times 224 \times 3
\xrightarrow{\substack{ \text{CONV64} \\ \times 2}}
224 \times 224 \times 64
$$

* Reading order: Alex -> VGG -> LeNet

### 2-3 Residual Network

* Residual block: Deep residual networks for image recognition

1. Main path

$$
z^{[l+1]} = W^{[l+1]} a^{[l]} + b^{[l+1]} \\
a^{[l+1]} = Relu(z^{[l+1]}) \\
z^{[l+2]} = W^{[l+2]} a^{[l+1]} + b^{[l+2]} \\
a^{[l+2]} = Relu(z^{[l+2]}) \\
$$

2. "short cut"
    1. 问题：what if the dimensions are not the same.

$$
a^{[l+2]} = Relu(z^{[l+2]} + a^{[l]})
$$

* Residual network

1. Although in theory, the errors should go down with more layers.
    However, the authors found the training errors can go up.
2. But with redidual network, the errors are indeed going down with more layers.

### 2-4 Why ResNets Work

* Why do residual networks work?

1. X ----> Big NN ----> $a^{[l]}$

$$
X \rightarrow \text{Big NN}\rightarrow a^{[l]}
\rightarrow \text{Res block} \rightarrow a^{[l+2]} \\
a^{[l+2]} = Relu(z^{[l+2]} + a^{[l]}) = g(W^{[l+2]} a^{[l+1]} + b^{[l+2]} + a^{[l]}) \\
\text{Assume } W^{[l+2]} = 0, b^{[l+2]} = 0, \text{then } \\
a^{[l+2]} = g(a^{[l]}) = a^{[l]} (\text{because applying relu on non-negative is the 
same}) \\
$$

2. The observation: identity function is easy for residule block to learn.
3. What's wrong with normal very deep network: difficult to learn even identity functions.
4. With residual net, it can guarantee at least learn identity functions. And we can hope
  sometimes to improve performance.
5. $a^{[l+2]}$ and $a^{[l]}$ same dimension, answered my questions before. If the 
  dimensions are different, you can use padding or multiply a matrix.

### 2-5 Networks in Networks and 1x1 Convolutions: "network in network"

* Why does a 1x1 convolution do?

1. If only has 6x6x1 image and 1x1x1 filter, is not very useful
2. If has 6x6x32 image and 1x1x32 filter, then it is different. Then output is
  6x6x # filters.
3. The details are not widely used, but ideas inspire other ideas.
4. One use is to shrink the # of channels as following.
5. 1x1 also does something non-trival: adding non-linearity, decrease/keep/increase
  # of channels.

$$
28 \times 28 \times 192
\xrightarrow[\text{ Relu }]{\substack{ \text{CONV 1x1} \\ 32 \text{ filters } }}
28 \times 28 \times 32
$$

### 2-6 Inception Network Motivation: "going deeper with convolutions"

* Motivation for inception network

1. Why do we have to decide which size of filter 1x1, 3x3 or 5x5? Use them all.

$$
28 \times 28 \times 192
\xrightarrow{
\substack{ 
  1 \times 1 \\
  3 \times 3 \text{ same} \\
  5 \times 5 \text{ same} \\
  \text{Max-Pool} \\
}
}
\begin{bmatrix}
  28 \times 28 \times 64 \\
  28 \times 28 \times 128 \\
  28 \times 28 \times 32 \\
  28 \times 28 \times 32 \\
\end{bmatrix}
$$

2. 问题：why the max pooling above is not same depth? (192 -> 32, 192/32 = 6)?
3. The max pooling above needs to use padding to keep same dimension (28x28) and s = 1.
4. Instead of you needing to pick one filter sizes, you can do them all and concatenate
  all outputs.

* The problem of computational cost

$$
28 \times 28 \times 192
\xrightarrow{
\substack{ 
  \text{ CONV} \\
  5 \times 5 \text{ same} \\
  32 \text{ filters} \\
}
}
28 \times 28 \times 32 \\
\text{cost: } 32 \text{ filters} \times (5 \times 5 \times 192) \\
\text{total multiplication: } 28 \times 28 \times 32 \times (5 \times 5 \times 192) = 120M
$$

* Use 1x1 Convolution

$$
28 \times 28 \times 192
\xrightarrow{
\substack{ 
  \text{ CONV} \\
  1 \times 1, \\
  16 \text{ filters}, \\
  1 \times 1 \times 192
}
}
28 \times 28 \times 16 \text{ (bottle neck layer)}
\xrightarrow{
\substack{ 
  \text{ CONV} \\
  5 \times 5, \\
  32 \text{ filters}, \\
  5 \times 5 \times 16
}
}
28 \times 28 \times 32 \\
\text{cost: } 28 \times 28 \times 192 \times 192 + 28 \times 28 \times 32 \times 5 \times 
5 \times 16 = 12.4M
$$

* The shrink down does not hurt the performance.

### 2-7 Inception Network

* Inception module

1. Previous Activation -> 1X1 conv
2. Previous Activation -> 1X1 conv -> 3X3 conv
3. Previous Activation -> 1X1 conv -> 5X5 conv
4. Maxpool is little big tricky and funny, first it's same, 2nd it needs to go to
  another 1x1 CONV layer.

$$
\text{Previous Activation } 28 \times 28 \times 192 \xrightarrow{}
\text{MAXPOOL } 3 \times 3, s=1 \text{ same } 28 \times 28 \times 192 \\
\xrightarrow{}
\text{1 x 1 CONV } 28 \times 28 \times 32
$$

5. Then channel concat.

* Inception network

1. If look at the figure in the paper, it's complicated. But there are a lot of
  repeating block.
2. In the original paper, there are side branches to make prediction.

### 2-8 Using Open-Source Implementation

* If you heard some work, look for the implementation from internet rather than starting
  from scratch.
* Some of them may have pre-trained the network, so you don't need to spend long time
  to train.

### 2-9 Transfer Learning

* Training becomes much faster if someone save the weights.
* ImageNet, MS COCO
* Use other's weight as initialization.

* Training set small.

1. Tigger/Misty/Neither, training data is small. What do you do?
2. Download not only the code but also the weights.
3. Just get rid of the last existing softmax layer, make a new Tigger/Misty/Neither
  softmax layer.
4. Freeze all the parameters in the previous layers, just train softmax layer. Might get
  good performance even with small dataset.
5. DL framework has `trainableParameter = 0` or `freeze = 1`
6. A trick to speed up training, precompute Given input X, compute the activation
  up to the last new softmax layer and save to disk. Then just train the softmax layer.

* What if you have large training data set?

1. Freeze the first several hidden layers, then
    1. Either retrain the last several few layers, use the weights as initialized weights
    2. Or just blow away the last several few layers, build own hidden layers.
2. More data, freeze less, train more layers.

* If you really have a lot data

1. Use weights as initialization, then updating all the networks.

* In all different applications of DL, computer vision is one where transfer learning
  should always do.

### 2-10 Data augmentation

* In computer vision, having more data is almost always help. Can not always get enough
  data.

* Common augmentation method

1. Mirroring
2. Random cropping, 问题：如果猫的头没有被截图到怎么办？
    1. In practice works well, if random crops are reasonably large subset of actual image
3. The following is used a little bit less perhaps because of the complexity.
    1. Rotation/Shearing/Local warping

* Color shifting

1. Given picture, adding different distortion to RGB channels.
2. In practice, draw RGB from some distribution that could be small.
3. Advanced: PCA (ml-class.org)
    1. AlexNet paper "PCA color augmentation": If has big red/blue, less green, PCA
      will do the same.

* Implementing distortions during training

1. If has a large training set
2. Use one tread to read the image, do the distortion
3. The pass the data to another CPU thread or GPU to do training.
4. The last 2 steps can run in parallel.
5. A good start to use someone else's implementation
6. If that does not capture the variance you need, then use the hyperparameters your
  self.

### 2-11 State of Computer Vision

* Data v.s. hand engineering

1. Speech recognition (reasonablely large amount of data)
2. Image recognition (even though there are a lot of data, still wish to have more data)
3. Object detection (even less data): given a picture, find bounding box, which is
  more expensive.
4. When you have lots of data, you can have simpler algorithms and less hand engineering.
5. When you have little data, people has more hand engineering ("hacks").

* Two sources of knowledge

1. Labeled data $(x, y)$
2. Hand engineered features/network architecture/other components
3. CV has complex architectures because in the absence of more data.
4. Hand engineering is very difficult and skillful, needs insight
5. One thing helps is transfer learning.
    1. Tigger/Misty/neither has so little data.

* Tips for doing well on benchmarks/winning competitions

1. Positive side: figure out the most effective algorithms
2. Negtive side: people do things on benchmark which is not used in production.
3. Ensembling
    1. Train several networks independently and average their outputs $\hat{y}$
        not their weights. 1% or 2% better.
    2. 3 ~ 15 networks, so slow down by a factor of 3 ~ 15.
    3. So almost never used in production to serve customer.
4. Multi-crop on test time
    1. Run classifier on multiple versions of test images and average results
    2. 10-crop, crop in the middle and 4 corners for original and mirrored
    3. More often used in benchmarks than in production systems.

* Use open source code

1. Use architectures of networks published in the literature
2. Use open source implementation if possible
3. Use pretrained models and fine-tune on your dataset.

# numpy functions

```
np.sum
np.square
np.zeros
np.random.randn
np.random.rand
np.multiply
np.linalg.norm
np.copy
# random sample in log scale
r = -4 * np.random.rand()
alpha = 10 ** r

# Implement the following function, which pads all the images of a batch of examples X with zeros. Use np.pad. Note if you want to pad the array "a" of shape  (5,5,5,5,5)(5,5,5,5,5)  with pad = 1 for the 2nd dimension, pad = 3 for the 4th dimension and pad = 0 for the rest, you would do:

a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))
```

# tensorflow functions

```
tf.matmul(..., ...)
tf.add(..., ...)
np.random.randn(...)
tf.one_hot(labels, depth, axis)
tf.ones()
tf.zeros()
```
